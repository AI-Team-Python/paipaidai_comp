{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/\"\n",
    "TRAIN_PATH = DATA_PATH + \"train.csv\"\n",
    "TEST_PATH = DATA_PATH + \"test.csv\"\n",
    "WORD_EMBED_PATH = DATA_PATH + \"word_embed.txt\"\n",
    "CHAR_EMBED_PATH = DATA_PATH + \"char_embed.txt\"\n",
    "QUEST_PATH = DATA_PATH + \"question.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(TRAIN_PATH)\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "question_data = pd.read_csv(QUEST_PATH)\n",
    "word_embedding_data = pd.read_csv(WORD_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "char_embedding_data = pd.read_csv(CHAR_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "\n",
    "question_data[\"words\"] = question_data[\"words\"].str.split(\" \")\n",
    "question_data[\"chars\"] = question_data[\"chars\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_WORD_NUMS = 10000\n",
    "\n",
    "word_tokenizer = Tokenizer(MAX_WORD_NUMS)\n",
    "word_tokenizer.fit_on_texts(question_data[\"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_data = np.concatenate(\n",
    "    (\n",
    "        np.zeros(shape=(1, word_embedding_data.shape[1]), dtype=np.float64),\n",
    "        word_embedding_data.loc[list(word_tokenizer.word_index.keys())[:MAX_WORD_NUMS]].values\n",
    "    ),\n",
    "    axis=0\n",
    ")\n",
    "word_embedding_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((254386, 30), (254386, 30), (172956, 30), (172956, 30))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "WORD_SEQ_LEN = 30\n",
    "\n",
    "def gen_data(data):\n",
    "    seq_word1 = word_tokenizer.texts_to_sequences(data.merge(question_data, how=\"left\", left_on=\"q1\", right_on=\"qid\")[\"words\"])\n",
    "    seq_word2 = word_tokenizer.texts_to_sequences(data.merge(question_data, how=\"left\", left_on=\"q2\", right_on=\"qid\")[\"words\"])\n",
    "    return pad_sequences(seq_word1, maxlen=WORD_SEQ_LEN, padding=\"pre\",truncating=\"pre\"), \\\n",
    "        pad_sequences(seq_word2, maxlen=WORD_SEQ_LEN, padding=\"pre\",truncating=\"pre\")\n",
    "\n",
    "word1, word2 = gen_data(train_data)\n",
    "test_word1, test_word2 = gen_data(test_data)\n",
    "\n",
    "word1.shape, word2.shape, test_word1.shape, test_word2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = train_data[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Conv1D, GlobalAveragePooling1D, Lambda, Dropout, BatchNormalization, Dense, K\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "CONV_LEN_1 = 128\n",
    "CONV_LEN_2 = 128\n",
    "CONV_LEN_3 = 128\n",
    "CONV_LEN_4 = 128\n",
    "CONV_LEN_5 = 128\n",
    "CONV_LEN_6 = 128\n",
    "CONV_LEN = CONV_LEN_1 + CONV_LEN_2 + CONV_LEN_3 + CONV_LEN_4 + CONV_LEN_5 + CONV_LEN_6\n",
    "DROP_RATE = 0.6\n",
    "DENSE_SIZE = 300\n",
    "BATCH_SIZE = 2048\n",
    "NUM_EPOCHES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Train on 203508 samples, validate on 50878 samples\n",
      "Epoch 1/100\n",
      "203508/203508 [==============================] - 29s 144us/step - loss: 0.5025 - acc: 0.7658 - val_loss: 0.3658 - val_acc: 0.8375\n",
      "Epoch 2/100\n",
      "203508/203508 [==============================] - 27s 133us/step - loss: 0.3788 - acc: 0.8302 - val_loss: 0.3247 - val_acc: 0.8572\n",
      "Epoch 3/100\n",
      "203508/203508 [==============================] - 27s 133us/step - loss: 0.3362 - acc: 0.8504 - val_loss: 0.3000 - val_acc: 0.8700\n",
      "Epoch 4/100\n",
      "203508/203508 [==============================] - 27s 135us/step - loss: 0.3121 - acc: 0.8627 - val_loss: 0.2846 - val_acc: 0.8797\n",
      "Epoch 5/100\n",
      "203508/203508 [==============================] - 28s 137us/step - loss: 0.2937 - acc: 0.8724 - val_loss: 0.2721 - val_acc: 0.8843\n",
      "Epoch 6/100\n",
      "203508/203508 [==============================] - 28s 137us/step - loss: 0.2771 - acc: 0.8796 - val_loss: 0.2620 - val_acc: 0.8891\n",
      "Epoch 7/100\n",
      "203508/203508 [==============================] - 28s 136us/step - loss: 0.2664 - acc: 0.8847 - val_loss: 0.2547 - val_acc: 0.8927\n",
      "Epoch 8/100\n",
      "203508/203508 [==============================] - 27s 135us/step - loss: 0.2567 - acc: 0.8896 - val_loss: 0.2476 - val_acc: 0.8935\n",
      "Epoch 9/100\n",
      "203508/203508 [==============================] - 28s 136us/step - loss: 0.2475 - acc: 0.8939 - val_loss: 0.2433 - val_acc: 0.8971\n",
      "Epoch 10/100\n",
      "203508/203508 [==============================] - 28s 136us/step - loss: 0.2399 - acc: 0.8973 - val_loss: 0.2397 - val_acc: 0.8988\n",
      "Epoch 11/100\n",
      "203508/203508 [==============================] - 28s 136us/step - loss: 0.2336 - acc: 0.9002 - val_loss: 0.2356 - val_acc: 0.8998\n",
      "Epoch 12/100\n",
      "203508/203508 [==============================] - 28s 137us/step - loss: 0.2274 - acc: 0.9036 - val_loss: 0.2343 - val_acc: 0.9018\n",
      "Epoch 13/100\n",
      "203508/203508 [==============================] - 28s 137us/step - loss: 0.2206 - acc: 0.9068 - val_loss: 0.2309 - val_acc: 0.9019\n",
      "Epoch 14/100\n",
      "203508/203508 [==============================] - 28s 137us/step - loss: 0.2162 - acc: 0.9087 - val_loss: 0.2290 - val_acc: 0.9024\n",
      "Epoch 15/100\n",
      "203508/203508 [==============================] - 28s 137us/step - loss: 0.2102 - acc: 0.9117 - val_loss: 0.2276 - val_acc: 0.9030\n",
      "Epoch 16/100\n",
      "203508/203508 [==============================] - 28s 138us/step - loss: 0.2054 - acc: 0.9135 - val_loss: 0.2255 - val_acc: 0.9044\n",
      "Epoch 17/100\n",
      "203508/203508 [==============================] - 28s 137us/step - loss: 0.1996 - acc: 0.9161 - val_loss: 0.2230 - val_acc: 0.9071\n",
      "Epoch 18/100\n",
      "203508/203508 [==============================] - 28s 137us/step - loss: 0.1973 - acc: 0.9171 - val_loss: 0.2210 - val_acc: 0.9066\n",
      "Epoch 19/100\n",
      "203508/203508 [==============================] - 28s 138us/step - loss: 0.1932 - acc: 0.9197 - val_loss: 0.2210 - val_acc: 0.9076\n",
      "Epoch 20/100\n",
      "203508/203508 [==============================] - 28s 136us/step - loss: 0.1901 - acc: 0.9205 - val_loss: 0.2196 - val_acc: 0.9070\n",
      "Epoch 21/100\n",
      "203508/203508 [==============================] - 28s 136us/step - loss: 0.1860 - acc: 0.9223 - val_loss: 0.2199 - val_acc: 0.9069\n",
      "Epoch 22/100\n",
      "203508/203508 [==============================] - 28s 137us/step - loss: 0.1829 - acc: 0.9236 - val_loss: 0.2184 - val_acc: 0.9082\n",
      "Epoch 23/100\n",
      "203508/203508 [==============================] - 28s 138us/step - loss: 0.1789 - acc: 0.9255 - val_loss: 0.2191 - val_acc: 0.9067\n",
      "Epoch 24/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1758 - acc: 0.9273 - val_loss: 0.2195 - val_acc: 0.9072\n",
      "Epoch 25/100\n",
      "203508/203508 [==============================] - 28s 138us/step - loss: 0.1740 - acc: 0.9277 - val_loss: 0.2182 - val_acc: 0.9083\n",
      "Epoch 26/100\n",
      "203508/203508 [==============================] - 28s 138us/step - loss: 0.1704 - acc: 0.9286 - val_loss: 0.2165 - val_acc: 0.9096\n",
      "Epoch 27/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1680 - acc: 0.9305 - val_loss: 0.2148 - val_acc: 0.9114\n",
      "Epoch 28/100\n",
      "203508/203508 [==============================] - 28s 137us/step - loss: 0.1650 - acc: 0.9312 - val_loss: 0.2165 - val_acc: 0.9096\n",
      "Epoch 29/100\n",
      "203508/203508 [==============================] - 28s 137us/step - loss: 0.1629 - acc: 0.9329 - val_loss: 0.2165 - val_acc: 0.9096\n",
      "Epoch 30/100\n",
      "203508/203508 [==============================] - 28s 137us/step - loss: 0.1613 - acc: 0.9331 - val_loss: 0.2141 - val_acc: 0.9109\n",
      "Epoch 31/100\n",
      "203508/203508 [==============================] - 28s 138us/step - loss: 0.1588 - acc: 0.9349 - val_loss: 0.2159 - val_acc: 0.9104\n",
      "Epoch 32/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1555 - acc: 0.9359 - val_loss: 0.2169 - val_acc: 0.9093\n",
      "Epoch 33/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1546 - acc: 0.9372 - val_loss: 0.2150 - val_acc: 0.9122\n",
      "Epoch 34/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1529 - acc: 0.9375 - val_loss: 0.2145 - val_acc: 0.9112\n",
      "Epoch 35/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1491 - acc: 0.9393 - val_loss: 0.2149 - val_acc: 0.9119\n",
      "Epoch 36/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1497 - acc: 0.9387 - val_loss: 0.2133 - val_acc: 0.9121\n",
      "Epoch 37/100\n",
      "203508/203508 [==============================] - 28s 138us/step - loss: 0.1471 - acc: 0.9406 - val_loss: 0.2175 - val_acc: 0.9106\n",
      "Epoch 38/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1452 - acc: 0.9410 - val_loss: 0.2133 - val_acc: 0.9121\n",
      "Epoch 39/100\n",
      "203508/203508 [==============================] - 28s 136us/step - loss: 0.1436 - acc: 0.9419 - val_loss: 0.2164 - val_acc: 0.9107\n",
      "Epoch 40/100\n",
      "203508/203508 [==============================] - 28s 138us/step - loss: 0.1423 - acc: 0.9425 - val_loss: 0.2148 - val_acc: 0.9115\n",
      "Epoch 41/100\n",
      "203508/203508 [==============================] - 28s 137us/step - loss: 0.1393 - acc: 0.9432 - val_loss: 0.2178 - val_acc: 0.9109\n",
      "Epoch 42/100\n",
      "203508/203508 [==============================] - 28s 138us/step - loss: 0.1389 - acc: 0.9444 - val_loss: 0.2167 - val_acc: 0.9111\n",
      "Epoch 43/100\n",
      "203508/203508 [==============================] - 28s 138us/step - loss: 0.1382 - acc: 0.9440 - val_loss: 0.2150 - val_acc: 0.9129\n",
      "Epoch 44/100\n",
      "203508/203508 [==============================] - 28s 138us/step - loss: 0.1360 - acc: 0.9448 - val_loss: 0.2136 - val_acc: 0.9126\n",
      "Epoch 45/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1369 - acc: 0.9444 - val_loss: 0.2130 - val_acc: 0.9134\n",
      "Epoch 46/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1325 - acc: 0.9465 - val_loss: 0.2186 - val_acc: 0.9114\n",
      "Epoch 47/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1316 - acc: 0.9466 - val_loss: 0.2172 - val_acc: 0.9117\n",
      "Epoch 48/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1317 - acc: 0.9471 - val_loss: 0.2154 - val_acc: 0.9121\n",
      "Epoch 49/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1284 - acc: 0.9483 - val_loss: 0.2219 - val_acc: 0.9098\n",
      "Epoch 50/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1290 - acc: 0.9480 - val_loss: 0.2189 - val_acc: 0.9121\n",
      "Epoch 51/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1268 - acc: 0.9495 - val_loss: 0.2149 - val_acc: 0.9128\n",
      "Epoch 52/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1257 - acc: 0.9502 - val_loss: 0.2206 - val_acc: 0.9109\n",
      "Epoch 53/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1257 - acc: 0.9496 - val_loss: 0.2224 - val_acc: 0.9110\n",
      "Epoch 54/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1224 - acc: 0.9512 - val_loss: 0.2173 - val_acc: 0.9136\n",
      "Epoch 55/100\n",
      "203508/203508 [==============================] - 28s 139us/step - loss: 0.1232 - acc: 0.9506 - val_loss: 0.2192 - val_acc: 0.9116\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_word1, dev_word1, train_word2, dev_word2, train_y, dev_y = train_test_split(\n",
    "    word1, word2, train_data[\"label\"].values,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "word_input1 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "word_input2 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=word_embedding_data.shape[0],\n",
    "    output_dim=word_embedding_data.shape[1],\n",
    "    weights=[word_embedding_data],\n",
    "    input_length=WORD_SEQ_LEN,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "word_vector1 = embedding_layer(word_input1)\n",
    "word_vector2 = embedding_layer(word_input2)\n",
    "\n",
    "conv1 = Conv1D(filters=CONV_LEN_1, kernel_size=1, padding=\"same\", activation=\"relu\")\n",
    "conv1a = conv1(word_vector1)\n",
    "conv1a = GlobalAveragePooling1D()(conv1a)\n",
    "conv1b = conv1(word_vector2)\n",
    "conv1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "conv2 = Conv1D(filters=CONV_LEN_2, kernel_size=2, padding=\"same\", activation=\"relu\")\n",
    "conv2a = conv2(word_vector1)\n",
    "conv2a = GlobalAveragePooling1D()(conv2a)\n",
    "conv2b = conv2(word_vector2)\n",
    "conv2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "conv3 = Conv1D(filters=CONV_LEN_3, kernel_size=3, padding=\"same\", activation=\"relu\")\n",
    "conv3a = conv3(word_vector1)\n",
    "conv3a = GlobalAveragePooling1D()(conv3a)\n",
    "conv3b = conv3(word_vector2)\n",
    "conv3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "conv4 = Conv1D(filters=CONV_LEN_4, kernel_size=4, padding=\"same\", activation=\"relu\")\n",
    "conv4a = conv4(word_vector1)\n",
    "conv4a = GlobalAveragePooling1D()(conv4a)\n",
    "conv4b = conv4(word_vector2)\n",
    "conv4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "conv5 = Conv1D(filters=CONV_LEN_5, kernel_size=5, padding=\"same\", activation=\"relu\")\n",
    "conv5a = conv5(word_vector1)\n",
    "conv5a = GlobalAveragePooling1D()(conv5a)\n",
    "conv5b = conv5(word_vector2)\n",
    "conv5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "conv6 = Conv1D(filters=CONV_LEN_6, kernel_size=6, padding=\"same\", activation=\"relu\")\n",
    "conv6a = conv6(word_vector1)\n",
    "conv6a = GlobalAveragePooling1D()(conv6a)\n",
    "conv6b = conv6(word_vector2)\n",
    "conv6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "merge_a = concatenate([conv1a, conv2a, conv3a, conv4a, conv5a, conv6a])\n",
    "merge_b = concatenate([conv1b, conv2b, conv3b, conv4b, conv5b, conv6b])\n",
    "# merge = concatenate([merge_a, merge_b])\n",
    "\n",
    "diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(CONV_LEN,))([merge_a, merge_b])\n",
    "mult = Lambda(lambda x: x[0] * x[1], output_shape=(CONV_LEN,))([merge_a, merge_b])\n",
    "merge = concatenate([diff, mult])\n",
    "\n",
    "x = Dropout(DROP_RATE)(merge)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(DENSE_SIZE, activation=\"relu\")(x)\n",
    "\n",
    "x = Dropout(DROP_RATE)(x)\n",
    "x = BatchNormalization()(x)\n",
    "pred = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(\n",
    "    inputs = [word_input1, word_input2],\n",
    "    outputs = pred\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"acc\"]\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(\"val_loss\", patience=10)\n",
    "check_point = ModelCheckpoint(\n",
    "    \"./log/%s.cnn.{epoch:03d}.hdf5\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "    monitor=\"val_loss\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "model_res = model.fit(\n",
    "    x=[train_word1, train_word2],\n",
    "    y=train_y,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=NUM_EPOCHES,\n",
    "    validation_data=([dev_word1, dev_word2], dev_y),\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stop, check_point]\n",
    ")\n",
    "\n",
    "test_pred = model.predict([test_word1, test_word2], batch_size=BATCH_SIZE)\n",
    "pd.DataFrame(test_pred, columns=[\"y_pre\"]).to_csv(\"./result/pred_last.csv\", index=False)\n",
    "\n",
    "print(\"load model %s\" % (glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"),))\n",
    "model.load_weights(glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"))\n",
    "test_pred = model.predict([test_word1, test_word2], batch_size=BATCH_SIZE)\n",
    "pd.DataFrame(test_pred, columns=[\"y_pre\"]).to_csv(\"./result/pred_best.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# pred_collect = []\n",
    "\n",
    "# for i, (train_index, dev_index) in enumerate(StratifiedKFold(n_splits=10).split(X=word1, y=label)):\n",
    "#     train_word1, train_word2, train_y = word1[train_index, :], word2[train_index, :], label[train_index]\n",
    "#     dev_word1, dev_word2, dev_y = word1[dev_index, :], word2[dev_index, :], label[dev_index]\n",
    "    \n",
    "#     word_input1 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "#     word_input2 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "    \n",
    "#     embedding_layer = Embedding(\n",
    "#         input_dim=word_embedding_data.shape[0],\n",
    "#         output_dim=word_embedding_data.shape[1],\n",
    "#         weights=[word_embedding_data],\n",
    "#         input_length=WORD_SEQ_LEN,\n",
    "#         trainable=False\n",
    "#     )\n",
    "    \n",
    "#     word_vector1 = embedding_layer(word_input1)\n",
    "#     word_vector2 = embedding_layer(word_input2)\n",
    "    \n",
    "#     conv1 = Conv1D(filters=128, kernel_size=1, padding=\"same\", activation=\"relu\")\n",
    "#     conv1a = conv1(word_vector1)\n",
    "#     conv1a = GlobalAveragePooling1D()(conv1a)\n",
    "#     conv1b = conv1(word_vector2)\n",
    "#     conv1b = GlobalAveragePooling1D()(conv1b)\n",
    "    \n",
    "#     conv2 = Conv1D(filters=128, kernel_size=2, padding=\"same\", activation=\"relu\")\n",
    "#     conv2a = conv2(word_vector1)\n",
    "#     conv2a = GlobalAveragePooling1D()(conv2a)\n",
    "#     conv2b = conv2(word_vector2)\n",
    "#     conv2b = GlobalAveragePooling1D()(conv2b)\n",
    "    \n",
    "#     conv3 = Conv1D(filters=128, kernel_size=3, padding=\"same\", activation=\"relu\")\n",
    "#     conv3a = conv3(word_vector1)\n",
    "#     conv3a = GlobalAveragePooling1D()(conv3a)\n",
    "#     conv3b = conv3(word_vector2)\n",
    "#     conv3b = GlobalAveragePooling1D()(conv3b)\n",
    "    \n",
    "#     conv4 = Conv1D(filters=128, kernel_size=4, padding=\"same\", activation=\"relu\")\n",
    "#     conv4a = conv4(word_vector1)\n",
    "#     conv4a = GlobalAveragePooling1D()(conv4a)\n",
    "#     conv4b = conv4(word_vector2)\n",
    "#     conv4b = GlobalAveragePooling1D()(conv4b)\n",
    "    \n",
    "#     conv5 = Conv1D(filters=128, kernel_size=5, padding=\"same\", activation=\"relu\")\n",
    "#     conv5a = conv5(word_vector1)\n",
    "#     conv5a = GlobalAveragePooling1D()(conv5a)\n",
    "#     conv5b = conv5(word_vector2)\n",
    "#     conv5b = GlobalAveragePooling1D()(conv5b)\n",
    "    \n",
    "#     conv6 = Conv1D(filters=128, kernel_size=6, padding=\"same\", activation=\"relu\")\n",
    "#     conv6a = conv6(word_vector1)\n",
    "#     conv6a = GlobalAveragePooling1D()(conv6a)\n",
    "#     conv6b = conv6(word_vector2)\n",
    "#     conv6b = GlobalAveragePooling1D()(conv6b)\n",
    "    \n",
    "#     merge_a = concatenate([conv1a, conv2a, conv3a, conv4a, conv5a, conv6a])\n",
    "#     merge_b = concatenate([conv1b, conv2b, conv3b, conv4b, conv5b, conv6b])\n",
    "    \n",
    "#     diff = Lambda(lambda x: x[0] - x[1], output_shape=(CONV_LEN,))([merge_a, merge_b])\n",
    "#     mult = Lambda(lambda x: x[0] * x[1], output_shape=(CONV_LEN,))([merge_a, merge_b])\n",
    "    \n",
    "#     merge = concatenate([diff, mult])\n",
    "    \n",
    "#     x = Dropout(DROP_RATE)(merge)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dense(DENSE_SIZE, activation=\"relu\")(x)\n",
    "    \n",
    "#     x = Dropout(DROP_RATE)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     pred = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "#     model = Model(\n",
    "#         inputs = [word_input1, word_input2],\n",
    "#         outputs = pred\n",
    "#     )\n",
    "#     model.compile(\n",
    "#         optimizer=\"adam\",\n",
    "#         loss=\"binary_crossentropy\",\n",
    "#         metrics=[\"acc\"]\n",
    "#     )\n",
    "    \n",
    "#     early_stop = EarlyStopping(\"val_loss\", patience=10)\n",
    "#     check_point = ModelCheckpoint(\n",
    "#         \"./log/cnn_%02d.{epoch:02d}_{val_loss:.3f}.hdf5\" % (i + 1),\n",
    "#         monitor=\"val_loss\",\n",
    "#         save_best_only=True,\n",
    "#         save_weights_only=True\n",
    "#     )\n",
    "    \n",
    "#     model_res = model.fit(\n",
    "#         x=[train_word1, train_word2],\n",
    "#         y=train_y,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         epochs=NUM_EPOCHES,\n",
    "#         validation_data=([dev_word1, dev_word2], dev_y),\n",
    "#         shuffle=True,\n",
    "#         callbacks=[early_stop, check_point]\n",
    "#     )\n",
    "    \n",
    "#     print(\"load model %s\" % (glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"),))\n",
    "#     model.load_weights(glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"))\n",
    "\n",
    "#     test_pred = model.predict([test_word1, test_word2], batch_size=BATCH_SIZE)\n",
    "#     pred_collect.append(pd.DataFrame(test_pred, columns=[\"y_pre\"]))\n",
    "\n",
    "# pd.DataFrame(pd.concat(pred_collect, axis=1).mean(axis=1), columns=[\"y_pre\"]).to_csv(\"./result/pred.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
