{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, TimeDistributed, Dense, Concatenate, Dropout, BatchNormalization,GRU,LSTM,Conv1D,MaxPool1D,Flatten,Lambda\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "que = pd.read_csv('./data/question.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "char_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/word_embed.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        s = line.strip('\\n').split(' ')\n",
    "        word_dict[s[0]] = [float(v) for v in s[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/char_embed.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        s = line.strip('\\n').split(' ')\n",
    "        char_dict[s[0]] = [float(v) for v in s[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train,que[['qid','words']],left_on='q1',right_on='qid',how='left')\n",
    "train = pd.merge(train,que[['qid','words']],left_on='q2',right_on='qid',how='left')\n",
    "train.drop(['qid_x','qid_y'],axis=1,inplace=True)\n",
    "train.columns = ['label','q1','q2','word1','word2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.merge(test,que[['qid','words']],left_on='q1',right_on='qid',how='left')\n",
    "test = pd.merge(test,que[['qid','words']],left_on='q2',right_on='qid',how='left')\n",
    "test.drop(['qid_x','qid_y'],axis=1,inplace=True)\n",
    "test.columns = ['q1','q2','word1','word2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 10000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(que['words'])\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_train = tokenizer.texts_to_sequences(train['word1'])\n",
    "q2_train = tokenizer.texts_to_sequences(train['word2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_test = tokenizer.texts_to_sequences(test['word1'])\n",
    "q2_test = tokenizer.texts_to_sequences(test['word2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构建embedding层\n",
    "EMBEDDING_DIM = 300\n",
    "word_embedding_matrix = np.zeros((MAX_NB_WORDS + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = word_dict.get(str(word).upper())\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构建输入张量\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "q1_data_tr = pad_sequences(q1_train,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data_tr = pad_sequences(q2_train,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q1_data_te = pad_sequences(q1_test,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data_te = pad_sequences(q2_test,maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_concat = np.stack([q1_data_tr,q2_data_tr],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Train on 228946 samples, validate on 25440 samples\n",
      "Epoch 1/10\n",
      "228946/228946 [==============================] - 30s 131us/step - loss: 0.4406 - acc: 0.7937 - val_loss: 0.3287 - val_acc: 0.8552\n",
      "Epoch 2/10\n",
      "228946/228946 [==============================] - 28s 121us/step - loss: 0.3137 - acc: 0.8626 - val_loss: 0.2828 - val_acc: 0.8768\n",
      "Epoch 3/10\n",
      "228946/228946 [==============================] - 28s 121us/step - loss: 0.2683 - acc: 0.8855 - val_loss: 0.2650 - val_acc: 0.8853\n",
      "Epoch 4/10\n",
      "228946/228946 [==============================] - 28s 121us/step - loss: 0.2395 - acc: 0.8988 - val_loss: 0.2483 - val_acc: 0.8914\n",
      "Epoch 5/10\n",
      "228946/228946 [==============================] - 28s 121us/step - loss: 0.2199 - acc: 0.9073 - val_loss: 0.2368 - val_acc: 0.8993\n",
      "Epoch 6/10\n",
      "228946/228946 [==============================] - 28s 121us/step - loss: 0.2039 - acc: 0.9156 - val_loss: 0.2343 - val_acc: 0.9015\n",
      "Epoch 7/10\n",
      "228946/228946 [==============================] - 28s 121us/step - loss: 0.1919 - acc: 0.9205 - val_loss: 0.2337 - val_acc: 0.9024\n",
      "Epoch 8/10\n",
      "228946/228946 [==============================] - 28s 121us/step - loss: 0.1801 - acc: 0.9258 - val_loss: 0.2378 - val_acc: 0.9003\n",
      "Epoch 9/10\n",
      "228946/228946 [==============================] - 28s 121us/step - loss: 0.1714 - acc: 0.9298 - val_loss: 0.2426 - val_acc: 0.9020\n",
      "Epoch 10/10\n",
      "228946/228946 [==============================] - 28s 121us/step - loss: 0.1645 - acc: 0.9323 - val_loss: 0.2454 - val_acc: 0.9012\n",
      "Train on 228946 samples, validate on 25440 samples\n",
      "Epoch 1/10\n",
      "228946/228946 [==============================] - 29s 127us/step - loss: 0.4346 - acc: 0.7967 - val_loss: 0.3273 - val_acc: 0.8588\n",
      "Epoch 2/10\n",
      "228946/228946 [==============================] - 28s 122us/step - loss: 0.3114 - acc: 0.8641 - val_loss: 0.2801 - val_acc: 0.8778\n",
      "Epoch 3/10\n",
      "228946/228946 [==============================] - 28s 122us/step - loss: 0.2678 - acc: 0.8850 - val_loss: 0.2642 - val_acc: 0.8872\n",
      "Epoch 4/10\n",
      "228946/228946 [==============================] - 28s 122us/step - loss: 0.2410 - acc: 0.8987 - val_loss: 0.2451 - val_acc: 0.8953\n",
      "Epoch 5/10\n",
      "228946/228946 [==============================] - 28s 122us/step - loss: 0.2215 - acc: 0.9076 - val_loss: 0.2438 - val_acc: 0.8985\n",
      "Epoch 6/10\n",
      "228946/228946 [==============================] - 28s 122us/step - loss: 0.2059 - acc: 0.9141 - val_loss: 0.2511 - val_acc: 0.8947\n",
      "Epoch 7/10\n",
      "228946/228946 [==============================] - 28s 122us/step - loss: 0.1935 - acc: 0.9199 - val_loss: 0.2433 - val_acc: 0.8999\n",
      "Epoch 8/10\n",
      "228946/228946 [==============================] - 28s 122us/step - loss: 0.1826 - acc: 0.9245 - val_loss: 0.2410 - val_acc: 0.9026\n",
      "Epoch 9/10\n",
      "228946/228946 [==============================] - 28s 122us/step - loss: 0.1724 - acc: 0.9294 - val_loss: 0.2457 - val_acc: 0.9014\n",
      "Epoch 10/10\n",
      "228946/228946 [==============================] - 28s 122us/step - loss: 0.1641 - acc: 0.9327 - val_loss: 0.2409 - val_acc: 0.9023\n",
      "Train on 228947 samples, validate on 25439 samples\n",
      "Epoch 1/10\n",
      "228947/228947 [==============================] - 30s 131us/step - loss: 0.4411 - acc: 0.7934 - val_loss: 0.3297 - val_acc: 0.8550\n",
      "Epoch 2/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.3123 - acc: 0.8631 - val_loss: 0.2939 - val_acc: 0.8720\n",
      "Epoch 3/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.2662 - acc: 0.8863 - val_loss: 0.2590 - val_acc: 0.8893\n",
      "Epoch 4/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.2382 - acc: 0.8985 - val_loss: 0.2508 - val_acc: 0.8934\n",
      "Epoch 5/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.2180 - acc: 0.9080 - val_loss: 0.2428 - val_acc: 0.8970\n",
      "Epoch 6/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.2019 - acc: 0.9156 - val_loss: 0.2379 - val_acc: 0.9009\n",
      "Epoch 7/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.1909 - acc: 0.9211 - val_loss: 0.2361 - val_acc: 0.9011\n",
      "Epoch 8/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.1794 - acc: 0.9258 - val_loss: 0.2367 - val_acc: 0.9032\n",
      "Epoch 9/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.1701 - acc: 0.9304 - val_loss: 0.2397 - val_acc: 0.9021\n",
      "Epoch 10/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.1609 - acc: 0.9344 - val_loss: 0.2439 - val_acc: 0.9014\n",
      "Train on 228947 samples, validate on 25439 samples\n",
      "Epoch 1/10\n",
      "228947/228947 [==============================] - 30s 129us/step - loss: 0.4451 - acc: 0.7902 - val_loss: 0.3368 - val_acc: 0.8531\n",
      "Epoch 2/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.3161 - acc: 0.8606 - val_loss: 0.2740 - val_acc: 0.8822\n",
      "Epoch 3/10\n",
      "228947/228947 [==============================] - 28s 121us/step - loss: 0.2678 - acc: 0.8850 - val_loss: 0.2599 - val_acc: 0.8885\n",
      "Epoch 4/10\n",
      "228947/228947 [==============================] - 28s 121us/step - loss: 0.2383 - acc: 0.8988 - val_loss: 0.2455 - val_acc: 0.8959\n",
      "Epoch 5/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.2184 - acc: 0.9085 - val_loss: 0.2345 - val_acc: 0.9009\n",
      "Epoch 6/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.2024 - acc: 0.9159 - val_loss: 0.2371 - val_acc: 0.8994\n",
      "Epoch 7/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.1889 - acc: 0.9222 - val_loss: 0.2446 - val_acc: 0.8992\n",
      "Epoch 8/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.1792 - acc: 0.9263 - val_loss: 0.2308 - val_acc: 0.9062\n",
      "Epoch 9/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.1686 - acc: 0.9305 - val_loss: 0.2444 - val_acc: 0.9008\n",
      "Epoch 10/10\n",
      "228947/228947 [==============================] - 28s 122us/step - loss: 0.1601 - acc: 0.9349 - val_loss: 0.2422 - val_acc: 0.9033\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/10\n",
      "228948/228948 [==============================] - 30s 130us/step - loss: 0.4379 - acc: 0.7954 - val_loss: 0.3290 - val_acc: 0.8557\n",
      "Epoch 2/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.3141 - acc: 0.8627 - val_loss: 0.2819 - val_acc: 0.8745\n",
      "Epoch 3/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2689 - acc: 0.8841 - val_loss: 0.2602 - val_acc: 0.8881\n",
      "Epoch 4/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2406 - acc: 0.8981 - val_loss: 0.2479 - val_acc: 0.8933\n",
      "Epoch 5/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2214 - acc: 0.9071 - val_loss: 0.2411 - val_acc: 0.8965\n",
      "Epoch 6/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2042 - acc: 0.9149 - val_loss: 0.2470 - val_acc: 0.8976\n",
      "Epoch 7/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1926 - acc: 0.9200 - val_loss: 0.2375 - val_acc: 0.9015\n",
      "Epoch 8/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1820 - acc: 0.9252 - val_loss: 0.2529 - val_acc: 0.8961\n",
      "Epoch 9/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1731 - acc: 0.9286 - val_loss: 0.2348 - val_acc: 0.9039\n",
      "Epoch 10/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1646 - acc: 0.9329 - val_loss: 0.2470 - val_acc: 0.9025\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/10\n",
      "228948/228948 [==============================] - 30s 130us/step - loss: 0.4365 - acc: 0.7951 - val_loss: 0.3252 - val_acc: 0.8563\n",
      "Epoch 2/10\n",
      "228948/228948 [==============================] - 28s 122us/step - loss: 0.3125 - acc: 0.8630 - val_loss: 0.2850 - val_acc: 0.8772\n",
      "Epoch 3/10\n",
      "228948/228948 [==============================] - 28s 122us/step - loss: 0.2680 - acc: 0.8851 - val_loss: 0.2664 - val_acc: 0.8863\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2403 - acc: 0.8987 - val_loss: 0.2559 - val_acc: 0.8938\n",
      "Epoch 5/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2194 - acc: 0.9081 - val_loss: 0.2502 - val_acc: 0.8941\n",
      "Epoch 6/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2041 - acc: 0.9154 - val_loss: 0.2527 - val_acc: 0.8964\n",
      "Epoch 7/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1909 - acc: 0.9212 - val_loss: 0.2468 - val_acc: 0.8989\n",
      "Epoch 8/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1799 - acc: 0.9256 - val_loss: 0.2457 - val_acc: 0.9006\n",
      "Epoch 9/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1711 - acc: 0.9301 - val_loss: 0.2446 - val_acc: 0.9012\n",
      "Epoch 10/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1625 - acc: 0.9335 - val_loss: 0.2407 - val_acc: 0.9045\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/10\n",
      "228948/228948 [==============================] - 30s 130us/step - loss: 0.4454 - acc: 0.7900 - val_loss: 0.3355 - val_acc: 0.8503\n",
      "Epoch 2/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.3163 - acc: 0.8615 - val_loss: 0.2807 - val_acc: 0.8777\n",
      "Epoch 3/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2705 - acc: 0.8837 - val_loss: 0.2606 - val_acc: 0.8879\n",
      "Epoch 4/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2418 - acc: 0.8980 - val_loss: 0.2496 - val_acc: 0.8956\n",
      "Epoch 5/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2220 - acc: 0.9071 - val_loss: 0.2434 - val_acc: 0.8968\n",
      "Epoch 6/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2059 - acc: 0.9135 - val_loss: 0.2348 - val_acc: 0.9019\n",
      "Epoch 7/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1942 - acc: 0.9192 - val_loss: 0.2320 - val_acc: 0.9038\n",
      "Epoch 8/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1822 - acc: 0.9247 - val_loss: 0.2274 - val_acc: 0.9057\n",
      "Epoch 9/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1724 - acc: 0.9293 - val_loss: 0.2337 - val_acc: 0.9050\n",
      "Epoch 10/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1640 - acc: 0.9325 - val_loss: 0.2403 - val_acc: 0.9041\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/10\n",
      "228948/228948 [==============================] - 30s 131us/step - loss: 0.4451 - acc: 0.7905 - val_loss: 0.3440 - val_acc: 0.8498\n",
      "Epoch 2/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.3143 - acc: 0.8621 - val_loss: 0.2872 - val_acc: 0.8777\n",
      "Epoch 3/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2663 - acc: 0.8868 - val_loss: 0.2658 - val_acc: 0.8861\n",
      "Epoch 4/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2366 - acc: 0.9000 - val_loss: 0.2463 - val_acc: 0.8953\n",
      "Epoch 5/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2172 - acc: 0.9092 - val_loss: 0.2484 - val_acc: 0.8968\n",
      "Epoch 6/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2017 - acc: 0.9164 - val_loss: 0.2428 - val_acc: 0.8999\n",
      "Epoch 7/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1905 - acc: 0.9217 - val_loss: 0.2480 - val_acc: 0.8982\n",
      "Epoch 8/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1788 - acc: 0.9266 - val_loss: 0.2363 - val_acc: 0.9045\n",
      "Epoch 9/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1680 - acc: 0.9311 - val_loss: 0.2360 - val_acc: 0.9044\n",
      "Epoch 10/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1616 - acc: 0.9343 - val_loss: 0.2447 - val_acc: 0.9023\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/10\n",
      "228948/228948 [==============================] - 30s 133us/step - loss: 0.4387 - acc: 0.7953 - val_loss: 0.3327 - val_acc: 0.8525\n",
      "Epoch 2/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.3133 - acc: 0.8627 - val_loss: 0.2788 - val_acc: 0.8780\n",
      "Epoch 3/10\n",
      "228948/228948 [==============================] - 28s 122us/step - loss: 0.2685 - acc: 0.8848 - val_loss: 0.2712 - val_acc: 0.8843\n",
      "Epoch 4/10\n",
      "228948/228948 [==============================] - 28s 122us/step - loss: 0.2405 - acc: 0.8981 - val_loss: 0.2486 - val_acc: 0.8941\n",
      "Epoch 5/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2194 - acc: 0.9083 - val_loss: 0.2454 - val_acc: 0.8964\n",
      "Epoch 6/10\n",
      "228948/228948 [==============================] - 28s 122us/step - loss: 0.2042 - acc: 0.9151 - val_loss: 0.2371 - val_acc: 0.9007\n",
      "Epoch 7/10\n",
      "228948/228948 [==============================] - 28s 122us/step - loss: 0.1914 - acc: 0.9212 - val_loss: 0.2379 - val_acc: 0.9016\n",
      "Epoch 8/10\n",
      "228948/228948 [==============================] - 28s 122us/step - loss: 0.1807 - acc: 0.9259 - val_loss: 0.2380 - val_acc: 0.9005\n",
      "Epoch 9/10\n",
      "228948/228948 [==============================] - 28s 122us/step - loss: 0.1703 - acc: 0.9303 - val_loss: 0.2326 - val_acc: 0.9058\n",
      "Epoch 10/10\n",
      "228948/228948 [==============================] - 28s 122us/step - loss: 0.1624 - acc: 0.9337 - val_loss: 0.2433 - val_acc: 0.9022\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/10\n",
      "228948/228948 [==============================] - 31s 133us/step - loss: 0.4383 - acc: 0.7943 - val_loss: 0.3190 - val_acc: 0.8588\n",
      "Epoch 2/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.3146 - acc: 0.8624 - val_loss: 0.2852 - val_acc: 0.8780\n",
      "Epoch 3/10\n",
      "228948/228948 [==============================] - 28s 122us/step - loss: 0.2678 - acc: 0.8851 - val_loss: 0.2546 - val_acc: 0.8924\n",
      "Epoch 4/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2401 - acc: 0.8990 - val_loss: 0.2533 - val_acc: 0.8924\n",
      "Epoch 5/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2203 - acc: 0.9077 - val_loss: 0.2362 - val_acc: 0.8990\n",
      "Epoch 6/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.2038 - acc: 0.9152 - val_loss: 0.2387 - val_acc: 0.9010\n",
      "Epoch 7/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1912 - acc: 0.9214 - val_loss: 0.2322 - val_acc: 0.9046\n",
      "Epoch 8/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1802 - acc: 0.9258 - val_loss: 0.2302 - val_acc: 0.9051\n",
      "Epoch 9/10\n",
      "228948/228948 [==============================] - 28s 121us/step - loss: 0.1704 - acc: 0.9302 - val_loss: 0.2414 - val_acc: 0.9024\n",
      "Epoch 10/10\n",
      "228948/228948 [==============================] - 28s 122us/step - loss: 0.1628 - acc: 0.9336 - val_loss: 0.2408 - val_acc: 0.9043\n"
     ]
    }
   ],
   "source": [
    "re = []\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "for tr,va in StratifiedKFold(n_splits=10).split(q_concat,train['label'].values):   \n",
    "    Q1_train = q_concat[tr][:,0]\n",
    "    Q2_train = q_concat[tr][:,1]\n",
    "    Q1_test = q_concat[va][:,0]\n",
    "    Q2_test = q_concat[va][:,1]\n",
    "    #构建embedding层，q1 和 q2共享此embedding层\n",
    "    embedding_layer = Embedding(MAX_NB_WORDS+1,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[word_embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)\n",
    "    #词嵌入\n",
    "    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "    #gru\n",
    "    gru = GRU(128, return_sequences=True)\n",
    "    q1 = gru(embedded_sequences_1)\n",
    "    q2 = gru(embedded_sequences_2)\n",
    "    q1 = Lambda(lambda x:K.reshape(x,(-1,25,128)))(q1)\n",
    "    q2 = Lambda(lambda x:K.reshape(x,(-1,25,128)))(q2)\n",
    "    #用类似TextCNN的思路构建不同卷积核的特征，两个句子共用同样的卷积层\n",
    "    kernel_size = [2,3,4,5]\n",
    "    conv_concat = []\n",
    "    for kernel in kernel_size:\n",
    "        conv = Conv1D(32,kernel_size=kernel,activation='relu',padding='same')\n",
    "        q1_conv = conv(q1)\n",
    "        q1_maxp = MaxPool1D(pool_size=25)(q1_conv)\n",
    "#         q1_meanp = MeanPool1D(pool_size=25)(q1_conv)\n",
    "        q2_conv = conv(q2)\n",
    "        q2_maxp = MaxPool1D(pool_size=25)(q2_conv)\n",
    "#         q1_meanp = MeanPool1D(pool_size=25)(q1_conv)\n",
    "        conv_concat.append(Concatenate()([q1_maxp,q2_maxp]))\n",
    "    conv = Concatenate()(conv_concat)\n",
    "    merged = Dropout(0.2)(Flatten()(conv))\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dense(256, activation='relu')(merged)\n",
    "    merged = Dropout(0.2)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    preds = Dense(1, activation='sigmoid')(merged)\n",
    "    model = Model(inputs=[sequence_1_input, sequence_2_input],outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "            optimizer='nadam',\n",
    "            metrics=['acc'])\n",
    "    hist = model.fit([Q1_train, Q2_train], train['label'].values[tr],validation_data=([Q1_test, Q2_test], train['label'].values[va]),epochs=10, batch_size=1024, shuffle=True)\n",
    "    pred = model.predict([q1_data_te,q2_data_te],batch_size=1024)\n",
    "    avg = [v[0] for v in pred]\n",
    "    re.append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(predict_prob):\n",
    "    with open('submission.csv', 'w') as file:\n",
    "        file.write(str('y_pre') + '\\n')\n",
    "        for line in predict_prob:\n",
    "            file.write(str(line) + '\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float32' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bf7b2026dc2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmake_submission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-b8cba19ef823>\u001b[0m in \u001b[0;36mmake_submission\u001b[1;34m(predict_prob)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'submission.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'y_pre'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpredict_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m             \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float32' object is not iterable"
     ]
    }
   ],
   "source": [
    "make_submission(np.mean(re))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29814544, 0.28900117, 0.2894758 , 0.28705674, 0.3016859 ,\n",
       "       0.27341452, 0.29639283, 0.2876494 , 0.2884639 , 0.29671726],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(re).mean(axis=0).shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
