{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/\"\n",
    "TRAIN_PATH = DATA_PATH + \"train.csv\"\n",
    "TEST_PATH = DATA_PATH + \"test.csv\"\n",
    "WORD_EMBED_PATH = DATA_PATH + \"word_embed.txt\"\n",
    "CHAR_EMBED_PATH = DATA_PATH + \"char_embed.txt\"\n",
    "QUEST_PATH = DATA_PATH + \"question.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(TRAIN_PATH)\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "question_data = pd.read_csv(QUEST_PATH)\n",
    "word_embedding_data = pd.read_csv(WORD_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "char_embedding_data = pd.read_csv(CHAR_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "\n",
    "question_data[\"words\"] = question_data[\"words\"].str.split(\" \")\n",
    "question_data[\"chars\"] = question_data[\"chars\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_WORD_NUMS = 10000\n",
    "\n",
    "word_tokenizer = Tokenizer(MAX_WORD_NUMS)\n",
    "word_tokenizer.fit_on_texts(question_data[\"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_data = np.concatenate(\n",
    "    (\n",
    "        np.zeros(shape=(1, word_embedding_data.shape[1]), dtype=np.float64),\n",
    "        word_embedding_data.loc[list(word_tokenizer.word_index.keys())[:MAX_WORD_NUMS]].values\n",
    "    ),\n",
    "    axis=0\n",
    ")\n",
    "word_embedding_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "WORD_SEQ_LEN = 30\n",
    "\n",
    "def gen_data(data):\n",
    "    seq_word1 = word_tokenizer.texts_to_sequences(data.merge(question_data, how=\"left\", left_on=\"q1\", right_on=\"qid\")[\"words\"])\n",
    "    seq_word2 = word_tokenizer.texts_to_sequences(data.merge(question_data, how=\"left\", left_on=\"q2\", right_on=\"qid\")[\"words\"])\n",
    "    return pad_sequences(seq_word1, maxlen=WORD_SEQ_LEN, padding=\"pre\",truncating=\"pre\"), \\\n",
    "        pad_sequences(seq_word2, maxlen=WORD_SEQ_LEN, padding=\"pre\",truncating=\"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word1, word2 = gen_data(train_data)\n",
    "test_word1, test_word2 = gen_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dropout, BatchNormalization, Dense\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 203508 samples, validate on 50878 samples\n",
      "Epoch 1/100\n",
      "203508/203508 [==============================] - 38s 186us/step - loss: 0.5248 - acc: 0.7526 - val_loss: 0.4054 - val_acc: 0.8111\n",
      "Epoch 2/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.4012 - acc: 0.8155 - val_loss: 0.3497 - val_acc: 0.8405\n",
      "Epoch 3/100\n",
      "203508/203508 [==============================] - 37s 181us/step - loss: 0.3727 - acc: 0.8295 - val_loss: 0.3295 - val_acc: 0.8522\n",
      "Epoch 4/100\n",
      "203508/203508 [==============================] - 37s 181us/step - loss: 0.3590 - acc: 0.8381 - val_loss: 0.3425 - val_acc: 0.8484\n",
      "Epoch 5/100\n",
      "203508/203508 [==============================] - 37s 181us/step - loss: 0.3490 - acc: 0.8423 - val_loss: 0.3187 - val_acc: 0.8593\n",
      "Epoch 6/100\n",
      "203508/203508 [==============================] - 37s 181us/step - loss: 0.3434 - acc: 0.8452 - val_loss: 0.3319 - val_acc: 0.8529\n",
      "Epoch 7/100\n",
      "203508/203508 [==============================] - 37s 181us/step - loss: 0.3384 - acc: 0.8482 - val_loss: 0.3072 - val_acc: 0.8635\n",
      "Epoch 8/100\n",
      "203508/203508 [==============================] - 37s 181us/step - loss: 0.3335 - acc: 0.8506 - val_loss: 0.3110 - val_acc: 0.8606\n",
      "Epoch 9/100\n",
      "203508/203508 [==============================] - 37s 181us/step - loss: 0.3302 - acc: 0.8524 - val_loss: 0.2966 - val_acc: 0.8677\n",
      "Epoch 10/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.3375 - acc: 0.8492 - val_loss: 0.3076 - val_acc: 0.8632\n",
      "Epoch 11/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.3309 - acc: 0.8527 - val_loss: 0.3138 - val_acc: 0.8623\n",
      "Epoch 12/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.3255 - acc: 0.8546 - val_loss: 0.2951 - val_acc: 0.8711\n",
      "Epoch 13/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.3236 - acc: 0.8566 - val_loss: 0.2946 - val_acc: 0.8705\n",
      "Epoch 14/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.3230 - acc: 0.8565 - val_loss: 0.3017 - val_acc: 0.8655\n",
      "Epoch 15/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.3205 - acc: 0.8571 - val_loss: 0.3104 - val_acc: 0.8632\n",
      "Epoch 16/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.3188 - acc: 0.8576 - val_loss: 0.3037 - val_acc: 0.8657\n",
      "Epoch 17/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.3095 - acc: 0.8630 - val_loss: 0.2847 - val_acc: 0.8739\n",
      "Epoch 18/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.3036 - acc: 0.8659 - val_loss: 0.2794 - val_acc: 0.8769\n",
      "Epoch 19/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2981 - acc: 0.8687 - val_loss: 0.2801 - val_acc: 0.8765\n",
      "Epoch 20/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2949 - acc: 0.8702 - val_loss: 0.2763 - val_acc: 0.8774\n",
      "Epoch 21/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2918 - acc: 0.8717 - val_loss: 0.2750 - val_acc: 0.8781\n",
      "Epoch 22/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2884 - acc: 0.8734 - val_loss: 0.2794 - val_acc: 0.8757\n",
      "Epoch 23/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2858 - acc: 0.8746 - val_loss: 0.2843 - val_acc: 0.8740\n",
      "Epoch 24/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2844 - acc: 0.8749 - val_loss: 0.2739 - val_acc: 0.8794\n",
      "Epoch 25/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2813 - acc: 0.8765 - val_loss: 0.2712 - val_acc: 0.8803\n",
      "Epoch 26/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2782 - acc: 0.8779 - val_loss: 0.2707 - val_acc: 0.8824\n",
      "Epoch 27/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2775 - acc: 0.8783 - val_loss: 0.2712 - val_acc: 0.8809\n",
      "Epoch 28/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2758 - acc: 0.8792 - val_loss: 0.2710 - val_acc: 0.8813\n",
      "Epoch 29/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2750 - acc: 0.8791 - val_loss: 0.2669 - val_acc: 0.8838\n",
      "Epoch 30/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2742 - acc: 0.8801 - val_loss: 0.2663 - val_acc: 0.8837\n",
      "Epoch 31/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2725 - acc: 0.8811 - val_loss: 0.2653 - val_acc: 0.8852\n",
      "Epoch 32/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2704 - acc: 0.8822 - val_loss: 0.2673 - val_acc: 0.8836\n",
      "Epoch 33/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2694 - acc: 0.8825 - val_loss: 0.2650 - val_acc: 0.8845\n",
      "Epoch 34/100\n",
      "203508/203508 [==============================] - 37s 179us/step - loss: 0.2680 - acc: 0.8834 - val_loss: 0.2599 - val_acc: 0.8870\n",
      "Epoch 35/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2683 - acc: 0.8830 - val_loss: 0.2619 - val_acc: 0.8849\n",
      "Epoch 36/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2674 - acc: 0.8836 - val_loss: 0.2619 - val_acc: 0.8862\n",
      "Epoch 37/100\n",
      "203508/203508 [==============================] - 37s 180us/step - loss: 0.2658 - acc: 0.8843 - val_loss: 0.2679 - val_acc: 0.8835\n",
      "Epoch 38/100\n",
      " 36864/203508 [====>.........................] - ETA: 26s - loss: 0.2563 - acc: 0.8888"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1a921d2abd45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdev_word1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_word2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlr_reducer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_point\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2665\u001b[0m                     \u001b[1;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[1;32m-> 2667\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2647\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2648\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2649\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2650\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_word1, dev_word1, train_word2, dev_word2, train_y, dev_y = train_test_split(\n",
    "    word1, word2, train_data[\"label\"].values,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "train_word1.shape, dev_word1.shape\n",
    "\n",
    "lstm_size = 256\n",
    "drop_rate = 0.5\n",
    "dense_size = 300\n",
    "num_epoch = 100\n",
    "batch_size = 2048\n",
    "\n",
    "word_input1 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "word_input2 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    input_dim=word_embedding_data.shape[0],\n",
    "    output_dim=word_embedding_data.shape[1],\n",
    "    weights=[word_embedding_data],\n",
    "    input_length=WORD_SEQ_LEN,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "word_vector1 = embedding_layer(word_input1)\n",
    "word_vector2 = embedding_layer(word_input2)\n",
    "\n",
    "lstm_layer = LSTM(lstm_size, dropout=drop_rate, recurrent_dropout=drop_rate)\n",
    "word1 = lstm_layer(word_vector1)\n",
    "word2 = lstm_layer(word_vector2)\n",
    "word = concatenate([word1, word2])\n",
    "\n",
    "x = Dropout(drop_rate)(word)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(dense_size, activation=\"relu\")(x)\n",
    "x = Dropout(drop_rate)(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "pred = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=[word_input1, word_input2], outputs=pred)\n",
    "model.compile(\n",
    "    optimizer=Adam(lr=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"acc\"]\n",
    ")\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_loss', mode=\"min\", factor=0.33, patience=3, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(\"val_loss\", patience=6)\n",
    "check_point = ModelCheckpoint(\"./log/lstm.{epoch:02d}_{val_loss:.3f}.hdf5\", monitor=\"val_loss\", save_best_only=True, save_weights_only=True)\n",
    "\n",
    "train_res = model.fit(\n",
    "    x=[train_word1, train_word2],\n",
    "    y=train_y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epoch,\n",
    "    validation_data=([dev_word1, dev_word2], dev_y),\n",
    "    shuffle=True,\n",
    "    callbacks=[lr_reducer, early_stop, check_point]\n",
    ")\n",
    "\n",
    "print(\"load model %s\" % (glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"),))\n",
    "model.load_weights(glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"))\n",
    "\n",
    "test_pred = model.predict([test_word1, test_word2], batch_size=batch_size)\n",
    "pd.DataFrame(test_pred, columns=[\"y_pre\"]).to_csv(\"./result/pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Embedding, LSTM, Dropout, BatchNormalization, Dense, Lambda\n",
    "# from keras.layers.merge import concatenate\n",
    "# from keras.models import Model\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# lstm_size = 256\n",
    "# drop_rate = 0.3\n",
    "# dense_size = 300\n",
    "# num_epoch = 100\n",
    "# batch_size = 2048\n",
    "\n",
    "# label = train_data[\"label\"].values\n",
    "\n",
    "# preds = []\n",
    "\n",
    "# for i, (train_index, dev_index) in enumerate(StratifiedKFold(n_splits=10).split(X=word1, y=label)):\n",
    "#     train_word1, train_word2, train_y = word1[train_index, :], word2[train_index, :], label[train_index]\n",
    "#     dev_word1, dev_word2, dev_y = word1[dev_index, :], word2[dev_index, :], label[dev_index]\n",
    "    \n",
    "#     word_input1 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "#     word_input2 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "\n",
    "#     embedding_layer = Embedding(\n",
    "#         input_dim=word_embedding_data.shape[0],\n",
    "#         output_dim=word_embedding_data.shape[1],\n",
    "#         weights=[word_embedding_data],\n",
    "#         input_length=WORD_SEQ_LEN,\n",
    "#         trainable=False\n",
    "#     )\n",
    "\n",
    "#     word_vector1 = embedding_layer(word_input1)\n",
    "#     word_vector2 = embedding_layer(word_input2)\n",
    "\n",
    "#     lstm_layer = LSTM(lstm_size, dropout=drop_rate, recurrent_dropout=drop_rate)\n",
    "#     word_output1 = lstm_layer(word_vector1)\n",
    "#     word_output2 = lstm_layer(word_vector2)\n",
    "#     word = concatenate([word_output1, word_output2])\n",
    "    \n",
    "# #     diff = Lambda(lambda x: x[0] - x[1], output_shape=(lstm_size,))([word_output1, word_output2])\n",
    "# #     mult = Lambda(lambda x: x[0] * x[1], output_shape=(lstm_size,))([word_output1, word_output2])\n",
    "# #     word = concatenate([diff, mult])\n",
    "\n",
    "#     x = Dropout(drop_rate)(word)\n",
    "#     x = BatchNormalization()(x)\n",
    "\n",
    "#     x = Dense(dense_size, activation=\"relu\")(x)\n",
    "#     x = Dropout(drop_rate)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "\n",
    "#     pred = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "#     model = Model(inputs=[word_input1, word_input2], outputs=pred)\n",
    "#     model.compile(optimizer=\"nadam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "#     early_stop = EarlyStopping(\"val_loss\", patience=10)\n",
    "#     check_point = ModelCheckpoint(\n",
    "#         \"./log/lstm_%02d.{epoch:02d}_{val_loss:.3f}.hdf5\" % (i + 1),\n",
    "#         monitor=\"val_loss\",\n",
    "#         save_best_only=True,\n",
    "#         save_weights_only=True\n",
    "#     )\n",
    "\n",
    "#     train_res = model.fit(\n",
    "#         x=[train_word1, train_word2],\n",
    "#         y=train_y,\n",
    "#         batch_size=batch_size,\n",
    "#         epochs=num_epoch,\n",
    "#         validation_data=([dev_word1, dev_word2], dev_y),\n",
    "#         shuffle=True,\n",
    "#         callbacks=[early_stop, check_point]\n",
    "#     )\n",
    "    \n",
    "#     print(\"load model %s\" % (glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"),))\n",
    "#     model.load_weights(glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"))\n",
    "\n",
    "#     test_pred = model.predict([test_word1, test_word2], batch_size=batch_size)\n",
    "#     preds.append(pd.DataFrame(test_pred, columns=[\"y_pre\"]))\n",
    "\n",
    "# pd.DataFrame(pd.concat(preds, axis=1).mean(axis=1), columns=[\"y_pre\"]).to_csv(\"./result/pred.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
