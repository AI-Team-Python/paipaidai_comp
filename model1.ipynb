{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/\"\n",
    "TRAIN_PATH = DATA_PATH + \"train.csv\"\n",
    "TEST_PATH = DATA_PATH + \"test.csv\"\n",
    "WORD_EMBED_PATH = DATA_PATH + \"word_embed.txt\"\n",
    "CHAR_EMBED_PATH = DATA_PATH + \"char_embed.txt\"\n",
    "QUEST_PATH = DATA_PATH + \"question.csv\"\n",
    "\n",
    "SEED = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(TRAIN_PATH)\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "question_data = pd.read_csv(QUEST_PATH)\n",
    "word_embedding_data = pd.read_csv(WORD_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "char_embedding_data = pd.read_csv(CHAR_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "\n",
    "question_data[\"words\"] = question_data[\"words\"].str.split(\" \")\n",
    "question_data[\"chars\"] = question_data[\"chars\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(question_data[\"words\"])\n",
    "word_index = word_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_SEQ_LEN = 8\n",
    "\n",
    "word_texts_q1 = train_data.merge(question_data, how=\"left\", left_on=\"q1\", right_on=\"qid\")[\"words\"]\n",
    "word_texts_q2 = train_data.merge(question_data, how=\"left\", left_on=\"q2\", right_on=\"qid\")[\"words\"]\n",
    "\n",
    "word_seq_q1 = word_tokenizer.texts_to_sequences(word_texts_q1)\n",
    "pad_word_seq_q1 = pad_sequences(word_seq_q1, maxlen=WORD_SEQ_LEN)\n",
    "word_seq_q2 = word_tokenizer.texts_to_sequences(word_texts_q2)\n",
    "pad_word_seq_q2 = pad_sequences(word_seq_q2, maxlen=WORD_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train_data[\"label\"].values\n",
    "train_seq1, dev_seq1, train_seq2, dev_seq2, train_y, dev_y = train_test_split(pad_word_seq_q1, pad_word_seq_q2, label, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_data = word_embedding_data.reindex(list(word_index.keys())).values\n",
    "word_embedding_data = np.concatenate((np.zeros((1, word_embedding_data.shape[1]), dtype=np.float32), word_embedding_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'output_layer/batch_normalization/AssignMovingAvg:0' shape=(256,) dtype=float64_ref>, <tf.Tensor 'output_layer/batch_normalization/AssignMovingAvg_1:0' shape=(256,) dtype=float64_ref>, <tf.Tensor 'output_layer/batch_normalization_1/AssignMovingAvg:0' shape=(128,) dtype=float64_ref>, <tf.Tensor 'output_layer/batch_normalization_1/AssignMovingAvg_1:0' shape=(128,) dtype=float64_ref>]\n",
      "[epoch 1 step 1] train loss: 0.8574914609992914, dev loss: 1.0217921001652481\n",
      "[epoch 1 step 2] train loss: 0.9848272175404253, dev loss: 0.8940810427182642\n",
      "[epoch 1 step 3] train loss: 0.8662045926183246, dev loss: 0.7703268891158105\n",
      "[epoch 1 step 4] train loss: 0.7687002173375244, dev loss: 0.7474213034287281\n",
      "[epoch 1 step 5] train loss: 0.746488186044315, dev loss: 0.7216810961452014\n",
      "[epoch 1 step 6] train loss: 0.7255934421976289, dev loss: 0.7185147816864392\n"
     ]
    }
   ],
   "source": [
    "KEEP_PROB = 0.8\n",
    "NUM_UNITS = 256\n",
    "DENSE_SIZE = 128\n",
    "INIT_STD = 0.1\n",
    "INIT_LEARN = 0.5\n",
    "\n",
    "NUM_EPOCH = 3\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "num_train = len(train_seq1)\n",
    "num_batches = num_train // BATCH_SIZE\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope(\"input_layer\"):\n",
    "        word1 = tf.placeholder(shape=(None, WORD_SEQ_LEN), dtype=tf.int64, name=\"word1_input\")\n",
    "        word2 = tf.placeholder(shape=(None, WORD_SEQ_LEN), dtype=tf.int64, name=\"word2_input\")\n",
    "        y = tf.placeholder(shape=(None,), dtype=tf.int64, name=\"true_label\")\n",
    "        y_ = tf.cast(y, dtype=tf.float64)\n",
    "        \n",
    "    with tf.variable_scope(\"embedding_layer\"):\n",
    "        word_embedding = tf.Variable(word_embedding_data, trainable=False, name=\"word_embedding\")\n",
    "        word_vector1 = tf.nn.embedding_lookup(word_embedding, word1, name=\"embedding_looking\")\n",
    "        word_vector2 = tf.nn.embedding_lookup(word_embedding, word2, name=\"embedding_looking\")\n",
    "    \n",
    "    with tf.variable_scope(\"lstm_layer\"):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(num_units=NUM_UNITS, name=\"word_lstm\")\n",
    "        lstm_output, _ = tf.nn.dynamic_rnn(cell, word_vector1, dtype=tf.float64)\n",
    "        final_output = lstm_output[:, -1, :]\n",
    "    \n",
    "    with tf.variable_scope(\"output_layer\"):\n",
    "        input_norm = tf.layers.batch_normalization(final_output, axis=1, training=True)\n",
    "        dense = tf.layers.dense(input_norm, DENSE_SIZE, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=tf.random_normal_initializer(stddev=INIT_STD),\n",
    "                                 bias_initializer=tf.zeros_initializer())\n",
    "        dense_norm = tf.layers.batch_normalization(dense, axis=1, training=True)\n",
    "        pred = tf.layers.dense(dense_norm, 1, activation=tf.nn.sigmoid,\n",
    "                               kernel_initializer=tf.random_normal_initializer(stddev=INIT_STD),\n",
    "                               bias_initializer=tf.zeros_initializer())\n",
    "    \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        loss = -tf.reduce_mean(y_ * tf.log(pred) + (1 - y_) * tf.log(1 - pred))\n",
    "    \n",
    "    with tf.variable_scope(\"train\"):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(INIT_LEARN)\n",
    "        grads, variables = zip(*optimizer.compute_gradients(loss, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)))\n",
    "        \n",
    "        global_step = tf.Variable(0, dtype=tf.int64, trainable=False, name=\"global_step\")\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        print(update_ops)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.apply_gradients(zip(grads, variables), global_step=global_step)\n",
    "        \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run([tf.global_variables_initializer()])\n",
    "    \n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        epoch_index = np.random.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_index = epoch_index[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n",
    "            data_w1 = train_seq1[batch_index, :]\n",
    "            data_w2 = train_seq2[batch_index, :]\n",
    "            data_y = train_y[batch_index]\n",
    "            \n",
    "            train_feed = {word1: data_w1, word2: data_w2, y: data_y}\n",
    "            _, train_loss, cur_step = sess.run([train_op, loss, global_step], feed_dict=train_feed)\n",
    "            dev_feed = {word1: dev_seq1, word2: dev_seq2, y: dev_y}\n",
    "            dev_loss = sess.run(loss, feed_dict=dev_feed)\n",
    "            \n",
    "            print(f\"[epoch {epoch + 1} step {cur_step}] train loss: {train_loss}, dev loss: {dev_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
