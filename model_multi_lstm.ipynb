{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAST: 0.217161\n",
    "# BEST: 0.217487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/\"\n",
    "TRAIN_PATH = DATA_PATH + \"train.csv\"\n",
    "TEST_PATH = DATA_PATH + \"test.csv\"\n",
    "WORD_EMBED_PATH = DATA_PATH + \"word_embed.txt\"\n",
    "CHAR_EMBED_PATH = DATA_PATH + \"char_embed.txt\"\n",
    "QUEST_PATH = DATA_PATH + \"question.csv\"\n",
    "\n",
    "train_data = pd.read_csv(TRAIN_PATH)\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "question_data = pd.read_csv(QUEST_PATH)\n",
    "word_embedding_data = pd.read_csv(WORD_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "char_embedding_data = pd.read_csv(CHAR_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "\n",
    "question_data[\"words\"] = question_data[\"words\"].str.split(\" \")\n",
    "question_data[\"chars\"] = question_data[\"chars\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10001, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_WORD_NUMS = 10000\n",
    "\n",
    "word_tokenizer = Tokenizer(MAX_WORD_NUMS)\n",
    "word_tokenizer.fit_on_texts(question_data[\"words\"])\n",
    "\n",
    "word_embedding_data = np.concatenate(\n",
    "    (\n",
    "        np.zeros(shape=(1, word_embedding_data.shape[1]), dtype=np.float64),\n",
    "        word_embedding_data.loc[list(word_tokenizer.word_index.keys())[:MAX_WORD_NUMS]].values\n",
    "    ),\n",
    "    axis=0\n",
    ")\n",
    "word_embedding_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((254386, 30), (254386, 30), (172956, 30), (172956, 30))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "WORD_SEQ_LEN = 30\n",
    "\n",
    "def gen_data(data):\n",
    "    seq_word1 = word_tokenizer.texts_to_sequences(data.merge(question_data, how=\"left\", left_on=\"q1\", right_on=\"qid\")[\"words\"])\n",
    "    seq_word2 = word_tokenizer.texts_to_sequences(data.merge(question_data, how=\"left\", left_on=\"q2\", right_on=\"qid\")[\"words\"])\n",
    "    return pad_sequences(seq_word1, maxlen=WORD_SEQ_LEN, padding=\"pre\",truncating=\"pre\"), \\\n",
    "        pad_sequences(seq_word2, maxlen=WORD_SEQ_LEN, padding=\"pre\",truncating=\"pre\")\n",
    "\n",
    "word1, word2 = gen_data(train_data)\n",
    "test_word1, test_word2 = gen_data(test_data)\n",
    "\n",
    "word1.shape, word2.shape, test_word1.shape, test_word2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = train_data[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dropout, Dense, BatchNormalization, K, LSTM\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "DROP_RATE = 0.25\n",
    "LSTM_SIZE_1 = 256\n",
    "LSTM_SIZE_2 = 256\n",
    "DENSE_SIZE = 300\n",
    "BATCH_SIZE = 2048\n",
    "NUM_EPOCHES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_word1, dev_word1, train_word2, dev_word2, train_y, dev_y = train_test_split(\n",
    "#     word1, word2, train_data[\"label\"].values,\n",
    "#     test_size=0.2\n",
    "# )\n",
    "\n",
    "# word_input1 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "# word_input2 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "\n",
    "# embedding_layer = Embedding(\n",
    "#     input_dim=word_embedding_data.shape[0],\n",
    "#     output_dim=word_embedding_data.shape[1],\n",
    "#     weights=[word_embedding_data],\n",
    "#     input_length=WORD_SEQ_LEN,\n",
    "#     trainable=False\n",
    "# )\n",
    "\n",
    "# word_vector1 = embedding_layer(word_input1)\n",
    "# word_vector2 = embedding_layer(word_input2)\n",
    "\n",
    "# lstm_layer1 = LSTM(LSTM_SIZE_1, dropout=DROP_RATE, recurrent_dropout=DROP_RATE, return_sequences=True)\n",
    "# word_first_1 = lstm_layer1(word_vector1)\n",
    "# word_first_1 = Dropout(DROP_RATE)(word_first_1)\n",
    "# word_first_2 = lstm_layer1(word_vector2)\n",
    "# word_first_2 = Dropout(DROP_RATE)(word_first_2)\n",
    "\n",
    "# lstm_layer2 = LSTM(LSTM_SIZE_2, dropout=DROP_RATE, recurrent_dropout=DROP_RATE, return_sequences=False)\n",
    "# word_second_1 = lstm_layer2(word_first_1)\n",
    "# word_second_2 = lstm_layer2(word_first_2)\n",
    "\n",
    "# x = concatenate([word_second_1, word_second_2])\n",
    "# x = Dropout(DROP_RATE)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "\n",
    "# x = Dense(DENSE_SIZE, activation=\"relu\")(x)\n",
    "# x = Dropout(DROP_RATE)(x)\n",
    "# x = BatchNormalization()(x)\n",
    "\n",
    "# pred = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# model = Model(inputs=[word_input1, word_input2], outputs=pred)\n",
    "# model.compile(\n",
    "#     optimizer=\"nadam\",\n",
    "#     loss=\"binary_crossentropy\",\n",
    "#     metrics=[\"acc\"]\n",
    "# )\n",
    "\n",
    "# early_stopping = EarlyStopping(\"val_loss\", patience=10)\n",
    "# check_point = ModelCheckpoint(\n",
    "#     \"./log/%s.multi_lstm.{epoch:03d}.hdf5\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "#     monitor=\"val_loss\",\n",
    "#     save_best_only=True,\n",
    "#     save_weights_only=True\n",
    "# )\n",
    "\n",
    "# train_res = model.fit(\n",
    "#     x=[train_word1, train_word2],\n",
    "#     y=train_y,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     epochs=NUM_EPOCHES,\n",
    "#     validation_data=([dev_word1, dev_word2], dev_y),\n",
    "#     shuffle=True,\n",
    "#     callbacks=[early_stopping, check_point]\n",
    "# )\n",
    "\n",
    "# print(\"load model %s\" % (glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"),))\n",
    "# model.load_weights(glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"))\n",
    "\n",
    "# test_pred = model.predict([test_word1, test_word2], batch_size=BATCH_SIZE)\n",
    "# pd.DataFrame(test_pred, columns=[\"y_pre\"]).to_csv(\"./result/pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 228946 samples, validate on 25440 samples\n",
      "Epoch 1/50\n",
      "228946/228946 [==============================] - 112s 490us/step - loss: 0.4825 - acc: 0.7704 - val_loss: 0.3700 - val_acc: 0.8417\n",
      "Epoch 2/50\n",
      "228946/228946 [==============================] - 109s 476us/step - loss: 0.3555 - acc: 0.8402 - val_loss: 0.3143 - val_acc: 0.8664\n",
      "Epoch 3/50\n",
      "228946/228946 [==============================] - 110s 481us/step - loss: 0.3103 - acc: 0.8635 - val_loss: 0.2773 - val_acc: 0.8804\n",
      "Epoch 4/50\n",
      "228946/228946 [==============================] - 110s 482us/step - loss: 0.2797 - acc: 0.8784 - val_loss: 0.2523 - val_acc: 0.8926\n",
      "Epoch 5/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.2590 - acc: 0.8886 - val_loss: 0.2428 - val_acc: 0.8953\n",
      "Epoch 6/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.2443 - acc: 0.8956 - val_loss: 0.2340 - val_acc: 0.8997\n",
      "Epoch 7/50\n",
      "228946/228946 [==============================] - 109s 478us/step - loss: 0.2306 - acc: 0.9015 - val_loss: 0.2230 - val_acc: 0.9058\n",
      "Epoch 8/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.2206 - acc: 0.9073 - val_loss: 0.2145 - val_acc: 0.9110\n",
      "Epoch 9/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.2125 - acc: 0.9111 - val_loss: 0.2241 - val_acc: 0.9097\n",
      "Epoch 10/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.2056 - acc: 0.9138 - val_loss: 0.2130 - val_acc: 0.9115\n",
      "Epoch 11/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1999 - acc: 0.9162 - val_loss: 0.2122 - val_acc: 0.9136\n",
      "Epoch 12/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1951 - acc: 0.9185 - val_loss: 0.2071 - val_acc: 0.9157\n",
      "Epoch 13/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1894 - acc: 0.9213 - val_loss: 0.2048 - val_acc: 0.9174\n",
      "Epoch 14/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1846 - acc: 0.9228 - val_loss: 0.2091 - val_acc: 0.9164\n",
      "Epoch 15/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1814 - acc: 0.9248 - val_loss: 0.2037 - val_acc: 0.9190\n",
      "Epoch 16/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1785 - acc: 0.9264 - val_loss: 0.2011 - val_acc: 0.9195\n",
      "Epoch 17/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1749 - acc: 0.9279 - val_loss: 0.2007 - val_acc: 0.9219\n",
      "Epoch 18/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1726 - acc: 0.9287 - val_loss: 0.2023 - val_acc: 0.9225\n",
      "Epoch 19/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1696 - acc: 0.9300 - val_loss: 0.2041 - val_acc: 0.9208\n",
      "Epoch 20/50\n",
      "228946/228946 [==============================] - 110s 478us/step - loss: 0.1666 - acc: 0.9310 - val_loss: 0.2039 - val_acc: 0.9219\n",
      "Epoch 21/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1658 - acc: 0.9318 - val_loss: 0.2048 - val_acc: 0.9208\n",
      "Epoch 22/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1625 - acc: 0.9329 - val_loss: 0.2011 - val_acc: 0.9228\n",
      "Epoch 23/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1621 - acc: 0.9334 - val_loss: 0.2009 - val_acc: 0.9218\n",
      "Epoch 24/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1606 - acc: 0.9335 - val_loss: 0.2028 - val_acc: 0.9210\n",
      "Epoch 25/50\n",
      "228946/228946 [==============================] - 110s 478us/step - loss: 0.1576 - acc: 0.9356 - val_loss: 0.2055 - val_acc: 0.9208\n",
      "Epoch 26/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1568 - acc: 0.9359 - val_loss: 0.2132 - val_acc: 0.9185\n",
      "Epoch 27/50\n",
      "228946/228946 [==============================] - 110s 479us/step - loss: 0.1546 - acc: 0.9360 - val_loss: 0.2023 - val_acc: 0.9232\n",
      "load model ./log/20180703-074404.multi_lstm.017.hdf5\n",
      "Train on 228946 samples, validate on 25440 samples\n",
      "Epoch 1/50\n",
      "228946/228946 [==============================] - 108s 471us/step - loss: 0.4818 - acc: 0.7716 - val_loss: 0.3868 - val_acc: 0.8403\n",
      "Epoch 2/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.3552 - acc: 0.8415 - val_loss: 0.3049 - val_acc: 0.8694\n",
      "Epoch 3/50\n",
      "228946/228946 [==============================] - 108s 471us/step - loss: 0.3080 - acc: 0.8649 - val_loss: 0.2746 - val_acc: 0.8822\n",
      "Epoch 4/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.2768 - acc: 0.8794 - val_loss: 0.2496 - val_acc: 0.8908\n",
      "Epoch 5/50\n",
      "228946/228946 [==============================] - 108s 471us/step - loss: 0.2580 - acc: 0.8887 - val_loss: 0.2468 - val_acc: 0.8930\n",
      "Epoch 6/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.2408 - acc: 0.8969 - val_loss: 0.2285 - val_acc: 0.9020\n",
      "Epoch 7/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.2307 - acc: 0.9022 - val_loss: 0.2321 - val_acc: 0.9022\n",
      "Epoch 8/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.2202 - acc: 0.9071 - val_loss: 0.2187 - val_acc: 0.9092\n",
      "Epoch 9/50\n",
      "228946/228946 [==============================] - 108s 471us/step - loss: 0.2127 - acc: 0.9106 - val_loss: 0.2144 - val_acc: 0.9088\n",
      "Epoch 10/50\n",
      "228946/228946 [==============================] - 108s 470us/step - loss: 0.2061 - acc: 0.9138 - val_loss: 0.2141 - val_acc: 0.9121\n",
      "Epoch 11/50\n",
      "228946/228946 [==============================] - 106s 464us/step - loss: 0.2006 - acc: 0.9162 - val_loss: 0.2101 - val_acc: 0.9118\n",
      "Epoch 12/50\n",
      "228946/228946 [==============================] - 108s 470us/step - loss: 0.1939 - acc: 0.9191 - val_loss: 0.2168 - val_acc: 0.9118\n",
      "Epoch 13/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.1891 - acc: 0.9214 - val_loss: 0.2054 - val_acc: 0.9162\n",
      "Epoch 14/50\n",
      "228946/228946 [==============================] - 108s 471us/step - loss: 0.1845 - acc: 0.9235 - val_loss: 0.2066 - val_acc: 0.9153\n",
      "Epoch 15/50\n",
      "228946/228946 [==============================] - 108s 471us/step - loss: 0.1809 - acc: 0.9248 - val_loss: 0.2019 - val_acc: 0.9182\n",
      "Epoch 16/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.1779 - acc: 0.9265 - val_loss: 0.2049 - val_acc: 0.9174\n",
      "Epoch 17/50\n",
      "228946/228946 [==============================] - 108s 471us/step - loss: 0.1745 - acc: 0.9280 - val_loss: 0.2075 - val_acc: 0.9181\n",
      "Epoch 18/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.1724 - acc: 0.9286 - val_loss: 0.1965 - val_acc: 0.9224\n",
      "Epoch 19/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.1692 - acc: 0.9297 - val_loss: 0.2057 - val_acc: 0.9216\n",
      "Epoch 20/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.1683 - acc: 0.9305 - val_loss: 0.2070 - val_acc: 0.9192\n",
      "Epoch 21/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.1658 - acc: 0.9317 - val_loss: 0.1996 - val_acc: 0.9203\n",
      "Epoch 22/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.1643 - acc: 0.9322 - val_loss: 0.1980 - val_acc: 0.9213\n",
      "Epoch 23/50\n",
      "228946/228946 [==============================] - 107s 469us/step - loss: 0.1620 - acc: 0.9335 - val_loss: 0.1970 - val_acc: 0.9223\n",
      "Epoch 24/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.1591 - acc: 0.9345 - val_loss: 0.2038 - val_acc: 0.9186\n",
      "Epoch 25/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.1568 - acc: 0.9351 - val_loss: 0.1975 - val_acc: 0.9221\n",
      "Epoch 26/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.1560 - acc: 0.9354 - val_loss: 0.2024 - val_acc: 0.9204\n",
      "Epoch 27/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.1544 - acc: 0.9364 - val_loss: 0.1989 - val_acc: 0.9238\n",
      "Epoch 28/50\n",
      "228946/228946 [==============================] - 108s 472us/step - loss: 0.1537 - acc: 0.9370 - val_loss: 0.2022 - val_acc: 0.9234\n",
      "load model ./log/20180703-083358.multi_lstm.018.hdf5\n",
      "Train on 228947 samples, validate on 25439 samples\n",
      "Epoch 1/50\n",
      "228947/228947 [==============================] - 113s 493us/step - loss: 0.4847 - acc: 0.7698 - val_loss: 0.3792 - val_acc: 0.8409\n",
      "Epoch 2/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.3566 - acc: 0.8397 - val_loss: 0.3203 - val_acc: 0.8658\n",
      "Epoch 3/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.3096 - acc: 0.8636 - val_loss: 0.2837 - val_acc: 0.8793\n",
      "Epoch 4/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.2788 - acc: 0.8790 - val_loss: 0.2600 - val_acc: 0.8883\n",
      "Epoch 5/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.2580 - acc: 0.8893 - val_loss: 0.2532 - val_acc: 0.8926\n",
      "Epoch 6/50\n",
      "228947/228947 [==============================] - 111s 484us/step - loss: 0.2424 - acc: 0.8963 - val_loss: 0.2339 - val_acc: 0.9013\n",
      "Epoch 7/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.2285 - acc: 0.9035 - val_loss: 0.2286 - val_acc: 0.9039\n",
      "Epoch 8/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.2192 - acc: 0.9070 - val_loss: 0.2227 - val_acc: 0.9069\n",
      "Epoch 9/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.2100 - acc: 0.9112 - val_loss: 0.2205 - val_acc: 0.9071\n",
      "Epoch 10/50\n",
      "228947/228947 [==============================] - 110s 481us/step - loss: 0.2040 - acc: 0.9144 - val_loss: 0.2234 - val_acc: 0.9099\n",
      "Epoch 11/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1991 - acc: 0.9172 - val_loss: 0.2192 - val_acc: 0.9085\n",
      "Epoch 12/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1924 - acc: 0.9198 - val_loss: 0.2147 - val_acc: 0.9119\n",
      "Epoch 13/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1863 - acc: 0.9224 - val_loss: 0.2108 - val_acc: 0.9130\n",
      "Epoch 14/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1843 - acc: 0.9235 - val_loss: 0.2133 - val_acc: 0.9140\n",
      "Epoch 15/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1800 - acc: 0.9253 - val_loss: 0.2136 - val_acc: 0.9145\n",
      "Epoch 16/50\n",
      "228947/228947 [==============================] - 110s 482us/step - loss: 0.1763 - acc: 0.9267 - val_loss: 0.2136 - val_acc: 0.9165\n",
      "Epoch 17/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1738 - acc: 0.9283 - val_loss: 0.2156 - val_acc: 0.9140\n",
      "Epoch 18/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1708 - acc: 0.9294 - val_loss: 0.2108 - val_acc: 0.9183\n",
      "Epoch 19/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1685 - acc: 0.9303 - val_loss: 0.2112 - val_acc: 0.9161\n",
      "Epoch 20/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1659 - acc: 0.9315 - val_loss: 0.2099 - val_acc: 0.9180\n",
      "Epoch 21/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1642 - acc: 0.9322 - val_loss: 0.2084 - val_acc: 0.9182\n",
      "Epoch 22/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1615 - acc: 0.9336 - val_loss: 0.2135 - val_acc: 0.9182\n",
      "Epoch 23/50\n",
      "228947/228947 [==============================] - 110s 480us/step - loss: 0.1595 - acc: 0.9346 - val_loss: 0.2102 - val_acc: 0.9174\n",
      "Epoch 24/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1584 - acc: 0.9351 - val_loss: 0.2118 - val_acc: 0.9195\n",
      "Epoch 25/50\n",
      "228947/228947 [==============================] - 111s 484us/step - loss: 0.1569 - acc: 0.9353 - val_loss: 0.2095 - val_acc: 0.9198\n",
      "Epoch 26/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1546 - acc: 0.9366 - val_loss: 0.2127 - val_acc: 0.9190\n",
      "Epoch 27/50\n",
      "228947/228947 [==============================] - 110s 479us/step - loss: 0.1539 - acc: 0.9365 - val_loss: 0.2131 - val_acc: 0.9202\n",
      "Epoch 28/50\n",
      "228947/228947 [==============================] - 109s 476us/step - loss: 0.1510 - acc: 0.9376 - val_loss: 0.2069 - val_acc: 0.9226\n",
      "Epoch 29/50\n",
      "228947/228947 [==============================] - 110s 482us/step - loss: 0.1504 - acc: 0.9383 - val_loss: 0.2156 - val_acc: 0.9188\n",
      "Epoch 30/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1494 - acc: 0.9388 - val_loss: 0.2065 - val_acc: 0.9220\n",
      "Epoch 31/50\n",
      "228947/228947 [==============================] - 110s 480us/step - loss: 0.1500 - acc: 0.9386 - val_loss: 0.2112 - val_acc: 0.9198\n",
      "Epoch 32/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1482 - acc: 0.9393 - val_loss: 0.2105 - val_acc: 0.9204\n",
      "Epoch 33/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1467 - acc: 0.9400 - val_loss: 0.2154 - val_acc: 0.9182\n",
      "Epoch 34/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1454 - acc: 0.9406 - val_loss: 0.2124 - val_acc: 0.9215\n",
      "Epoch 35/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1436 - acc: 0.9407 - val_loss: 0.2107 - val_acc: 0.9219\n",
      "Epoch 36/50\n",
      "228947/228947 [==============================] - 111s 484us/step - loss: 0.1436 - acc: 0.9408 - val_loss: 0.2177 - val_acc: 0.9203\n",
      "Epoch 37/50\n",
      "228947/228947 [==============================] - 110s 480us/step - loss: 0.1421 - acc: 0.9417 - val_loss: 0.2147 - val_acc: 0.9211\n",
      "Epoch 38/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1426 - acc: 0.9414 - val_loss: 0.2129 - val_acc: 0.9203\n",
      "Epoch 39/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1416 - acc: 0.9421 - val_loss: 0.2213 - val_acc: 0.9200\n",
      "Epoch 40/50\n",
      "228947/228947 [==============================] - 110s 480us/step - loss: 0.1399 - acc: 0.9423 - val_loss: 0.2131 - val_acc: 0.9224\n",
      "load model ./log/20180703-092452.multi_lstm.030.hdf5\n",
      "Train on 228947 samples, validate on 25439 samples\n",
      "Epoch 1/50\n",
      "228947/228947 [==============================] - 114s 499us/step - loss: 0.4843 - acc: 0.7702 - val_loss: 0.3695 - val_acc: 0.8410\n",
      "Epoch 2/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.3562 - acc: 0.8394 - val_loss: 0.3029 - val_acc: 0.8705\n",
      "Epoch 3/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.3091 - acc: 0.8638 - val_loss: 0.2607 - val_acc: 0.8863\n",
      "Epoch 4/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.2786 - acc: 0.8788 - val_loss: 0.2398 - val_acc: 0.8969\n",
      "Epoch 5/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.2579 - acc: 0.8895 - val_loss: 0.2316 - val_acc: 0.9022\n",
      "Epoch 6/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.2429 - acc: 0.8961 - val_loss: 0.2194 - val_acc: 0.9072\n",
      "Epoch 7/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.2305 - acc: 0.9023 - val_loss: 0.2117 - val_acc: 0.9120\n",
      "Epoch 8/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.2205 - acc: 0.9068 - val_loss: 0.2127 - val_acc: 0.9124\n",
      "Epoch 9/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.2134 - acc: 0.9109 - val_loss: 0.2120 - val_acc: 0.9136\n",
      "Epoch 10/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.2054 - acc: 0.9136 - val_loss: 0.2023 - val_acc: 0.9156\n",
      "Epoch 11/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.1996 - acc: 0.9162 - val_loss: 0.2017 - val_acc: 0.9166\n",
      "Epoch 12/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.1946 - acc: 0.9189 - val_loss: 0.2048 - val_acc: 0.9167\n",
      "Epoch 13/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.1898 - acc: 0.9208 - val_loss: 0.2039 - val_acc: 0.9192\n",
      "Epoch 14/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.1845 - acc: 0.9232 - val_loss: 0.1964 - val_acc: 0.9211\n",
      "Epoch 15/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.1815 - acc: 0.9245 - val_loss: 0.2058 - val_acc: 0.9186\n",
      "Epoch 16/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.1780 - acc: 0.9257 - val_loss: 0.1953 - val_acc: 0.9228\n",
      "Epoch 17/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.1749 - acc: 0.9275 - val_loss: 0.2048 - val_acc: 0.9207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.1726 - acc: 0.9282 - val_loss: 0.1944 - val_acc: 0.9226\n",
      "Epoch 19/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.1700 - acc: 0.9303 - val_loss: 0.1988 - val_acc: 0.9233\n",
      "Epoch 20/50\n",
      "228947/228947 [==============================] - 110s 481us/step - loss: 0.1661 - acc: 0.9308 - val_loss: 0.2007 - val_acc: 0.9227\n",
      "Epoch 21/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.1649 - acc: 0.9321 - val_loss: 0.1972 - val_acc: 0.9238\n",
      "Epoch 22/50\n",
      "228947/228947 [==============================] - 111s 484us/step - loss: 0.1635 - acc: 0.9325 - val_loss: 0.1979 - val_acc: 0.9234\n",
      "Epoch 23/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1612 - acc: 0.9336 - val_loss: 0.1979 - val_acc: 0.9245\n",
      "Epoch 24/50\n",
      "228947/228947 [==============================] - 111s 484us/step - loss: 0.1592 - acc: 0.9344 - val_loss: 0.1986 - val_acc: 0.9241\n",
      "Epoch 25/50\n",
      "228947/228947 [==============================] - 111s 483us/step - loss: 0.1576 - acc: 0.9355 - val_loss: 0.2045 - val_acc: 0.9239\n",
      "Epoch 26/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.1553 - acc: 0.9361 - val_loss: 0.2034 - val_acc: 0.9244\n",
      "Epoch 27/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.1560 - acc: 0.9360 - val_loss: 0.2045 - val_acc: 0.9238\n",
      "Epoch 28/50\n",
      "228947/228947 [==============================] - 111s 485us/step - loss: 0.1549 - acc: 0.9361 - val_loss: 0.2011 - val_acc: 0.9259\n",
      "load model ./log/20180703-103906.multi_lstm.018.hdf5\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 115s 503us/step - loss: 0.4865 - acc: 0.7694 - val_loss: 0.3708 - val_acc: 0.8409\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 111s 487us/step - loss: 0.3554 - acc: 0.8411 - val_loss: 0.3046 - val_acc: 0.8681\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 112s 489us/step - loss: 0.3108 - acc: 0.8630 - val_loss: 0.2771 - val_acc: 0.8818\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 112s 488us/step - loss: 0.2798 - acc: 0.8778 - val_loss: 0.2601 - val_acc: 0.8870\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 112s 487us/step - loss: 0.2569 - acc: 0.8895 - val_loss: 0.2535 - val_acc: 0.8923\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 112s 488us/step - loss: 0.2431 - acc: 0.8951 - val_loss: 0.2331 - val_acc: 0.9032\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 112s 487us/step - loss: 0.2304 - acc: 0.9020 - val_loss: 0.2250 - val_acc: 0.9063\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 112s 487us/step - loss: 0.2208 - acc: 0.9067 - val_loss: 0.2202 - val_acc: 0.9091\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 112s 487us/step - loss: 0.2122 - acc: 0.9105 - val_loss: 0.2154 - val_acc: 0.9086\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 112s 488us/step - loss: 0.2045 - acc: 0.9140 - val_loss: 0.2185 - val_acc: 0.9110\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 112s 487us/step - loss: 0.1990 - acc: 0.9171 - val_loss: 0.2106 - val_acc: 0.9135\n",
      "Epoch 12/50\n",
      "228948/228948 [==============================] - 111s 483us/step - loss: 0.1937 - acc: 0.9190 - val_loss: 0.2131 - val_acc: 0.9147\n",
      "Epoch 13/50\n",
      "228948/228948 [==============================] - 111s 483us/step - loss: 0.1880 - acc: 0.9218 - val_loss: 0.2120 - val_acc: 0.9156\n",
      "Epoch 14/50\n",
      "228948/228948 [==============================] - 111s 487us/step - loss: 0.1846 - acc: 0.9236 - val_loss: 0.2045 - val_acc: 0.9192\n",
      "Epoch 15/50\n",
      "228948/228948 [==============================] - 111s 486us/step - loss: 0.1815 - acc: 0.9244 - val_loss: 0.2109 - val_acc: 0.9150\n",
      "Epoch 16/50\n",
      "228948/228948 [==============================] - 111s 486us/step - loss: 0.1776 - acc: 0.9266 - val_loss: 0.2115 - val_acc: 0.9158\n",
      "Epoch 17/50\n",
      "228948/228948 [==============================] - 111s 487us/step - loss: 0.1750 - acc: 0.9275 - val_loss: 0.2055 - val_acc: 0.9162\n",
      "Epoch 18/50\n",
      "228948/228948 [==============================] - 111s 487us/step - loss: 0.1715 - acc: 0.9289 - val_loss: 0.2005 - val_acc: 0.9207\n",
      "Epoch 19/50\n",
      "228948/228948 [==============================] - 111s 487us/step - loss: 0.1695 - acc: 0.9299 - val_loss: 0.2086 - val_acc: 0.9190\n",
      "Epoch 20/50\n",
      "228948/228948 [==============================] - 111s 487us/step - loss: 0.1677 - acc: 0.9304 - val_loss: 0.2030 - val_acc: 0.9216\n",
      "Epoch 21/50\n",
      "228948/228948 [==============================] - 111s 486us/step - loss: 0.1632 - acc: 0.9322 - val_loss: 0.2072 - val_acc: 0.9207\n",
      "Epoch 22/50\n",
      "228948/228948 [==============================] - 111s 487us/step - loss: 0.1634 - acc: 0.9323 - val_loss: 0.1979 - val_acc: 0.9244\n",
      "Epoch 23/50\n",
      "228948/228948 [==============================] - 111s 485us/step - loss: 0.1610 - acc: 0.9337 - val_loss: 0.2053 - val_acc: 0.9209\n",
      "Epoch 24/50\n",
      "228948/228948 [==============================] - 111s 487us/step - loss: 0.1590 - acc: 0.9342 - val_loss: 0.2014 - val_acc: 0.9231\n",
      "Epoch 25/50\n",
      "228948/228948 [==============================] - 111s 484us/step - loss: 0.1578 - acc: 0.9352 - val_loss: 0.2037 - val_acc: 0.9228\n",
      "Epoch 26/50\n",
      "228948/228948 [==============================] - 111s 484us/step - loss: 0.1567 - acc: 0.9359 - val_loss: 0.2012 - val_acc: 0.9245\n",
      "Epoch 27/50\n",
      "228948/228948 [==============================] - 111s 486us/step - loss: 0.1561 - acc: 0.9357 - val_loss: 0.2072 - val_acc: 0.9219\n",
      "Epoch 28/50\n",
      "228948/228948 [==============================] - 111s 485us/step - loss: 0.1526 - acc: 0.9375 - val_loss: 0.2094 - val_acc: 0.9212\n",
      "Epoch 29/50\n",
      "228948/228948 [==============================] - 111s 487us/step - loss: 0.1514 - acc: 0.9378 - val_loss: 0.2052 - val_acc: 0.9223\n",
      "Epoch 30/50\n",
      "228948/228948 [==============================] - 112s 488us/step - loss: 0.1512 - acc: 0.9383 - val_loss: 0.2071 - val_acc: 0.9219\n",
      "Epoch 31/50\n",
      "228948/228948 [==============================] - 113s 492us/step - loss: 0.1499 - acc: 0.9384 - val_loss: 0.1995 - val_acc: 0.9244\n",
      "Epoch 32/50\n",
      "228948/228948 [==============================] - 112s 491us/step - loss: 0.1491 - acc: 0.9389 - val_loss: 0.2066 - val_acc: 0.9228\n",
      "load model ./log/20180703-113130.multi_lstm.022.hdf5\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 115s 503us/step - loss: 0.4791 - acc: 0.7724 - val_loss: 0.3667 - val_acc: 0.8464\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.3511 - acc: 0.8422 - val_loss: 0.2990 - val_acc: 0.8716\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.3062 - acc: 0.8653 - val_loss: 0.2713 - val_acc: 0.8840\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 110s 479us/step - loss: 0.2779 - acc: 0.8789 - val_loss: 0.2610 - val_acc: 0.8906\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.2559 - acc: 0.8895 - val_loss: 0.2577 - val_acc: 0.8921\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.2419 - acc: 0.8966 - val_loss: 0.2375 - val_acc: 0.8989\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.2295 - acc: 0.9022 - val_loss: 0.2371 - val_acc: 0.9007\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.2213 - acc: 0.9064 - val_loss: 0.2212 - val_acc: 0.9064\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 110s 482us/step - loss: 0.2112 - acc: 0.9112 - val_loss: 0.2210 - val_acc: 0.9107\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.2058 - acc: 0.9138 - val_loss: 0.2196 - val_acc: 0.9102\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 110s 482us/step - loss: 0.1997 - acc: 0.9162 - val_loss: 0.2121 - val_acc: 0.9130\n",
      "Epoch 12/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1932 - acc: 0.9195 - val_loss: 0.2146 - val_acc: 0.9123\n",
      "Epoch 13/50\n",
      "228948/228948 [==============================] - 110s 482us/step - loss: 0.1896 - acc: 0.9211 - val_loss: 0.2134 - val_acc: 0.9140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1848 - acc: 0.9233 - val_loss: 0.2141 - val_acc: 0.9145\n",
      "Epoch 15/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1815 - acc: 0.9254 - val_loss: 0.2138 - val_acc: 0.9152\n",
      "Epoch 16/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1789 - acc: 0.9256 - val_loss: 0.2171 - val_acc: 0.9146\n",
      "Epoch 17/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1756 - acc: 0.9271 - val_loss: 0.2140 - val_acc: 0.9165\n",
      "Epoch 18/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1732 - acc: 0.9279 - val_loss: 0.2076 - val_acc: 0.9182\n",
      "Epoch 19/50\n",
      "228948/228948 [==============================] - 110s 480us/step - loss: 0.1712 - acc: 0.9289 - val_loss: 0.2179 - val_acc: 0.9156\n",
      "Epoch 20/50\n",
      "228948/228948 [==============================] - 110s 482us/step - loss: 0.1681 - acc: 0.9307 - val_loss: 0.2090 - val_acc: 0.9176\n",
      "Epoch 21/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1668 - acc: 0.9315 - val_loss: 0.2096 - val_acc: 0.9189\n",
      "Epoch 22/50\n",
      "228948/228948 [==============================] - 110s 480us/step - loss: 0.1634 - acc: 0.9328 - val_loss: 0.2061 - val_acc: 0.9191\n",
      "Epoch 23/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1611 - acc: 0.9332 - val_loss: 0.2125 - val_acc: 0.9185\n",
      "Epoch 24/50\n",
      "228948/228948 [==============================] - 109s 476us/step - loss: 0.1588 - acc: 0.9346 - val_loss: 0.2112 - val_acc: 0.9194\n",
      "Epoch 25/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1578 - acc: 0.9354 - val_loss: 0.2113 - val_acc: 0.9183\n",
      "Epoch 26/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1567 - acc: 0.9360 - val_loss: 0.2093 - val_acc: 0.9187\n",
      "Epoch 27/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1542 - acc: 0.9371 - val_loss: 0.2091 - val_acc: 0.9213\n",
      "Epoch 28/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1532 - acc: 0.9370 - val_loss: 0.2098 - val_acc: 0.9213\n",
      "Epoch 29/50\n",
      "228948/228948 [==============================] - 110s 480us/step - loss: 0.1536 - acc: 0.9371 - val_loss: 0.2068 - val_acc: 0.9222\n",
      "Epoch 30/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1514 - acc: 0.9378 - val_loss: 0.2103 - val_acc: 0.9214\n",
      "Epoch 31/50\n",
      "228948/228948 [==============================] - 110s 481us/step - loss: 0.1505 - acc: 0.9384 - val_loss: 0.2122 - val_acc: 0.9209\n",
      "Epoch 32/50\n",
      "228948/228948 [==============================] - 110s 482us/step - loss: 0.1498 - acc: 0.9386 - val_loss: 0.2089 - val_acc: 0.9223\n",
      "load model ./log/20180703-123135.multi_lstm.022.hdf5\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 117s 509us/step - loss: 0.4872 - acc: 0.7674 - val_loss: 0.3851 - val_acc: 0.8418\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.3543 - acc: 0.8413 - val_loss: 0.3042 - val_acc: 0.8695\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.3066 - acc: 0.8652 - val_loss: 0.2655 - val_acc: 0.8867\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.2768 - acc: 0.8801 - val_loss: 0.2466 - val_acc: 0.8954\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.2575 - acc: 0.8893 - val_loss: 0.2281 - val_acc: 0.9025\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 111s 485us/step - loss: 0.2407 - acc: 0.8971 - val_loss: 0.2291 - val_acc: 0.9029\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.2294 - acc: 0.9026 - val_loss: 0.2231 - val_acc: 0.9072\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.2186 - acc: 0.9080 - val_loss: 0.2062 - val_acc: 0.9148\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.2120 - acc: 0.9113 - val_loss: 0.2184 - val_acc: 0.9118\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 111s 485us/step - loss: 0.2031 - acc: 0.9146 - val_loss: 0.2034 - val_acc: 0.9195\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.1970 - acc: 0.9175 - val_loss: 0.2025 - val_acc: 0.9170\n",
      "Epoch 12/50\n",
      "228948/228948 [==============================] - 112s 488us/step - loss: 0.1923 - acc: 0.9198 - val_loss: 0.2023 - val_acc: 0.9189\n",
      "Epoch 13/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.1882 - acc: 0.9219 - val_loss: 0.2071 - val_acc: 0.9171\n",
      "Epoch 14/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.1833 - acc: 0.9241 - val_loss: 0.1995 - val_acc: 0.9195\n",
      "Epoch 15/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.1808 - acc: 0.9248 - val_loss: 0.1968 - val_acc: 0.9216\n",
      "Epoch 16/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.1775 - acc: 0.9267 - val_loss: 0.1963 - val_acc: 0.9228\n",
      "Epoch 17/50\n",
      "228948/228948 [==============================] - 111s 486us/step - loss: 0.1735 - acc: 0.9283 - val_loss: 0.2008 - val_acc: 0.9198\n",
      "Epoch 18/50\n",
      "228948/228948 [==============================] - 112s 488us/step - loss: 0.1719 - acc: 0.9295 - val_loss: 0.1984 - val_acc: 0.9213\n",
      "Epoch 19/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.1692 - acc: 0.9306 - val_loss: 0.1996 - val_acc: 0.9225\n",
      "Epoch 20/50\n",
      "228948/228948 [==============================] - 112s 488us/step - loss: 0.1658 - acc: 0.9312 - val_loss: 0.2049 - val_acc: 0.9214\n",
      "Epoch 21/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.1641 - acc: 0.9323 - val_loss: 0.2015 - val_acc: 0.9215\n",
      "Epoch 22/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.1624 - acc: 0.9335 - val_loss: 0.1996 - val_acc: 0.9240\n",
      "Epoch 23/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.1602 - acc: 0.9340 - val_loss: 0.2031 - val_acc: 0.9217\n",
      "Epoch 24/50\n",
      "228948/228948 [==============================] - 112s 488us/step - loss: 0.1587 - acc: 0.9346 - val_loss: 0.2013 - val_acc: 0.9240\n",
      "Epoch 25/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.1573 - acc: 0.9351 - val_loss: 0.2004 - val_acc: 0.9248\n",
      "Epoch 26/50\n",
      "228948/228948 [==============================] - 112s 490us/step - loss: 0.1554 - acc: 0.9356 - val_loss: 0.2020 - val_acc: 0.9236\n",
      "load model ./log/20180703-133100.multi_lstm.016.hdf5\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 117s 512us/step - loss: 0.4804 - acc: 0.7715 - val_loss: 0.3859 - val_acc: 0.8388\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.3527 - acc: 0.8415 - val_loss: 0.3082 - val_acc: 0.8707\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 112s 491us/step - loss: 0.3056 - acc: 0.8655 - val_loss: 0.2649 - val_acc: 0.8861\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.2776 - acc: 0.8798 - val_loss: 0.2573 - val_acc: 0.8898\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 111s 486us/step - loss: 0.2561 - acc: 0.8892 - val_loss: 0.2348 - val_acc: 0.9013\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 112s 489us/step - loss: 0.2408 - acc: 0.8976 - val_loss: 0.2387 - val_acc: 0.8975\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.2287 - acc: 0.9024 - val_loss: 0.2211 - val_acc: 0.9082\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.2189 - acc: 0.9081 - val_loss: 0.2165 - val_acc: 0.9119\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.2105 - acc: 0.9121 - val_loss: 0.2113 - val_acc: 0.9149\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 113s 494us/step - loss: 0.2035 - acc: 0.9148 - val_loss: 0.2086 - val_acc: 0.9162\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 112s 491us/step - loss: 0.1976 - acc: 0.9176 - val_loss: 0.2111 - val_acc: 0.9149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1932 - acc: 0.9196 - val_loss: 0.2101 - val_acc: 0.9160\n",
      "Epoch 13/50\n",
      "228948/228948 [==============================] - 113s 494us/step - loss: 0.1875 - acc: 0.9221 - val_loss: 0.2033 - val_acc: 0.9189\n",
      "Epoch 14/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1838 - acc: 0.9233 - val_loss: 0.2052 - val_acc: 0.9184\n",
      "Epoch 15/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1817 - acc: 0.9246 - val_loss: 0.2079 - val_acc: 0.9187\n",
      "Epoch 16/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1766 - acc: 0.9264 - val_loss: 0.1978 - val_acc: 0.9206\n",
      "Epoch 17/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1737 - acc: 0.9283 - val_loss: 0.2048 - val_acc: 0.9207\n",
      "Epoch 18/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1707 - acc: 0.9295 - val_loss: 0.2070 - val_acc: 0.9212\n",
      "Epoch 19/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1683 - acc: 0.9309 - val_loss: 0.2130 - val_acc: 0.9179\n",
      "Epoch 20/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1670 - acc: 0.9306 - val_loss: 0.2045 - val_acc: 0.9221\n",
      "Epoch 21/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1630 - acc: 0.9327 - val_loss: 0.2038 - val_acc: 0.9212\n",
      "Epoch 22/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1623 - acc: 0.9330 - val_loss: 0.2010 - val_acc: 0.9222\n",
      "Epoch 23/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1614 - acc: 0.9336 - val_loss: 0.2008 - val_acc: 0.9226\n",
      "Epoch 24/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1586 - acc: 0.9348 - val_loss: 0.2050 - val_acc: 0.9233\n",
      "Epoch 25/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1572 - acc: 0.9353 - val_loss: 0.2039 - val_acc: 0.9242\n",
      "Epoch 26/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1556 - acc: 0.9362 - val_loss: 0.2012 - val_acc: 0.9242\n",
      "load model ./log/20180703-142013.multi_lstm.016.hdf5\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 119s 521us/step - loss: 0.4818 - acc: 0.7733 - val_loss: 0.3746 - val_acc: 0.8416\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.3549 - acc: 0.8417 - val_loss: 0.3140 - val_acc: 0.8697\n",
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.3076 - acc: 0.8646 - val_loss: 0.2795 - val_acc: 0.8796\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.2769 - acc: 0.8799 - val_loss: 0.2544 - val_acc: 0.8911\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.2574 - acc: 0.8881 - val_loss: 0.2447 - val_acc: 0.8948\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.2422 - acc: 0.8966 - val_loss: 0.2386 - val_acc: 0.9005\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.2295 - acc: 0.9018 - val_loss: 0.2212 - val_acc: 0.9083\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.2207 - acc: 0.9069 - val_loss: 0.2236 - val_acc: 0.9064\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.2118 - acc: 0.9107 - val_loss: 0.2173 - val_acc: 0.9088\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.2038 - acc: 0.9145 - val_loss: 0.2149 - val_acc: 0.9102\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.1986 - acc: 0.9168 - val_loss: 0.2144 - val_acc: 0.9143\n",
      "Epoch 12/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.1928 - acc: 0.9193 - val_loss: 0.2127 - val_acc: 0.9135\n",
      "Epoch 13/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1893 - acc: 0.9214 - val_loss: 0.2087 - val_acc: 0.9158\n",
      "Epoch 14/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.1846 - acc: 0.9233 - val_loss: 0.2060 - val_acc: 0.9170\n",
      "Epoch 15/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1809 - acc: 0.9252 - val_loss: 0.2033 - val_acc: 0.9193\n",
      "Epoch 16/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1770 - acc: 0.9267 - val_loss: 0.2016 - val_acc: 0.9196\n",
      "Epoch 17/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.1745 - acc: 0.9277 - val_loss: 0.2095 - val_acc: 0.9165\n",
      "Epoch 18/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1711 - acc: 0.9290 - val_loss: 0.2061 - val_acc: 0.9183\n",
      "Epoch 19/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1694 - acc: 0.9299 - val_loss: 0.2030 - val_acc: 0.9196\n",
      "Epoch 20/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1669 - acc: 0.9308 - val_loss: 0.2025 - val_acc: 0.9209\n",
      "Epoch 21/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1647 - acc: 0.9326 - val_loss: 0.2056 - val_acc: 0.9199\n",
      "Epoch 22/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1628 - acc: 0.9332 - val_loss: 0.2021 - val_acc: 0.9195\n",
      "Epoch 23/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.1606 - acc: 0.9340 - val_loss: 0.2077 - val_acc: 0.9185\n",
      "Epoch 24/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.1580 - acc: 0.9353 - val_loss: 0.2016 - val_acc: 0.9215\n",
      "Epoch 25/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1586 - acc: 0.9348 - val_loss: 0.1998 - val_acc: 0.9234\n",
      "Epoch 26/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.1562 - acc: 0.9357 - val_loss: 0.2063 - val_acc: 0.9207\n",
      "Epoch 27/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1538 - acc: 0.9367 - val_loss: 0.2024 - val_acc: 0.9233\n",
      "Epoch 28/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.1520 - acc: 0.9374 - val_loss: 0.2032 - val_acc: 0.9226\n",
      "Epoch 29/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.1504 - acc: 0.9386 - val_loss: 0.1981 - val_acc: 0.9239\n",
      "Epoch 30/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1495 - acc: 0.9388 - val_loss: 0.2062 - val_acc: 0.9211\n",
      "Epoch 31/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1491 - acc: 0.9391 - val_loss: 0.2029 - val_acc: 0.9226\n",
      "Epoch 32/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1478 - acc: 0.9395 - val_loss: 0.2084 - val_acc: 0.9205\n",
      "Epoch 33/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1468 - acc: 0.9402 - val_loss: 0.2085 - val_acc: 0.9222\n",
      "Epoch 34/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.1469 - acc: 0.9399 - val_loss: 0.2034 - val_acc: 0.9239\n",
      "Epoch 35/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1451 - acc: 0.9406 - val_loss: 0.2039 - val_acc: 0.9239\n",
      "Epoch 36/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1438 - acc: 0.9411 - val_loss: 0.2082 - val_acc: 0.9218\n",
      "Epoch 37/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1429 - acc: 0.9417 - val_loss: 0.2082 - val_acc: 0.9226\n",
      "Epoch 38/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.1436 - acc: 0.9415 - val_loss: 0.2061 - val_acc: 0.9235\n",
      "Epoch 39/50\n",
      "228948/228948 [==============================] - 114s 498us/step - loss: 0.1419 - acc: 0.9416 - val_loss: 0.2029 - val_acc: 0.9241\n",
      "load model ./log/20180703-150948.multi_lstm.029.hdf5\n",
      "Train on 228948 samples, validate on 25438 samples\n",
      "Epoch 1/50\n",
      "228948/228948 [==============================] - 119s 521us/step - loss: 0.4791 - acc: 0.7726 - val_loss: 0.3572 - val_acc: 0.8468\n",
      "Epoch 2/50\n",
      "228948/228948 [==============================] - 114s 499us/step - loss: 0.3545 - acc: 0.8411 - val_loss: 0.2970 - val_acc: 0.8750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n",
      "228948/228948 [==============================] - 114s 500us/step - loss: 0.3085 - acc: 0.8645 - val_loss: 0.2695 - val_acc: 0.8829\n",
      "Epoch 4/50\n",
      "228948/228948 [==============================] - 114s 500us/step - loss: 0.2800 - acc: 0.8786 - val_loss: 0.2571 - val_acc: 0.8907\n",
      "Epoch 5/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.2585 - acc: 0.8885 - val_loss: 0.2319 - val_acc: 0.9003\n",
      "Epoch 6/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.2444 - acc: 0.8949 - val_loss: 0.2251 - val_acc: 0.9046\n",
      "Epoch 7/50\n",
      "228948/228948 [==============================] - 114s 500us/step - loss: 0.2306 - acc: 0.9019 - val_loss: 0.2238 - val_acc: 0.9070\n",
      "Epoch 8/50\n",
      "228948/228948 [==============================] - 114s 500us/step - loss: 0.2203 - acc: 0.9071 - val_loss: 0.2144 - val_acc: 0.9120\n",
      "Epoch 9/50\n",
      "228948/228948 [==============================] - 114s 500us/step - loss: 0.2119 - acc: 0.9106 - val_loss: 0.2120 - val_acc: 0.9108\n",
      "Epoch 10/50\n",
      "228948/228948 [==============================] - 115s 500us/step - loss: 0.2061 - acc: 0.9131 - val_loss: 0.2099 - val_acc: 0.9138\n",
      "Epoch 11/50\n",
      "228948/228948 [==============================] - 115s 500us/step - loss: 0.1993 - acc: 0.9164 - val_loss: 0.2009 - val_acc: 0.9178\n",
      "Epoch 12/50\n",
      "228948/228948 [==============================] - 113s 493us/step - loss: 0.1944 - acc: 0.9186 - val_loss: 0.2003 - val_acc: 0.9197\n",
      "Epoch 13/50\n",
      "228948/228948 [==============================] - 114s 499us/step - loss: 0.1904 - acc: 0.9205 - val_loss: 0.1996 - val_acc: 0.9209\n",
      "Epoch 14/50\n",
      "228948/228948 [==============================] - 115s 500us/step - loss: 0.1864 - acc: 0.9227 - val_loss: 0.1953 - val_acc: 0.9228\n",
      "Epoch 15/50\n",
      "228948/228948 [==============================] - 114s 500us/step - loss: 0.1830 - acc: 0.9239 - val_loss: 0.1955 - val_acc: 0.9210\n",
      "Epoch 16/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.1783 - acc: 0.9261 - val_loss: 0.1960 - val_acc: 0.9211\n",
      "Epoch 17/50\n",
      "228948/228948 [==============================] - 113s 496us/step - loss: 0.1767 - acc: 0.9274 - val_loss: 0.1974 - val_acc: 0.9200\n",
      "Epoch 18/50\n",
      "228948/228948 [==============================] - 115s 500us/step - loss: 0.1733 - acc: 0.9279 - val_loss: 0.1897 - val_acc: 0.9242\n",
      "Epoch 19/50\n",
      "228948/228948 [==============================] - 114s 497us/step - loss: 0.1697 - acc: 0.9293 - val_loss: 0.1990 - val_acc: 0.9224\n",
      "Epoch 20/50\n",
      "228948/228948 [==============================] - 114s 500us/step - loss: 0.1687 - acc: 0.9302 - val_loss: 0.1919 - val_acc: 0.9230\n",
      "Epoch 21/50\n",
      "228948/228948 [==============================] - 115s 500us/step - loss: 0.1655 - acc: 0.9312 - val_loss: 0.1955 - val_acc: 0.9231\n",
      "Epoch 22/50\n",
      "228948/228948 [==============================] - 115s 500us/step - loss: 0.1645 - acc: 0.9325 - val_loss: 0.2011 - val_acc: 0.9214\n",
      "Epoch 23/50\n",
      "228948/228948 [==============================] - 114s 500us/step - loss: 0.1618 - acc: 0.9332 - val_loss: 0.1949 - val_acc: 0.9248\n",
      "Epoch 24/50\n",
      "228948/228948 [==============================] - 114s 500us/step - loss: 0.1608 - acc: 0.9339 - val_loss: 0.1929 - val_acc: 0.9253\n",
      "Epoch 25/50\n",
      "228948/228948 [==============================] - 114s 500us/step - loss: 0.1581 - acc: 0.9345 - val_loss: 0.1964 - val_acc: 0.9244\n",
      "Epoch 26/50\n",
      "228948/228948 [==============================] - 115s 500us/step - loss: 0.1562 - acc: 0.9359 - val_loss: 0.1957 - val_acc: 0.9236\n",
      "Epoch 27/50\n",
      "228948/228948 [==============================] - 114s 500us/step - loss: 0.1565 - acc: 0.9355 - val_loss: 0.1940 - val_acc: 0.9252\n",
      "Epoch 28/50\n",
      "228948/228948 [==============================] - 115s 500us/step - loss: 0.1548 - acc: 0.9361 - val_loss: 0.1929 - val_acc: 0.9247\n",
      "load model ./log/20180703-162436.multi_lstm.018.hdf5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "pred_collect = []\n",
    "\n",
    "for i, (train_index, dev_index) in enumerate(StratifiedKFold(n_splits=10).split(X=word1, y=label)):\n",
    "    train_word1, train_word2, train_y = word1[train_index, :], word2[train_index, :], label[train_index]\n",
    "    dev_word1, dev_word2, dev_y = word1[dev_index, :], word2[dev_index, :], label[dev_index]\n",
    "\n",
    "    word_input1 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "    word_input2 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=word_embedding_data.shape[0],\n",
    "        output_dim=word_embedding_data.shape[1],\n",
    "        weights=[word_embedding_data],\n",
    "        input_length=WORD_SEQ_LEN,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    word_vector1 = embedding_layer(word_input1)\n",
    "    word_vector2 = embedding_layer(word_input2)\n",
    "\n",
    "    lstm_layer1 = LSTM(LSTM_SIZE_1, dropout=DROP_RATE, recurrent_dropout=DROP_RATE, return_sequences=True)\n",
    "    word_first_1 = lstm_layer1(word_vector1)\n",
    "    word_first_1 = Dropout(DROP_RATE)(word_first_1)\n",
    "    word_first_2 = lstm_layer1(word_vector2)\n",
    "    word_first_2 = Dropout(DROP_RATE)(word_first_2)\n",
    "\n",
    "    lstm_layer2 = LSTM(LSTM_SIZE_2, dropout=DROP_RATE, recurrent_dropout=DROP_RATE, return_sequences=False)\n",
    "    word_second_1 = lstm_layer2(word_first_1)\n",
    "    word_second_2 = lstm_layer2(word_first_2)\n",
    "\n",
    "    x = concatenate([word_second_1, word_second_2])\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(DENSE_SIZE, activation=\"relu\")(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    pred = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=[word_input1, word_input2], outputs=pred)\n",
    "    model.compile(\n",
    "        optimizer=\"nadam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"acc\"]\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\"val_loss\", patience=10)\n",
    "    check_point = ModelCheckpoint(\n",
    "        \"./log/%s.multi_lstm.{epoch:03d}.hdf5\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "\n",
    "    train_res = model.fit(\n",
    "        x=[train_word1, train_word2],\n",
    "        y=train_y,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHES,\n",
    "        validation_data=([dev_word1, dev_word2], dev_y),\n",
    "        shuffle=True,\n",
    "        callbacks=[early_stopping, check_point]\n",
    "    )\n",
    "\n",
    "    print(\"load model %s\" % (glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"),))\n",
    "    model.load_weights(glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"))\n",
    "\n",
    "    test_pred = model.predict([test_word1, test_word2], batch_size=BATCH_SIZE)\n",
    "    pred_collect.append(pd.DataFrame(test_pred, columns=[\"y_pre\"]))\n",
    "\n",
    "pd.DataFrame(pd.concat(pred_collect, axis=1).mean(axis=1), columns=[\"y_pre\"]).to_csv(\n",
    "    \"./result/%s-pred.csv\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
