{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/\"\n",
    "TRAIN_PATH = DATA_PATH + \"train.csv\"\n",
    "TEST_PATH = DATA_PATH + \"test.csv\"\n",
    "WORD_EMBED_PATH = DATA_PATH + \"word_embed.txt\"\n",
    "CHAR_EMBED_PATH = DATA_PATH + \"char_embed.txt\"\n",
    "QUEST_PATH = DATA_PATH + \"question.csv\"\n",
    "\n",
    "SEED = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(TRAIN_PATH)\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "question_data = pd.read_csv(QUEST_PATH)\n",
    "word_embedding_data = pd.read_csv(WORD_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "char_embedding_data = pd.read_csv(CHAR_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "\n",
    "question_data[\"words\"] = question_data[\"words\"].str.split(\" \")\n",
    "question_data[\"chars\"] = question_data[\"chars\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(question_data[\"words\"])\n",
    "word_index = word_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_SEQ_LEN = 8\n",
    "\n",
    "word_texts_q1 = train_data.merge(question_data, how=\"left\", left_on=\"q1\", right_on=\"qid\")[\"words\"]\n",
    "word_texts_q2 = train_data.merge(question_data, how=\"left\", left_on=\"q2\", right_on=\"qid\")[\"words\"]\n",
    "\n",
    "word_seq_q1 = word_tokenizer.texts_to_sequences(word_texts_q1)\n",
    "pad_word_seq_q1 = pad_sequences(word_seq_q1, maxlen=WORD_SEQ_LEN)\n",
    "word_seq_q2 = word_tokenizer.texts_to_sequences(word_texts_q2)\n",
    "pad_word_seq_q2 = pad_sequences(word_seq_q2, maxlen=WORD_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train_data[\"label\"].values\n",
    "train_seq1, dev_seq1, train_seq2, dev_seq2, train_y, dev_y = train_test_split(pad_word_seq_q1, pad_word_seq_q2, label, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_data = word_embedding_data.reindex(list(word_index.keys())).values\n",
    "word_embedding_data = np.concatenate((np.zeros((1, word_embedding_data.shape[1]), dtype=np.float32), word_embedding_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'output_layer/batch_normalization/AssignMovingAvg:0' shape=(256,) dtype=float64_ref>, <tf.Tensor 'output_layer/batch_normalization/AssignMovingAvg_1:0' shape=(256,) dtype=float64_ref>, <tf.Tensor 'output_layer/batch_normalization_1/AssignMovingAvg:0' shape=(128,) dtype=float64_ref>, <tf.Tensor 'output_layer/batch_normalization_1/AssignMovingAvg_1:0' shape=(128,) dtype=float64_ref>]\n",
      "[epoch 1 step 1] train loss: 0.8574914609992914, dev loss: 1.0217921001652481\n",
      "[epoch 1 step 2] train loss: 0.9848272175404253, dev loss: 0.8940810427182642\n",
      "[epoch 1 step 3] train loss: 0.8662045926183246, dev loss: 0.7703268891158105\n",
      "[epoch 1 step 4] train loss: 0.7687002173375244, dev loss: 0.7474213034287281\n",
      "[epoch 1 step 5] train loss: 0.746488186044315, dev loss: 0.7216810961452014\n",
      "[epoch 1 step 6] train loss: 0.7255934421976289, dev loss: 0.7185147816864392\n",
      "[epoch 1 step 7] train loss: 0.7147638674707633, dev loss: 0.7090136811502167\n",
      "[epoch 1 step 8] train loss: 0.7121108178053692, dev loss: 0.7070082511684789\n",
      "[epoch 1 step 9] train loss: 0.7076387851949776, dev loss: 0.705486773910814\n",
      "[epoch 1 step 10] train loss: 0.7062411886213562, dev loss: 0.7042018636238009\n",
      "[epoch 1 step 11] train loss: 0.7027911440535151, dev loss: 0.702517832496644\n",
      "[epoch 1 step 12] train loss: 0.7050566519039565, dev loss: 0.7016568711623958\n",
      "[epoch 1 step 13] train loss: 0.7011776556957136, dev loss: 0.7009651547892366\n",
      "[epoch 1 step 14] train loss: 0.7027970666103242, dev loss: 0.7006882206875844\n",
      "[epoch 1 step 15] train loss: 0.6998462034940343, dev loss: 0.6999085003461328\n",
      "[epoch 1 step 16] train loss: 0.7014344768644231, dev loss: 0.6992357624829068\n",
      "[epoch 1 step 17] train loss: 0.6939644733096182, dev loss: 0.6988015317390309\n",
      "[epoch 1 step 18] train loss: 0.6908197724819304, dev loss: 0.6991889270959554\n",
      "[epoch 1 step 19] train loss: 0.6979655254997499, dev loss: 0.6989206087450087\n",
      "[epoch 1 step 20] train loss: 0.7085777944932545, dev loss: 0.6979267257714096\n",
      "[epoch 1 step 21] train loss: 0.6935328640766301, dev loss: 0.6974354505551736\n",
      "[epoch 1 step 22] train loss: 0.695147633857432, dev loss: 0.6977644794177704\n",
      "[epoch 1 step 23] train loss: 0.7073708922117845, dev loss: 0.6967880668891119\n",
      "[epoch 1 step 24] train loss: 0.7067341160281244, dev loss: 0.6973862848448132\n",
      "[epoch 1 step 25] train loss: 0.699828886527483, dev loss: 0.6979568649294989\n",
      "[epoch 1 step 26] train loss: 0.697666439196734, dev loss: 0.6972147260585786\n",
      "[epoch 1 step 27] train loss: 0.6976590265979575, dev loss: 0.6962813761651625\n",
      "[epoch 1 step 28] train loss: 0.6955487595229781, dev loss: 0.6957443943080105\n",
      "[epoch 1 step 29] train loss: 0.6937244860852239, dev loss: 0.695617941493069\n",
      "[epoch 1 step 30] train loss: 0.7009678241281575, dev loss: 0.6956377271607466\n",
      "[epoch 1 step 31] train loss: 0.6986306718427697, dev loss: 0.6961796631648588\n",
      "[epoch 1 step 32] train loss: 0.6954424627859206, dev loss: 0.6953920311036577\n",
      "[epoch 1 step 33] train loss: 0.695356217678447, dev loss: 0.6953197422133923\n",
      "[epoch 1 step 34] train loss: 0.6915080497223973, dev loss: 0.6952520007923849\n",
      "[epoch 1 step 35] train loss: 0.6924475627461726, dev loss: 0.695268261528664\n",
      "[epoch 1 step 36] train loss: 0.6929550136127376, dev loss: 0.6951219340460281\n",
      "[epoch 1 step 37] train loss: 0.6968086262502791, dev loss: 0.6949165268618817\n",
      "[epoch 1 step 38] train loss: 0.6923073084001219, dev loss: 0.695001346493385\n",
      "[epoch 1 step 39] train loss: 0.6851700585341142, dev loss: 0.6956304130170938\n",
      "[epoch 1 step 40] train loss: 0.6966870591148251, dev loss: 0.6953974845679849\n",
      "[epoch 1 step 41] train loss: 0.6966319559822174, dev loss: 0.6948416982072985\n",
      "[epoch 1 step 42] train loss: 0.6973606686297061, dev loss: 0.694677485251277\n",
      "[epoch 1 step 43] train loss: 0.7015438529453295, dev loss: 0.6943811598890234\n",
      "[epoch 1 step 44] train loss: 0.6947562734192502, dev loss: 0.6943752041968929\n",
      "[epoch 1 step 45] train loss: 0.6924719466200924, dev loss: 0.6943641223140555\n",
      "[epoch 1 step 46] train loss: 0.6906409098293751, dev loss: 0.6944192353350661\n",
      "[epoch 1 step 47] train loss: 0.6977007119695567, dev loss: 0.6942237463774896\n",
      "[epoch 1 step 48] train loss: 0.6891353201074164, dev loss: 0.6943432573214147\n",
      "[epoch 1 step 49] train loss: 0.6965052217441909, dev loss: 0.6941605520499341\n",
      "[epoch 1 step 50] train loss: 0.6871793913804214, dev loss: 0.6945023263665371\n",
      "[epoch 1 step 51] train loss: 0.6871150342259438, dev loss: 0.6947910094844006\n",
      "[epoch 1 step 52] train loss: 0.6912716888380559, dev loss: 0.694751616894881\n",
      "[epoch 1 step 53] train loss: 0.6994712235522829, dev loss: 0.6942166714811524\n",
      "[epoch 1 step 54] train loss: 0.6954599722730921, dev loss: 0.6940405896157603\n",
      "[epoch 1 step 55] train loss: 0.6909199069308967, dev loss: 0.6941778456425732\n",
      "[epoch 1 step 56] train loss: 0.7036100334841564, dev loss: 0.693755205295409\n",
      "[epoch 1 step 57] train loss: 0.6940332164329408, dev loss: 0.6936780421675811\n",
      "[epoch 1 step 58] train loss: 0.7005836882998135, dev loss: 0.693962787597154\n",
      "[epoch 1 step 59] train loss: 0.6933601401576215, dev loss: 0.6938337455628646\n",
      "[epoch 1 step 60] train loss: 0.695842765994909, dev loss: 0.6939378313269103\n",
      "[epoch 1 step 61] train loss: 0.6947807346925531, dev loss: 0.6942502230581235\n",
      "[epoch 1 step 62] train loss: 0.6942653462708691, dev loss: 0.6937683899790832\n",
      "[epoch 1 step 63] train loss: 0.6948037581754091, dev loss: 0.6938309826215937\n",
      "[epoch 1 step 64] train loss: 0.6931573766877714, dev loss: 0.6937261602244963\n",
      "[epoch 1 step 65] train loss: 0.6916623091113872, dev loss: 0.6934791296921969\n",
      "[epoch 1 step 66] train loss: 0.6972030404024354, dev loss: 0.6935295994716268\n",
      "[epoch 1 step 67] train loss: 0.6936082811086695, dev loss: 0.6934595368577644\n",
      "[epoch 1 step 68] train loss: 0.690283539148457, dev loss: 0.6934672038101165\n",
      "[epoch 1 step 69] train loss: 0.7003393856884145, dev loss: 0.6934480301577332\n",
      "[epoch 1 step 70] train loss: 0.695414526015745, dev loss: 0.6935843379448441\n",
      "[epoch 1 step 71] train loss: 0.6948655303795862, dev loss: 0.6939202922780974\n",
      "[epoch 1 step 72] train loss: 0.694471974760295, dev loss: 0.6937437139673761\n",
      "[epoch 1 step 73] train loss: 0.695224324434583, dev loss: 0.6941566355040601\n",
      "[epoch 1 step 74] train loss: 0.6945684346309716, dev loss: 0.6937161290855457\n",
      "[epoch 1 step 75] train loss: 0.6921249900437205, dev loss: 0.6932105362069323\n",
      "[epoch 1 step 76] train loss: 0.696299788619194, dev loss: 0.6933778872074368\n",
      "[epoch 1 step 77] train loss: 0.6950327127168718, dev loss: 0.6933864372558833\n",
      "[epoch 1 step 78] train loss: 0.6927703546719031, dev loss: 0.6932677700869013\n",
      "[epoch 1 step 79] train loss: 0.6926162582676658, dev loss: 0.6932082162959604\n",
      "[epoch 1 step 80] train loss: 0.6923367737693151, dev loss: 0.6931453058856382\n",
      "[epoch 1 step 81] train loss: 0.693904886327154, dev loss: 0.6931664515336499\n",
      "[epoch 1 step 82] train loss: 0.6950601228264052, dev loss: 0.6931925877233873\n",
      "[epoch 1 step 83] train loss: 0.6918045107029853, dev loss: 0.6930732276758838\n",
      "[epoch 1 step 84] train loss: 0.696286858812185, dev loss: 0.6931634294635999\n",
      "[epoch 1 step 85] train loss: 0.6911385989780976, dev loss: 0.693092986908139\n",
      "[epoch 1 step 86] train loss: 0.6979794376578043, dev loss: 0.6932247748456893\n",
      "[epoch 1 step 87] train loss: 0.6909294476334906, dev loss: 0.6930074557741558\n",
      "[epoch 1 step 88] train loss: 0.6885306854048019, dev loss: 0.6931480374029633\n",
      "[epoch 1 step 89] train loss: 0.6876931684100291, dev loss: 0.6934030901109903\n",
      "[epoch 1 step 90] train loss: 0.6898611692717089, dev loss: 0.6934879909446812\n",
      "[epoch 1 step 91] train loss: 0.7014950602767427, dev loss: 0.6930192986355046\n",
      "[epoch 1 step 92] train loss: 0.6925740933609114, dev loss: 0.6930128203842432\n",
      "[epoch 1 step 93] train loss: 0.692565306573401, dev loss: 0.693012304534141\n",
      "[epoch 1 step 94] train loss: 0.6948041379616285, dev loss: 0.6929655908454546\n",
      "[epoch 1 step 95] train loss: 0.6933031034371, dev loss: 0.6929493361294795\n",
      "[epoch 1 step 96] train loss: 0.6900259838450854, dev loss: 0.6929950031408595\n",
      "[epoch 1 step 97] train loss: 0.6973664547158351, dev loss: 0.6929419919464839\n",
      "[epoch 1 step 98] train loss: 0.6954686707843245, dev loss: 0.6929536944262822\n",
      "[epoch 1 step 99] train loss: 0.6913892318009137, dev loss: 0.6928876453537097\n",
      "[epoch 1 step 100] train loss: 0.6938412873870297, dev loss: 0.6929939987975972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1 step 101] train loss: 0.6906544368187454, dev loss: 0.6929116137896555\n",
      "[epoch 1 step 102] train loss: 0.691763889481235, dev loss: 0.6929143366020737\n",
      "[epoch 1 step 103] train loss: 0.6975720216603123, dev loss: 0.6928913549575815\n",
      "[epoch 1 step 104] train loss: 0.6936452364280283, dev loss: 0.6928829139794669\n",
      "[epoch 1 step 105] train loss: 0.6936157116529837, dev loss: 0.6929022545210618\n",
      "[epoch 1 step 106] train loss: 0.6960673302061884, dev loss: 0.693132790681204\n",
      "[epoch 1 step 107] train loss: 0.6912694268838377, dev loss: 0.6928304168632975\n",
      "[epoch 1 step 108] train loss: 0.6915358645552601, dev loss: 0.6928465647779726\n",
      "[epoch 1 step 109] train loss: 0.6950452428264686, dev loss: 0.6928573745414257\n",
      "[epoch 1 step 110] train loss: 0.6936553165842625, dev loss: 0.6928354241226622\n",
      "[epoch 1 step 111] train loss: 0.6955575649926642, dev loss: 0.692956579323819\n",
      "[epoch 1 step 112] train loss: 0.6937023009788422, dev loss: 0.6929954485454235\n",
      "[epoch 1 step 113] train loss: 0.6941865104991842, dev loss: 0.6931252925583316\n",
      "[epoch 1 step 114] train loss: 0.6925269233178861, dev loss: 0.6929059864098887\n",
      "[epoch 1 step 115] train loss: 0.6927148093383164, dev loss: 0.6928470434909448\n",
      "[epoch 1 step 116] train loss: 0.6930490728618964, dev loss: 0.6928404877649823\n",
      "[epoch 1 step 117] train loss: 0.6925621970101813, dev loss: 0.692788198740675\n",
      "[epoch 1 step 118] train loss: 0.6908055483343674, dev loss: 0.6927339116623874\n",
      "[epoch 1 step 119] train loss: 0.6948161378644064, dev loss: 0.6927638261754971\n",
      "[epoch 1 step 120] train loss: 0.6984255164343878, dev loss: 0.6931526261037889\n",
      "[epoch 1 step 121] train loss: 0.694475674173581, dev loss: 0.6936845774022679\n",
      "[epoch 1 step 122] train loss: 0.6943049216706381, dev loss: 0.6930360965364876\n",
      "[epoch 1 step 123] train loss: 0.691987086347323, dev loss: 0.6927784442194794\n",
      "[epoch 1 step 124] train loss: 0.6917407783070867, dev loss: 0.6927257437377151\n",
      "[epoch 1 step 125] train loss: 0.6929868214147487, dev loss: 0.6927398212457306\n",
      "[epoch 1 step 126] train loss: 0.6898235209107558, dev loss: 0.6927416211308044\n",
      "[epoch 1 step 127] train loss: 0.6937573439463793, dev loss: 0.692704067293521\n",
      "[epoch 1 step 128] train loss: 0.6988543234044595, dev loss: 0.6928360669535019\n",
      "[epoch 1 step 129] train loss: 0.6923292235938285, dev loss: 0.6927433187990929\n",
      "[epoch 1 step 130] train loss: 0.6938408497541326, dev loss: 0.6928098086495588\n",
      "[epoch 1 step 131] train loss: 0.6914820507981633, dev loss: 0.692690807445763\n",
      "[epoch 1 step 132] train loss: 0.6990168043350686, dev loss: 0.6931050722483876\n",
      "[epoch 1 step 133] train loss: 0.692780067119086, dev loss: 0.6929086173392927\n",
      "[epoch 1 step 134] train loss: 0.6938655251831757, dev loss: 0.6929923881125765\n",
      "[epoch 1 step 135] train loss: 0.6939277594495932, dev loss: 0.6931415319706198\n",
      "[epoch 1 step 136] train loss: 0.6930714775781022, dev loss: 0.6929951932180404\n",
      "[epoch 1 step 137] train loss: 0.6940086204606126, dev loss: 0.6932115441726533\n",
      "[epoch 1 step 138] train loss: 0.6934696799328643, dev loss: 0.6932962435235304\n",
      "[epoch 1 step 139] train loss: 0.6933631531591453, dev loss: 0.6932241860922843\n",
      "[epoch 1 step 140] train loss: 0.6931853976767934, dev loss: 0.6929988163349476\n",
      "[epoch 1 step 141] train loss: 0.692162671698162, dev loss: 0.6927583472198439\n",
      "[epoch 1 step 142] train loss: 0.6899409216863075, dev loss: 0.6926390997586851\n",
      "[epoch 1 step 143] train loss: 0.6903712575441803, dev loss: 0.692684883666535\n",
      "[epoch 1 step 144] train loss: 0.6922824337143447, dev loss: 0.6926740409550595\n",
      "[epoch 1 step 145] train loss: 0.694538078124446, dev loss: 0.6926304393382007\n",
      "[epoch 1 step 146] train loss: 0.6923150327924543, dev loss: 0.6926303048391829\n",
      "[epoch 1 step 147] train loss: 0.6968602085614738, dev loss: 0.692666822382894\n",
      "[epoch 1 step 148] train loss: 0.6973308234167925, dev loss: 0.693033351186256\n",
      "[epoch 1 step 149] train loss: 0.6915360106964552, dev loss: 0.6926232679296062\n",
      "[epoch 1 step 150] train loss: 0.6971009178341511, dev loss: 0.6928142068961899\n",
      "[epoch 1 step 151] train loss: 0.6918777854851379, dev loss: 0.6926591144439431\n",
      "[epoch 1 step 152] train loss: 0.6911603749122177, dev loss: 0.6926014537777768\n",
      "[epoch 1 step 153] train loss: 0.6917379285084736, dev loss: 0.692603085601635\n",
      "[epoch 1 step 154] train loss: 0.6989532451569449, dev loss: 0.6927954974358094\n",
      "[epoch 1 step 155] train loss: 0.6944030424869412, dev loss: 0.6930039046825581\n",
      "[epoch 1 step 156] train loss: 0.6933862747135938, dev loss: 0.6930402871665495\n",
      "[epoch 1 step 157] train loss: 0.6924751550515761, dev loss: 0.6927224675605318\n",
      "[epoch 1 step 158] train loss: 0.6938264729075792, dev loss: 0.6928062108148921\n",
      "[epoch 1 step 159] train loss: 0.6914456936506774, dev loss: 0.6926440422485257\n",
      "[epoch 1 step 160] train loss: 0.6923886340917849, dev loss: 0.6926131610908713\n",
      "[epoch 1 step 161] train loss: 0.6923061109933983, dev loss: 0.6925792551371804\n",
      "[epoch 1 step 162] train loss: 0.6906592945160783, dev loss: 0.6925908843189695\n",
      "[epoch 1 step 163] train loss: 0.6942544380410007, dev loss: 0.6925791080915614\n",
      "[epoch 1 step 164] train loss: 0.6945667421500098, dev loss: 0.6926047797749328\n",
      "[epoch 1 step 165] train loss: 0.6867237855818535, dev loss: 0.6927258275332014\n",
      "[epoch 1 step 166] train loss: 0.6852212898418587, dev loss: 0.6931042902652176\n",
      "[epoch 1 step 167] train loss: 0.7002169018269639, dev loss: 0.6926836752380127\n",
      "[epoch 1 step 168] train loss: 0.6930813780151344, dev loss: 0.6926448227080538\n",
      "[epoch 1 step 169] train loss: 0.6866456600112358, dev loss: 0.6929142332905117\n",
      "[epoch 1 step 170] train loss: 0.6903050956606155, dev loss: 0.6929643692534216\n",
      "[epoch 1 step 171] train loss: 0.696555318978542, dev loss: 0.6927272055688326\n",
      "[epoch 1 step 172] train loss: 0.695706296546832, dev loss: 0.6925953046135156\n",
      "[epoch 1 step 173] train loss: 0.6972669130988114, dev loss: 0.6925625152621151\n",
      "[epoch 1 step 174] train loss: 0.691798849454691, dev loss: 0.6925486595283733\n",
      "[epoch 1 step 175] train loss: 0.693374239966362, dev loss: 0.692555601597078\n",
      "[epoch 1 step 176] train loss: 0.6854046800650122, dev loss: 0.6928210701906989\n",
      "[epoch 1 step 177] train loss: 0.6942104737945082, dev loss: 0.6927003976452483\n",
      "[epoch 1 step 178] train loss: 0.6991773267093576, dev loss: 0.6925526163264872\n",
      "[epoch 1 step 179] train loss: 0.6898508764614736, dev loss: 0.6926145218920299\n",
      "[epoch 1 step 180] train loss: 0.6937565741354179, dev loss: 0.6925673752218939\n",
      "[epoch 1 step 181] train loss: 0.6894580192962998, dev loss: 0.6926292659649834\n",
      "[epoch 1 step 182] train loss: 0.6986386271475417, dev loss: 0.6925474386616258\n",
      "[epoch 1 step 183] train loss: 0.6922332439116297, dev loss: 0.6925297099898944\n",
      "[epoch 1 step 184] train loss: 0.6938942621213927, dev loss: 0.6925443907995295\n",
      "[epoch 1 step 185] train loss: 0.6907607901782618, dev loss: 0.6925271556746346\n",
      "[epoch 1 step 186] train loss: 0.6952909884206977, dev loss: 0.6925516359598908\n",
      "[epoch 1 step 187] train loss: 0.6951682453117214, dev loss: 0.6926667063452555\n",
      "[epoch 1 step 188] train loss: 0.6960784733925145, dev loss: 0.6931536722877208\n",
      "[epoch 1 step 189] train loss: 0.6934420777488974, dev loss: 0.6935750816135767\n",
      "[epoch 1 step 190] train loss: 0.6931631464933714, dev loss: 0.6936196060715352\n",
      "[epoch 1 step 191] train loss: 0.6940976952347784, dev loss: 0.6930922268657842\n",
      "[epoch 1 step 192] train loss: 0.6933279486442768, dev loss: 0.6931767063767156\n",
      "[epoch 1 step 193] train loss: 0.6932510844673746, dev loss: 0.6932566132989704\n",
      "[epoch 1 step 194] train loss: 0.6932130095682105, dev loss: 0.693420268269636\n",
      "[epoch 1 step 195] train loss: 0.6936638160581068, dev loss: 0.6929624053988861\n",
      "[epoch 1 step 196] train loss: 0.6920231605459224, dev loss: 0.6925746207739617\n",
      "[epoch 1 step 197] train loss: 0.695685131601115, dev loss: 0.6928134929190635\n",
      "[epoch 1 step 198] train loss: 0.6926998344013078, dev loss: 0.6927157785230049\n",
      "[epoch 1 step 199] train loss: 0.6936648688041502, dev loss: 0.6928178157266124\n",
      "[epoch 1 step 200] train loss: 0.6933135232303083, dev loss: 0.6928501198017379\n",
      "[epoch 1 step 201] train loss: 0.6899793958816094, dev loss: 0.692544762959391\n",
      "[epoch 1 step 202] train loss: 0.6957099309965534, dev loss: 0.6925163132641381\n",
      "[epoch 1 step 203] train loss: 0.6972731562014095, dev loss: 0.6926775955529597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1 step 204] train loss: 0.6937391541124291, dev loss: 0.6927613090256808\n",
      "[epoch 1 step 205] train loss: 0.693095844480648, dev loss: 0.6927598648781701\n",
      "[epoch 1 step 206] train loss: 0.6914213768841365, dev loss: 0.6925389245286542\n",
      "[epoch 1 step 207] train loss: 0.6958944997018546, dev loss: 0.6927495301992148\n",
      "[epoch 1 step 208] train loss: 0.6929089258427168, dev loss: 0.6927099071986237\n",
      "[epoch 1 step 209] train loss: 0.6928353041303219, dev loss: 0.6926724506493047\n",
      "[epoch 1 step 210] train loss: 0.6946068895372888, dev loss: 0.6929200277394606\n",
      "[epoch 1 step 211] train loss: 0.6935325959577212, dev loss: 0.6930846498528963\n",
      "[epoch 1 step 212] train loss: 0.693355389920678, dev loss: 0.6932414954905037\n",
      "[epoch 1 step 213] train loss: 0.6932442154606036, dev loss: 0.6930991417014465\n",
      "[epoch 1 step 214] train loss: 0.6925335747063004, dev loss: 0.6925370438025148\n",
      "[epoch 1 step 215] train loss: 0.6936366269295373, dev loss: 0.6925751297969568\n",
      "[epoch 1 step 216] train loss: 0.6927313291578416, dev loss: 0.6925644613991567\n",
      "[epoch 1 step 217] train loss: 0.6971716082365207, dev loss: 0.6930421233886411\n",
      "[epoch 1 step 218] train loss: 0.6925611233831583, dev loss: 0.6926267218401166\n",
      "[epoch 1 step 219] train loss: 0.6951775803703109, dev loss: 0.6929002864128226\n",
      "[epoch 1 step 220] train loss: 0.6916829328262187, dev loss: 0.692521243937502\n",
      "[epoch 1 step 221] train loss: 0.6922676016418277, dev loss: 0.6925074241691355\n",
      "[epoch 1 step 222] train loss: 0.6947261638580756, dev loss: 0.6925858495263104\n"
     ]
    }
   ],
   "source": [
    "KEEP_PROB = 0.8\n",
    "NUM_UNITS = 256\n",
    "DENSE_SIZE = 128\n",
    "INIT_STD = 0.1\n",
    "INIT_LEARN = 0.5\n",
    "\n",
    "NUM_EPOCH = 3\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "num_train = len(train_seq1)\n",
    "num_batches = num_train // BATCH_SIZE\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope(\"input_layer\"):\n",
    "        word1 = tf.placeholder(shape=(None, WORD_SEQ_LEN), dtype=tf.int64, name=\"word1_input\")\n",
    "        word2 = tf.placeholder(shape=(None, WORD_SEQ_LEN), dtype=tf.int64, name=\"word2_input\")\n",
    "        y = tf.placeholder(shape=(None,), dtype=tf.int64, name=\"true_label\")\n",
    "        y_ = tf.cast(y, dtype=tf.float64)\n",
    "        \n",
    "    with tf.variable_scope(\"embedding_layer\"):\n",
    "        word_embedding = tf.Variable(word_embedding_data, trainable=False, name=\"word_embedding\")\n",
    "        word_vector1 = tf.nn.embedding_lookup(word_embedding, word1, name=\"embedding_looking\")\n",
    "        word_vector2 = tf.nn.embedding_lookup(word_embedding, word2, name=\"embedding_looking\")\n",
    "    \n",
    "    with tf.variable_scope(\"lstm_layer\"):\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(num_units=NUM_UNITS, name=\"word_lstm\")\n",
    "        lstm_output, _ = tf.nn.dynamic_rnn(cell, word_vector1, dtype=tf.float64)\n",
    "        final_output = lstm_output[:, -1, :]\n",
    "    \n",
    "    with tf.variable_scope(\"output_layer\"):\n",
    "        input_norm = tf.layers.batch_normalization(final_output, axis=1, training=True)\n",
    "        dense = tf.layers.dense(input_norm, DENSE_SIZE, activation=tf.nn.relu,\n",
    "                                 kernel_initializer=tf.random_normal_initializer(stddev=INIT_STD),\n",
    "                                 bias_initializer=tf.zeros_initializer())\n",
    "        dense_norm = tf.layers.batch_normalization(dense, axis=1, training=True)\n",
    "        pred = tf.layers.dense(dense_norm, 1, activation=tf.nn.sigmoid,\n",
    "                               kernel_initializer=tf.random_normal_initializer(stddev=INIT_STD),\n",
    "                               bias_initializer=tf.zeros_initializer())\n",
    "    \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        loss = -tf.reduce_mean(y_ * tf.log(pred) + (1 - y_) * tf.log(1 - pred))\n",
    "    \n",
    "    with tf.variable_scope(\"train\"):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(INIT_LEARN)\n",
    "        grads, variables = zip(*optimizer.compute_gradients(loss, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)))\n",
    "        \n",
    "        global_step = tf.Variable(0, dtype=tf.int64, trainable=False, name=\"global_step\")\n",
    "        \n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        print(update_ops)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            train_op = optimizer.apply_gradients(zip(grads, variables), global_step=global_step)\n",
    "        \n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run([tf.global_variables_initializer()])\n",
    "    \n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        epoch_index = np.random.permutation(num_train)\n",
    "        for i in range(num_batches):\n",
    "            batch_index = epoch_index[i * BATCH_SIZE: (i + 1) * BATCH_SIZE]\n",
    "            data_w1 = train_seq1[batch_index, :]\n",
    "            data_w2 = train_seq2[batch_index, :]\n",
    "            data_y = train_y[batch_index]\n",
    "            \n",
    "            train_feed = {word1: data_w1, word2: data_w2, y: data_y}\n",
    "            _, train_loss, cur_step = sess.run([train_op, loss, global_step], feed_dict=train_feed)\n",
    "            dev_feed = {word1: dev_seq1, word2: dev_seq2, y: dev_y}\n",
    "            dev_loss = sess.run(loss, feed_dict=dev_feed)\n",
    "            \n",
    "            print(f\"[epoch {epoch + 1} step {cur_step}] train loss: {train_loss}, dev loss: {dev_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
