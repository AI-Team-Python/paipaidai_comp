{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/\"\n",
    "TRAIN_PATH = DATA_PATH + \"train.csv\"\n",
    "TEST_PATH = DATA_PATH + \"test.csv\"\n",
    "WORD_EMBED_PATH = DATA_PATH + \"word_embed.txt\"\n",
    "CHAR_EMBED_PATH = DATA_PATH + \"char_embed.txt\"\n",
    "QUEST_PATH = DATA_PATH + \"question.csv\"\n",
    "\n",
    "SEED = 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(TRAIN_PATH)\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "question_data = pd.read_csv(QUEST_PATH)\n",
    "word_embedding_data = pd.read_csv(WORD_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "char_embedding_data = pd.read_csv(CHAR_EMBED_PATH, delimiter=\" \", header=None, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "question_data[\"words\"] = question_data[\"words\"].str.split(\" \")\n",
    "question_data[\"chars\"] = question_data[\"chars\"].str.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20890, 3048)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "word_dict = Dictionary(question_data[\"words\"])\n",
    "char_dict = Dictionary(question_data[\"chars\"])\n",
    "\n",
    "num_words, num_chars = len(word_dict.dfs), len(char_dict.dfs)\n",
    "num_words, num_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_embedding(embedding, corp_dict):\n",
    "    index = sorted([k for k, v in corp_dict.token2id.items()], key=lambda x: corp_dict.token2id[x])\n",
    "    return embedding.reindex(index).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding_data = adjust_embedding(word_embedding_data, word_dict)\n",
    "char_embedding_data = adjust_embedding(char_embedding_data, char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_embedding(embedding, method=\"mean\"):\n",
    "    new_vector = np.zeros(embedding.shape[1], dtype=np.float32)\n",
    "    if method == \"mean\":\n",
    "        new_vector = embedding.mean(axis=0)\n",
    "    return np.concatenate((embedding, new_vector.reshape((1, -1))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "former word embedding matrix shape: (20890, 300)\n",
      "final word embedding matrix shape: (20891, 300)\n",
      "former char embedding matrix shape: (3048, 300)\n",
      "final char embedding matrix shape: (3049, 300)\n"
     ]
    }
   ],
   "source": [
    "print(f\"former word embedding matrix shape: {word_embedding_data.shape}\")\n",
    "word_embedding_data = pad_embedding(word_embedding_data, method=\"zero\")\n",
    "print(f\"final word embedding matrix shape: {word_embedding_data.shape}\")\n",
    "\n",
    "print(f\"former char embedding matrix shape: {char_embedding_data.shape}\")\n",
    "char_embedding_data = pad_embedding(char_embedding_data, method=\"zero\")\n",
    "print(f\"final char embedding matrix shape: {char_embedding_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['W17378', 'W19355', 'W16319', 'W18238', 'W18103'],\n",
       " ['L0104',\n",
       "  'L2214',\n",
       "  'L1861',\n",
       "  'L2582',\n",
       "  'L3019',\n",
       "  'L0143',\n",
       "  'L2218',\n",
       "  'L1132',\n",
       "  'L1128',\n",
       "  'L0362',\n",
       "  'L1187'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_STOP = 100000\n",
    "USE_STOP = True\n",
    "\n",
    "word_count = sorted([(token, word_dict.dfs[id_]) for token, id_ in word_dict.token2id.items()],\n",
    "                    key=lambda x: x[1], reverse=True)\n",
    "char_count = sorted([(token, char_dict.dfs[id_]) for token, id_ in char_dict.token2id.items()],\n",
    "                    key=lambda x: x[1], reverse=True)\n",
    "\n",
    "word_stop = [token for token, count in word_count if count >= T_STOP]\n",
    "char_stop = [token for token, count in char_count if count >= T_STOP]\n",
    "word_stop, char_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pair2vec(pair, question, use_stop=USE_STOP):\n",
    "    return ques2vec(pair[[\"q1\"]].rename(columns={\"q1\": \"qid\"}), question, use_stop) + \\\n",
    "           ques2vec(pair[[\"q2\"]].rename(columns={\"q2\": \"qid\"}), question, use_stop)\n",
    "    \n",
    "def ques2vec(ques, question, use_stop=True):\n",
    "    q = ques.merge(question, how=\"left\", on=\"qid\")\n",
    "    return seq2vec(q, \"words\", use_stop), seq2vec(q, \"chars\", use_stop)\n",
    "    \n",
    "def seq2vec(ques, col, use_stop=True):\n",
    "    seq = ques[col]\n",
    "    if col == \"words\":\n",
    "        d = word_dict.token2id\n",
    "        s = word_stop\n",
    "    else:\n",
    "        d = char_dict.token2id\n",
    "        s = char_stop\n",
    "    \n",
    "    if use_stop:\n",
    "        return [[d[token] for token in text if token not in s] for text in seq]\n",
    "    else:\n",
    "        return [[d[token] for token in text] for text in seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_word1, train_word2, train_char1, train_char2 = pair2vec(train_data, question_data)\n",
    "test_word1, test_word2, test_char1, test_char2 = pair2vec(test_data, question_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T_WORD = 6\n",
    "T_CHAR = 10\n",
    "\n",
    "def padding(text, length, pad_index, cut_method):\n",
    "    text = text[0]\n",
    "    num_text = len(text)\n",
    "    if num_text < length:\n",
    "        return text + [pad_index] * (length - num_text)\n",
    "    elif num_text > length:\n",
    "        if cut_method == \"direct\":\n",
    "            return text[:length]\n",
    "        else:\n",
    "            return text\n",
    "    return text\n",
    "\n",
    "def sample_padding(texts, length, pad_index, cut_method=\"direct\"):\n",
    "    texts = np.array(texts).reshape(-1, 1)\n",
    "    return np.apply_along_axis(padding, 1, texts, length, pad_index, cut_method=\"direct\")\n",
    "\n",
    "def padding_all(all_words, all_chars):\n",
    "    num_words = len(word_dict.dfs)\n",
    "    num_chars = len(char_dict.dfs)\n",
    "    words_res = [sample_padding(s, T_WORD, num_words) for s in all_words]\n",
    "    chars_res = [sample_padding(s, T_CHAR, num_chars) for s in all_chars]\n",
    "    return tuple(words_res + chars_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word1, train_word2, train_char1, train_char2 = padding_all([train_word1, train_word2], [train_char1, train_char2])\n",
    "test_word1, test_word2, test_char1, test_char2 = padding_all([test_word1, test_word2], [test_char1, test_char2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word1, dev_word1, \\\n",
    "train_word2, dev_word2, \\\n",
    "train_char1, dev_char1, \\\n",
    "train_char2, dev_char2, \\\n",
    "train_y, dev_y = train_test_split(train_word1, train_word2, train_char1, train_char2, train_data[\"label\"], test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "batch_size = 64\n",
    "keep_prob = 0.8\n",
    "unit_size = 128\n",
    "dense_size = 64\n",
    "learning_rate = 0.01\n",
    "clip_norm = 2\n",
    "weight_init = 1.0\n",
    "\n",
    "\n",
    "def lstm_cell(unit_size, keep):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(unit_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep)\n",
    "    return drop\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope(\"dataset\"):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((train_word1, train_word2, train_char1, train_char2, train_y))\n",
    "        dataset = dataset.repeat(epoch).shuffle(batch_size * 3 + 1000).batch(batch_size)\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        word1, word2, char1, char2, label = iterator.get_next()\n",
    "    \n",
    "    with tf.variable_scope(\"input\"):\n",
    "        w1 = tf.placeholder(shape=(None, T_WORD), dtype=tf.int64, name=\"w1\")\n",
    "        w2 = tf.placeholder(shape=(None, T_WORD), dtype=tf.int64, name=\"w2\")\n",
    "        c1 = tf.placeholder(shape=(None, T_CHAR), dtype=tf.int64, name=\"c1\")\n",
    "        c2 = tf.placeholder(shape=(None, T_CHAR), dtype=tf.int64, name=\"c2\")\n",
    "        y = tf.placeholder(shape=(None,), dtype=tf.float64, name=\"y_true\")\n",
    "        keep = tf.placeholder(shape=(), dtype=tf.float64, name=\"keep_prob\")\n",
    "    \n",
    "    with tf.variable_scope(\"embedding\"):\n",
    "        word_embedding = tf.Variable(word_embedding_data, trainable=False, name=\"word_embedding\")\n",
    "        char_embedding = tf.Variable(char_embedding_data, trainable=False, name=\"char_embedding\")\n",
    "        w1_embed = tf.nn.embedding_lookup(word_embedding, w1, name=\"word1_embed\")\n",
    "        w2_embed = tf.nn.embedding_lookup(word_embedding, w2, name=\"word2_embed\")\n",
    "        c1_embed = tf.nn.embedding_lookup(char_embedding, c1, name=\"char1_embed\")\n",
    "        c2_embed = tf.nn.embedding_lookup(char_embedding, c2, name=\"char2_embed\")\n",
    "    \n",
    "    with tf.variable_scope(\"word_layers\"):\n",
    "        word_cell = lstm_cell(unit_size, keep)\n",
    "        output_w1, _ = tf.nn.dynamic_rnn(word_cell, w1_embed, dtype=tf.float64)\n",
    "        output_w2, _ = tf.nn.dynamic_rnn(word_cell, w2_embed, dtype=tf.float64)\n",
    "    \n",
    "    with tf.variable_scope(\"char_layers\"):\n",
    "        char_cell = lstm_cell(unit_size, keep)\n",
    "        output_c1, _ = tf.nn.dynamic_rnn(char_cell, c1_embed, dtype=tf.float64)\n",
    "        output_c2, _ = tf.nn.dynamic_rnn(char_cell, c2_embed, dtype=tf.float64)\n",
    "        \n",
    "    with tf.variable_scope(\"combination\"):\n",
    "        o_w1, o_w2 = output_w1[:, -1, :], output_w2[:, -1, :]\n",
    "        o_c1, o_c2 = output_c1[:, -1, :], output_c2[:, -1, :]\n",
    "        final_output = tf.concat((o_w1, o_w2, o_c1, o_c2), axis=1)\n",
    "    \n",
    "    with tf.variable_scope(\"output_layer\"):\n",
    "        dense_output = tf.layers.dense(final_output, dense_size, activation=tf.nn.relu,\n",
    "                                       kernel_initializer=tf.truncated_normal_initializer(stddev=weight_init),\n",
    "                                       bias_initializer=tf.zeros_initializer)\n",
    "        pred = tf.layers.dense(dense_output, 1,\n",
    "                               activation=tf.nn.sigmoid, kernel_initializer=tf.truncated_normal_initializer(stddev=weight_init),\n",
    "                               bias_initializer=tf.zeros_initializer)\n",
    "    \n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        loss = -tf.reduce_mean(y * tf.log(pred) + (1 - y) * tf.log(1 - pred))\n",
    "        \n",
    "    with tf.variable_scope(\"train\"):\n",
    "        global_step = tf.Variable(0, trainable=False, dtype=tf.int64, name=\"global_step\")\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        # grads, variables = zip(*optimizer.compute_gradients(loss, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)))\n",
    "        # clip_grads = tf.clip_by_global_norm(grads, clip_norm)[0]\n",
    "        # train_op = optimizer.apply_gradients(zip(clip_grads, variables), global_step=global_step)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    tf.summary.scalar(\"batch_loss\", loss)\n",
    "    for v in tf.trainable_variables():\n",
    "        tf.summary.histogram(v.name, v)\n",
    "    for g in clip_grads:\n",
    "        tf.summary.histogram(\"grad_\" + g.name, g)\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"./log/\", graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    tf.local_variables_initializer().run()\n",
    "    sess.run(iterator.initializer)\n",
    "    \n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            res_w1, res_w2, res_c1, res_c2, res_y = sess.run([word1, word2, char1, char2, label])\n",
    "            train_feed = {\n",
    "                w1: res_w1,\n",
    "                w2: res_w2,\n",
    "                c1: res_c1,\n",
    "                c2: res_c2,\n",
    "                y: res_y,\n",
    "                keep: keep_prob,\n",
    "            }\n",
    "            \n",
    "            _, total_steps, train_loss, summary = sess.run([train_op, global_step, loss, summary_op], feed_dict=train_feed)\n",
    "            writer.add_summary(summary, global_step=total_steps)\n",
    "            \n",
    "            if total_steps % 100  == 0:\n",
    "                test_feed = {\n",
    "                    w1: dev_word1,\n",
    "                    w2: dev_word2,\n",
    "                    c1: dev_char1,\n",
    "                    c2: dev_char2,\n",
    "                    y: dev_y,\n",
    "                    keep: 1.0,\n",
    "                }\n",
    "                dev_less = sess.run(loss, feed_dict=test_feed)\n",
    "                \n",
    "                print(f\"[step {total_steps}] train loss: {train_loss}, dev loss: {dev_less}\")\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
