{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型结构\n",
    "\n",
    "- 两层单向LSTM, 输出序列结果, 即(batch_size, step_size, feature_size)\n",
    "- 分别输入到1, 2, 3, 4, 5, 6共6个不同长度的卷积层中\n",
    "- 卷积层为双层, 最后的池化层有Average和Max两种\n",
    "- 对于每个问题, 将所有卷积核结果并起来\n",
    "- 将两个问题并起来的结果, 分别[相减并取绝对值], [x]\n",
    "- 融入特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"./log/\")\n",
    "    os.mkdir(\"./result/\")\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/\"\n",
    "TRAIN_PATH = DATA_PATH + \"train.csv\"\n",
    "TEST_PATH = DATA_PATH + \"test.csv\"\n",
    "WORD_EMBED_PATH = DATA_PATH + \"word_embed.txt\"\n",
    "CHAR_EMBED_PATH = DATA_PATH + \"char_embed.txt\"\n",
    "QUEST_PATH = DATA_PATH + \"question.csv\"\n",
    "TRAIN_FEATURE = DATA_PATH + \"train_feature.csv\"\n",
    "TEST_FEATURE = DATA_PATH + \"test_feature.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10001, 300), (3049, 300))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(TRAIN_PATH)\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "question_data = pd.read_csv(QUEST_PATH)\n",
    "word_embedding_data = pd.read_csv(WORD_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "char_embedding_data = pd.read_csv(CHAR_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "train_feature = pd.read_csv(TRAIN_FEATURE).values\n",
    "test_feature = pd.read_csv(TEST_FEATURE).values\n",
    "\n",
    "question_data[\"words\"] = question_data[\"words\"].str.split(\" \")\n",
    "question_data[\"chars\"] = question_data[\"chars\"].str.split(\" \")\n",
    "\n",
    "label = train_data[\"label\"].values\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_COUNT = 10000\n",
    "\n",
    "word_tokenizer = Tokenizer(MAX_COUNT)\n",
    "word_tokenizer.fit_on_texts(question_data[\"words\"])\n",
    "\n",
    "word_embedding_data = np.concatenate(\n",
    "    (\n",
    "        np.zeros(shape=(1, word_embedding_data.shape[1]), dtype=np.float64),\n",
    "        word_embedding_data.loc[list(word_tokenizer.word_index.keys())[:MAX_COUNT]].values\n",
    "    ),\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "char_tokenizer = Tokenizer(MAX_COUNT)\n",
    "char_tokenizer.fit_on_texts(question_data[\"chars\"])\n",
    "\n",
    "char_embedding_data = np.concatenate(\n",
    "    (\n",
    "        np.zeros(shape=(1, char_embedding_data.shape[1]), dtype=np.float64),\n",
    "        char_embedding_data.loc[list(char_tokenizer.word_index.keys())[:MAX_COUNT]].values\n",
    "    ),\n",
    "    axis=0\n",
    ")\n",
    "\n",
    "word_embedding_data.shape, char_embedding_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((254386, 25),\n",
       " (254386, 25),\n",
       " (172956, 25),\n",
       " (172956, 25),\n",
       " (254386, 25),\n",
       " (254386, 25),\n",
       " (172956, 25),\n",
       " (172956, 25))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "SEQ_LEN = 25\n",
    "\n",
    "def gen_word_data(data):\n",
    "    seq_word1 = word_tokenizer.texts_to_sequences(data.merge(question_data, how=\"left\", left_on=\"q1\", right_on=\"qid\")[\"words\"])\n",
    "    seq_word2 = word_tokenizer.texts_to_sequences(data.merge(question_data, how=\"left\", left_on=\"q2\", right_on=\"qid\")[\"words\"])\n",
    "    return pad_sequences(seq_word1, maxlen=SEQ_LEN, padding=\"pre\",truncating=\"pre\"), \\\n",
    "        pad_sequences(seq_word2, maxlen=SEQ_LEN, padding=\"pre\",truncating=\"pre\")\n",
    "    \n",
    "def gen_char_data(data):\n",
    "    seq_char1 = char_tokenizer.texts_to_sequences(data.merge(question_data, how=\"left\", left_on=\"q1\", right_on=\"qid\")[\"chars\"])\n",
    "    seq_char2 = char_tokenizer.texts_to_sequences(data.merge(question_data, how=\"left\", left_on=\"q2\", right_on=\"qid\")[\"chars\"])\n",
    "    return pad_sequences(seq_char1, maxlen=SEQ_LEN, padding=\"pre\",truncating=\"pre\"), \\\n",
    "        pad_sequences(seq_char2, maxlen=SEQ_LEN, padding=\"pre\",truncating=\"pre\")\n",
    "\n",
    "word1, word2 = gen_word_data(train_data)\n",
    "char1, char2 = gen_char_data(train_data)\n",
    "test_word1, test_word2 = gen_word_data(test_data)\n",
    "test_char1, test_char2 = gen_char_data(test_data)\n",
    "\n",
    "word1.shape, word2.shape, test_word1.shape, test_word2.shape, char1.shape, char2.shape, test_char1.shape, test_char2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import LSTM, Bidirectional, TimeDistributed\n",
    "from keras.layers import Conv1D, MaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dropout, BatchNormalization, Dense, Flatten, Lambda, K, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general\n",
    "NUM_EPOCHES = 25\n",
    "EPOCHES1 = 5\n",
    "EPOCHES2 = 25 # 5\n",
    "EPOCHES3 = 22\n",
    "BATCH_SIZE = 1024\n",
    "DROP_RATE = 0.3\n",
    "\n",
    "# cnn\n",
    "CONV_LEN1 = 128\n",
    "CONV_LEN2 = 128\n",
    "CONV_LEN3 = 128\n",
    "CONV_LEN4 = 128\n",
    "CONV_LEN5 = 128\n",
    "CONV_LEN6 = 128\n",
    "\n",
    "# lstm\n",
    "LSTM_SIZE1 = 256\n",
    "LSTM_SIZE2 = 256\n",
    "LSTM_DROP_RATE = 0.3\n",
    "\n",
    "# dense\n",
    "DENSE_SIZE1 = 512\n",
    "DENSE_SIZE2 = 256\n",
    "DENSE_FEATURE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_double_layer(inputa, inputb, filters, kernel_size):\n",
    "    conv1 = Conv1D(filters=filters, kernel_size=kernel_size, padding=\"same\")\n",
    "    conv2 = Conv1D(filters=filters, kernel_size=kernel_size, padding=\"same\")\n",
    "    \n",
    "    conv1a = conv1(inputa)\n",
    "    conv1a = BatchNormalization()(conv1a)\n",
    "    conv1a = Activation(activation=\"relu\")(conv1a)\n",
    "    conv2a = conv2(conv1a)\n",
    "    conv2a = BatchNormalization()(conv2a)\n",
    "    conv2a = Activation(activation=\"relu\")(conv2a)\n",
    "    output_avg_a = GlobalAveragePooling1D()(conv2a)\n",
    "    output_max_a = Flatten()(MaxPool1D(pool_size=SEQ_LEN)(conv2a))\n",
    "    output_a = concatenate([output_avg_a, output_max_a])\n",
    "    \n",
    "    conv1b = conv1(inputb)\n",
    "    conv1b = BatchNormalization()(conv1b)\n",
    "    conv1b = Activation(activation=\"relu\")(conv1b)\n",
    "    conv2b = conv2(conv1b)\n",
    "    conv2b = BatchNormalization()(conv2b)\n",
    "    conv2b = Activation(activation=\"relu\")(conv2b)\n",
    "    output_avg_b = GlobalAveragePooling1D()(conv2b)\n",
    "    output_max_b = Flatten()(MaxPool1D(pool_size=SEQ_LEN)(conv2b))\n",
    "    output_b = concatenate([output_avg_b, output_max_b])\n",
    "    \n",
    "    return output_a, output_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# best_results = []\n",
    "# # last_results = []\n",
    "# best_file_names = []\n",
    "# dev_predictions = []\n",
    "\n",
    "# for i, (train_index, dev_index) in enumerate(StratifiedKFold(n_splits=10).split(X=word1, y=label)):  # word/char switch\n",
    "#     print(\"-\" * 60)\n",
    "#     print(\"Fold {} training start...\".format(i))\n",
    "    \n",
    "#     train_x1, train_x2, train_y = word1[train_index, :], word2[train_index, :], label[train_index]  # word/char switch\n",
    "#     dev_x1, dev_x2, dev_y = word1[dev_index, :], word2[dev_index, :], label[dev_index]  # word/char switch\n",
    "#     train_f, dev_f = train_feature[train_index, :], train_feature[dev_index, :]\n",
    "    \n",
    "#     input1 = Input(shape=(SEQ_LEN,), dtype=\"int32\")\n",
    "#     input2 = Input(shape=(SEQ_LEN,), dtype=\"int32\")\n",
    "#     inputf = Input(shape=(train_f.shape[1],), dtype=\"float32\")\n",
    "\n",
    "#     embedding_layer = Embedding(\n",
    "#         input_dim=word_embedding_data.shape[0],  # word/char switch\n",
    "#         output_dim=word_embedding_data.shape[1],  # word/char switch\n",
    "#         weights=[word_embedding_data],  # word/char switch\n",
    "#         input_length=SEQ_LEN,\n",
    "#         trainable=False\n",
    "#     )\n",
    "    \n",
    "#     vector1 = embedding_layer(input1)\n",
    "#     vector2 = embedding_layer(input2)\n",
    "    \n",
    "#     lstm_layer1 = LSTM(LSTM_SIZE1, dropout=LSTM_DROP_RATE, recurrent_dropout=LSTM_DROP_RATE, return_sequences=True)\n",
    "#     layer1a = lstm_layer1(vector1)\n",
    "#     layer1a = Dropout(LSTM_DROP_RATE)(layer1a)\n",
    "#     layer1b = lstm_layer1(vector2)\n",
    "#     layer1b = Dropout(LSTM_DROP_RATE)(layer1b)\n",
    "\n",
    "#     lstm_layer2 = LSTM(LSTM_SIZE2, dropout=LSTM_DROP_RATE, recurrent_dropout=LSTM_DROP_RATE, return_sequences=True)\n",
    "#     layer2a = lstm_layer2(layer1a)\n",
    "#     layer2b = lstm_layer2(layer1b)\n",
    "    \n",
    "#     conv1a, conv1b = cnn_double_layer(layer2a, layer2b, filters=CONV_LEN1, kernel_size=1)\n",
    "#     conv2a, conv2b = cnn_double_layer(layer2a, layer2b, filters=CONV_LEN2, kernel_size=2)\n",
    "#     conv3a, conv3b = cnn_double_layer(layer2a, layer2b, filters=CONV_LEN3, kernel_size=3)\n",
    "#     conv4a, conv4b = cnn_double_layer(layer2a, layer2b, filters=CONV_LEN4, kernel_size=4)\n",
    "#     conv5a, conv5b = cnn_double_layer(layer2a, layer2b, filters=CONV_LEN5, kernel_size=5)\n",
    "#     conv6a, conv6b = cnn_double_layer(layer2a, layer2b, filters=CONV_LEN6, kernel_size=6)\n",
    "    \n",
    "#     merge_a = concatenate([conv1a, conv2a, conv3a, conv4a, conv5a, conv6a])\n",
    "#     merge_b = concatenate([conv1b, conv2b, conv3b, conv4b, conv5b, conv6b])\n",
    "#     diff = Lambda(lambda x: K.abs(x[0] - x[1]))([merge_a, merge_b])\n",
    "#     mult = Lambda(lambda x: x[0] * x[1])([merge_a, merge_b])\n",
    "#     merge = concatenate([diff, mult])\n",
    "    \n",
    "#     x = Dropout(DROP_RATE)(merge)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dense(DENSE_SIZE1, activation=\"relu\")(x)\n",
    "    \n",
    "#     fe = BatchNormalization()(inputf)\n",
    "#     fe = Dense(DENSE_FEATURE, activation=\"relu\")(fe)\n",
    "    \n",
    "#     x = concatenate([x, fe])\n",
    "    \n",
    "#     x = Dropout(DROP_RATE)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dense(DENSE_SIZE2, activation=\"relu\")(x)\n",
    "#     x = Dropout(DROP_RATE)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     pred = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "#     model = Model(inputs=[input1, input2, inputf], outputs=pred)\n",
    "#     model.compile(\n",
    "#         optimizer=\"nadam\",\n",
    "#         loss=\"binary_crossentropy\",\n",
    "#         metrics=[\"acc\"]\n",
    "#     )\n",
    "    \n",
    "#     early_stopping = EarlyStopping(\"val_loss\", patience=6)\n",
    "#     lr_reducer = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.0005)\n",
    "#     check_point = ModelCheckpoint(\n",
    "#         \"./log/%s.Multi_LSTM_CNN_v3.word.{epoch:03d}.hdf5\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),  # word/char switch\n",
    "#         monitor=\"val_loss\",\n",
    "#         save_best_only=True,\n",
    "#     )\n",
    "    \n",
    "#     fit_res = model.fit(\n",
    "#         x=[train_x1, train_x2, train_f],\n",
    "#         y=train_y,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         epochs=NUM_EPOCHES,\n",
    "#         validation_data=([dev_x1, dev_x2, dev_f], dev_y),\n",
    "#         shuffle=True,\n",
    "#         callbacks=[early_stopping, lr_reducer, check_point]\n",
    "#     )\n",
    "    \n",
    "# #     pred_last = model.predict([test_word1, test_word2], batch_size=BATCH_SIZE)  # word/char switch\n",
    "# #     last_results.append(pd.DataFrame(pred_last, columns=[\"y_pre\"]))\n",
    "    \n",
    "#     best_model_file = glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\")\n",
    "#     best_file_names.append(best_model_file)\n",
    "#     print(\"load model %s\" % (best_model_file,))\n",
    "#     model.load_weights(best_model_file)\n",
    "#     pred_best = model.predict([test_word1, test_word2, test_feature], batch_size=BATCH_SIZE)  # word/char switch\n",
    "#     best_results.append(pd.DataFrame(pred_best, columns=[\"y_pre\"]))\n",
    "    \n",
    "#     dev_pred = model.predict([dev_x1, dev_x2, dev_f], batch_size=BATCH_SIZE)\n",
    "#     dev_result = pd.DataFrame({\"pred\": dev_pred.ravel(), \"label\": dev_y})\n",
    "#     dev_predictions.append(dev_result)\n",
    "\n",
    "# # pd.DataFrame(pd.concat(last_results, axis=1).mean(axis=1), columns=[\"y_pre\"]).to_csv(\n",
    "# #     \"./result/%s-Multi_LSTM_CNN_v5_word_last.csv\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),  # word/char switch\n",
    "# #     index=False\n",
    "# # )\n",
    "# pd.DataFrame(pd.concat(best_results, axis=1).mean(axis=1), columns=[\"y_pre\"]).to_csv(\n",
    "#     \"./result/%s-Multi_LSTM_CNN_v5_word_best.csv\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),  # word/char switch\n",
    "#     index=False\n",
    "# )\n",
    "\n",
    "# total_dev = pd.concat(dev_predictions, axis=0)\n",
    "# total_dev.to_csv(\n",
    "#     \"./result/%s-Multi_LSTM_CNN_v5_word_dev_result.csv\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "#     index=False\n",
    "# )\n",
    "\n",
    "# model_path = \"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "# os.mkdir(model_path)\n",
    "# for model_name in best_file_names:\n",
    "#     abs_name = os.path.split(model_name)[1]\n",
    "#     os.rename(model_name, model_path + abs_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Fold 0 training start...\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Train on 228946 samples, validate on 25440 samples\n",
      "Epoch 1/25\n",
      "228946/228946 [==============================] - 198s 866us/step - loss: 0.3947 - acc: 0.8071 - val_loss: 0.3225 - val_acc: 0.8560\n",
      "Epoch 2/25\n",
      "228946/228946 [==============================] - 182s 797us/step - loss: 0.2892 - acc: 0.8666 - val_loss: 0.2669 - val_acc: 0.8855\n",
      "Epoch 3/25\n",
      "228946/228946 [==============================] - 187s 818us/step - loss: 0.2522 - acc: 0.8873 - val_loss: 0.2193 - val_acc: 0.9046\n",
      "Epoch 4/25\n",
      "228946/228946 [==============================] - 193s 843us/step - loss: 0.2297 - acc: 0.8979 - val_loss: 0.2147 - val_acc: 0.9078\n",
      "Epoch 5/25\n",
      "228946/228946 [==============================] - 194s 848us/step - loss: 0.2145 - acc: 0.9059 - val_loss: 0.1984 - val_acc: 0.9136\n",
      "Epoch 6/25\n",
      "228946/228946 [==============================] - 182s 797us/step - loss: 0.2030 - acc: 0.9114 - val_loss: 0.1981 - val_acc: 0.9162\n",
      "Epoch 7/25\n",
      "228946/228946 [==============================] - 188s 823us/step - loss: 0.1934 - acc: 0.9166 - val_loss: 0.1876 - val_acc: 0.9191\n",
      "Epoch 8/25\n",
      "228946/228946 [==============================] - 185s 809us/step - loss: 0.1857 - acc: 0.9196 - val_loss: 0.1846 - val_acc: 0.9231\n",
      "Epoch 9/25\n",
      "228946/228946 [==============================] - 192s 839us/step - loss: 0.1798 - acc: 0.9231 - val_loss: 0.1780 - val_acc: 0.9268\n",
      "Epoch 10/25\n",
      "228946/228946 [==============================] - 187s 818us/step - loss: 0.1743 - acc: 0.9254 - val_loss: 0.1820 - val_acc: 0.9276\n",
      "Epoch 11/25\n",
      "228946/228946 [==============================] - 192s 838us/step - loss: 0.1693 - acc: 0.9275 - val_loss: 0.1749 - val_acc: 0.9291\n",
      "Epoch 12/25\n",
      "228946/228946 [==============================] - 193s 842us/step - loss: 0.1663 - acc: 0.9290 - val_loss: 0.1740 - val_acc: 0.9301\n",
      "Epoch 13/25\n",
      "228946/228946 [==============================] - 193s 844us/step - loss: 0.1614 - acc: 0.9309 - val_loss: 0.1744 - val_acc: 0.9292\n",
      "Epoch 14/25\n",
      "228946/228946 [==============================] - 193s 844us/step - loss: 0.1577 - acc: 0.9333 - val_loss: 0.1710 - val_acc: 0.9306\n",
      "Epoch 15/25\n",
      "228946/228946 [==============================] - 194s 846us/step - loss: 0.1543 - acc: 0.9346 - val_loss: 0.1704 - val_acc: 0.9309\n",
      "Epoch 16/25\n",
      "228946/228946 [==============================] - 192s 839us/step - loss: 0.1524 - acc: 0.9356 - val_loss: 0.1730 - val_acc: 0.9318\n",
      "Epoch 17/25\n",
      "228946/228946 [==============================] - 193s 842us/step - loss: 0.1477 - acc: 0.9377 - val_loss: 0.1647 - val_acc: 0.9346\n",
      "Epoch 18/25\n",
      "228946/228946 [==============================] - 183s 799us/step - loss: 0.1451 - acc: 0.9387 - val_loss: 0.1720 - val_acc: 0.9333\n",
      "Epoch 19/25\n",
      "228946/228946 [==============================] - 183s 799us/step - loss: 0.1433 - acc: 0.9395 - val_loss: 0.1699 - val_acc: 0.9341\n",
      "Epoch 20/25\n",
      "228946/228946 [==============================] - 190s 831us/step - loss: 0.1409 - acc: 0.9407 - val_loss: 0.1677 - val_acc: 0.9338\n",
      "Epoch 21/25\n",
      "228946/228946 [==============================] - 190s 831us/step - loss: 0.1315 - acc: 0.9447 - val_loss: 0.1687 - val_acc: 0.9346\n",
      "Epoch 22/25\n",
      "228946/228946 [==============================] - 193s 842us/step - loss: 0.1256 - acc: 0.9471 - val_loss: 0.1647 - val_acc: 0.9372\n",
      "Epoch 23/25\n",
      "228946/228946 [==============================] - 193s 845us/step - loss: 0.1222 - acc: 0.9490 - val_loss: 0.1644 - val_acc: 0.9365\n",
      "Epoch 24/25\n",
      "228946/228946 [==============================] - 192s 839us/step - loss: 0.1197 - acc: 0.9498 - val_loss: 0.1695 - val_acc: 0.9358\n",
      "Epoch 25/25\n",
      "228946/228946 [==============================] - 192s 840us/step - loss: 0.1168 - acc: 0.9513 - val_loss: 0.1724 - val_acc: 0.9350\n",
      "load model ./log/20180715-102931.Multi_LSTM_CNN_v5.char.023.hdf5\n",
      "------------------------------------------------------------\n",
      "Fold 1 training start...\n",
      "Train on 228946 samples, validate on 25440 samples\n",
      "Epoch 1/25\n",
      "228946/228946 [==============================] - 198s 866us/step - loss: 0.3974 - acc: 0.8062 - val_loss: 0.3109 - val_acc: 0.8622\n",
      "Epoch 2/25\n",
      "228946/228946 [==============================] - 185s 807us/step - loss: 0.2894 - acc: 0.8665 - val_loss: 0.2533 - val_acc: 0.8928\n",
      "Epoch 3/25\n",
      "228946/228946 [==============================] - 190s 828us/step - loss: 0.2506 - acc: 0.8879 - val_loss: 0.2277 - val_acc: 0.9017\n",
      "Epoch 4/25\n",
      "228946/228946 [==============================] - 187s 815us/step - loss: 0.2285 - acc: 0.8992 - val_loss: 0.2086 - val_acc: 0.9099\n",
      "Epoch 5/25\n",
      "228946/228946 [==============================] - 183s 798us/step - loss: 0.2128 - acc: 0.9065 - val_loss: 0.2071 - val_acc: 0.9135\n",
      "Epoch 6/25\n",
      "228946/228946 [==============================] - 183s 798us/step - loss: 0.2018 - acc: 0.9124 - val_loss: 0.1964 - val_acc: 0.9168\n",
      "Epoch 7/25\n",
      "228946/228946 [==============================] - 189s 827us/step - loss: 0.1925 - acc: 0.9173 - val_loss: 0.1846 - val_acc: 0.9245\n",
      "Epoch 8/25\n",
      "228946/228946 [==============================] - 183s 799us/step - loss: 0.1860 - acc: 0.9197 - val_loss: 0.1793 - val_acc: 0.9238\n",
      "Epoch 9/25\n",
      "228946/228946 [==============================] - 188s 822us/step - loss: 0.1799 - acc: 0.9231 - val_loss: 0.1794 - val_acc: 0.9254\n",
      "Epoch 10/25\n",
      "228946/228946 [==============================] - 192s 839us/step - loss: 0.1736 - acc: 0.9255 - val_loss: 0.1836 - val_acc: 0.9225\n",
      "Epoch 11/25\n",
      "228946/228946 [==============================] - 188s 820us/step - loss: 0.1688 - acc: 0.9276 - val_loss: 0.1729 - val_acc: 0.9289\n",
      "Epoch 12/25\n",
      "228946/228946 [==============================] - 190s 831us/step - loss: 0.1657 - acc: 0.9291 - val_loss: 0.1711 - val_acc: 0.9296\n",
      "Epoch 13/25\n",
      "228946/228946 [==============================] - 191s 836us/step - loss: 0.1609 - acc: 0.9311 - val_loss: 0.1759 - val_acc: 0.9300\n",
      "Epoch 14/25\n",
      "228946/228946 [==============================] - 191s 835us/step - loss: 0.1578 - acc: 0.9328 - val_loss: 0.1673 - val_acc: 0.9318\n",
      "Epoch 15/25\n",
      "228946/228946 [==============================] - 191s 833us/step - loss: 0.1544 - acc: 0.9346 - val_loss: 0.1677 - val_acc: 0.9309\n",
      "Epoch 16/25\n",
      "228946/228946 [==============================] - 192s 840us/step - loss: 0.1521 - acc: 0.9353 - val_loss: 0.1710 - val_acc: 0.9315\n",
      "Epoch 17/25\n",
      "228946/228946 [==============================] - 193s 845us/step - loss: 0.1481 - acc: 0.9372 - val_loss: 0.1729 - val_acc: 0.9318\n",
      "Epoch 18/25\n",
      "228946/228946 [==============================] - 188s 820us/step - loss: 0.1380 - acc: 0.9415 - val_loss: 0.1689 - val_acc: 0.9338\n",
      "Epoch 19/25\n",
      "228946/228946 [==============================] - 191s 836us/step - loss: 0.1325 - acc: 0.9440 - val_loss: 0.1628 - val_acc: 0.9357\n",
      "Epoch 20/25\n",
      "228946/228946 [==============================] - 183s 798us/step - loss: 0.1287 - acc: 0.9464 - val_loss: 0.1655 - val_acc: 0.9359\n",
      "Epoch 21/25\n",
      "228946/228946 [==============================] - 183s 798us/step - loss: 0.1250 - acc: 0.9476 - val_loss: 0.1651 - val_acc: 0.9364\n",
      "Epoch 22/25\n",
      "228946/228946 [==============================] - 183s 798us/step - loss: 0.1227 - acc: 0.9490 - val_loss: 0.1689 - val_acc: 0.9354\n",
      "Epoch 23/25\n",
      "228946/228946 [==============================] - 183s 798us/step - loss: 0.1152 - acc: 0.9512 - val_loss: 0.1671 - val_acc: 0.9377\n",
      "Epoch 24/25\n",
      "228946/228946 [==============================] - 183s 799us/step - loss: 0.1117 - acc: 0.9535 - val_loss: 0.1682 - val_acc: 0.9373\n",
      "Epoch 25/25\n",
      "228946/228946 [==============================] - 189s 828us/step - loss: 0.1096 - acc: 0.9542 - val_loss: 0.1683 - val_acc: 0.9369\n",
      "load model ./log/20180715-115006.Multi_LSTM_CNN_v5.char.019.hdf5\n",
      "------------------------------------------------------------\n",
      "Fold 2 training start...\n",
      "Train on 228947 samples, validate on 25439 samples\n",
      "Epoch 1/25\n",
      "228947/228947 [==============================] - 193s 843us/step - loss: 0.3951 - acc: 0.8062 - val_loss: 0.3309 - val_acc: 0.8579\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228947/228947 [==============================] - 183s 800us/step - loss: 0.2904 - acc: 0.8656 - val_loss: 0.2692 - val_acc: 0.8873\n",
      "Epoch 3/25\n",
      "228947/228947 [==============================] - 183s 799us/step - loss: 0.2526 - acc: 0.8868 - val_loss: 0.2245 - val_acc: 0.9035\n",
      "Epoch 4/25\n",
      "228947/228947 [==============================] - 183s 800us/step - loss: 0.2295 - acc: 0.8988 - val_loss: 0.2094 - val_acc: 0.9101\n",
      "Epoch 5/25\n",
      "228947/228947 [==============================] - 183s 800us/step - loss: 0.2141 - acc: 0.9066 - val_loss: 0.2024 - val_acc: 0.9129\n",
      "Epoch 6/25\n",
      "228947/228947 [==============================] - 183s 800us/step - loss: 0.2020 - acc: 0.9126 - val_loss: 0.1879 - val_acc: 0.9185\n",
      "Epoch 7/25\n",
      "228947/228947 [==============================] - 183s 800us/step - loss: 0.1924 - acc: 0.9168 - val_loss: 0.1918 - val_acc: 0.9192\n",
      "Epoch 8/25\n",
      "228947/228947 [==============================] - 187s 819us/step - loss: 0.1853 - acc: 0.9209 - val_loss: 0.1880 - val_acc: 0.9228\n",
      "Epoch 9/25\n",
      "228947/228947 [==============================] - 187s 817us/step - loss: 0.1789 - acc: 0.9233 - val_loss: 0.1787 - val_acc: 0.9243\n",
      "Epoch 10/25\n",
      "228947/228947 [==============================] - 189s 824us/step - loss: 0.1732 - acc: 0.9263 - val_loss: 0.1790 - val_acc: 0.9259\n",
      "Epoch 11/25\n",
      " 71680/228947 [========>.....................] - ETA: 2:04 - loss: 0.1647 - acc: 0.9303"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a59053ed2bb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdev_x1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_x2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_f\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_reducer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_point\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     )\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2665\u001b[0m                     \u001b[1;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[1;32m-> 2667\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2647\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2648\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2649\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2650\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "best_results = []\n",
    "# last_results = []\n",
    "best_file_names = []\n",
    "dev_predictions = []\n",
    "\n",
    "for i, (train_index, dev_index) in enumerate(StratifiedKFold(n_splits=10).split(X=char1, y=label)):  # word/char switch\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Fold {} training start...\".format(i))\n",
    "    \n",
    "    train_x1, train_x2, train_y = char1[train_index, :], char2[train_index, :], label[train_index]  # word/char switch\n",
    "    dev_x1, dev_x2, dev_y = char1[dev_index, :], char2[dev_index, :], label[dev_index]  # word/char switch\n",
    "    train_f, dev_f = train_feature[train_index, :], train_feature[dev_index, :]\n",
    "    \n",
    "    input1 = Input(shape=(SEQ_LEN,), dtype=\"int32\")\n",
    "    input2 = Input(shape=(SEQ_LEN,), dtype=\"int32\")\n",
    "    inputf = Input(shape=(train_f.shape[1],), dtype=\"float32\")\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=char_embedding_data.shape[0],  # word/char switch\n",
    "        output_dim=char_embedding_data.shape[1],  # word/char switch\n",
    "        weights=[char_embedding_data],  # word/char switch\n",
    "        input_length=SEQ_LEN,\n",
    "        trainable=False\n",
    "    )\n",
    "    \n",
    "    vector1 = embedding_layer(input1)\n",
    "    vector2 = embedding_layer(input2)\n",
    "    \n",
    "    lstm_layer1 = LSTM(LSTM_SIZE1, dropout=LSTM_DROP_RATE, recurrent_dropout=LSTM_DROP_RATE, return_sequences=True)\n",
    "    layer1a = lstm_layer1(vector1)\n",
    "    layer1a = Dropout(LSTM_DROP_RATE)(layer1a)\n",
    "    layer1b = lstm_layer1(vector2)\n",
    "    layer1b = Dropout(LSTM_DROP_RATE)(layer1b)\n",
    "\n",
    "    lstm_layer2 = LSTM(LSTM_SIZE2, dropout=LSTM_DROP_RATE, recurrent_dropout=LSTM_DROP_RATE, return_sequences=True)\n",
    "    layer2a = lstm_layer2(layer1a)\n",
    "    layer2b = lstm_layer2(layer1b)\n",
    "    \n",
    "    conv1a, conv1b = cnn_double_layer(layer2a, layer2b, filters=CONV_LEN1, kernel_size=1)\n",
    "    conv2a, conv2b = cnn_double_layer(layer2a, layer2b, filters=CONV_LEN2, kernel_size=2)\n",
    "    conv3a, conv3b = cnn_double_layer(layer2a, layer2b, filters=CONV_LEN3, kernel_size=3)\n",
    "    conv4a, conv4b = cnn_double_layer(layer2a, layer2b, filters=CONV_LEN4, kernel_size=4)\n",
    "    conv5a, conv5b = cnn_double_layer(layer2a, layer2b, filters=CONV_LEN5, kernel_size=5)\n",
    "    conv6a, conv6b = cnn_double_layer(layer2a, layer2b, filters=CONV_LEN6, kernel_size=6)\n",
    "    \n",
    "    merge_a = concatenate([conv1a, conv2a, conv3a, conv4a, conv5a, conv6a])\n",
    "    merge_b = concatenate([conv1b, conv2b, conv3b, conv4b, conv5b, conv6b])\n",
    "    diff = Lambda(lambda x: K.abs(x[0] - x[1]))([merge_a, merge_b])\n",
    "    mult = Lambda(lambda x: x[0] * x[1])([merge_a, merge_b])\n",
    "    merge = concatenate([diff, mult])\n",
    "    \n",
    "    x = Dropout(DROP_RATE)(merge)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(DENSE_SIZE1, activation=\"relu\")(x)\n",
    "    \n",
    "    fe = BatchNormalization()(inputf)\n",
    "    fe = Dense(DENSE_FEATURE, activation=\"relu\")(fe)\n",
    "    \n",
    "    x = concatenate([x, fe])\n",
    "    \n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(DENSE_SIZE2, activation=\"relu\")(x)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[input1, input2, inputf], outputs=pred)\n",
    "    model.compile(\n",
    "        optimizer=\"nadam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"acc\"]\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\"val_loss\", patience=6)\n",
    "    lr_reducer = ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.0005)\n",
    "    check_point = ModelCheckpoint(\n",
    "        \"./log/%s.Multi_LSTM_CNN_v5.char.{epoch:03d}.hdf5\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),  # word/char switch\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "    )\n",
    "    \n",
    "    fit_res = model.fit(\n",
    "        x=[train_x1, train_x2, train_f],\n",
    "        y=train_y,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHES,\n",
    "        validation_data=([dev_x1, dev_x2, dev_f], dev_y),\n",
    "        shuffle=True,\n",
    "        callbacks=[early_stopping, lr_reducer, check_point]\n",
    "    )\n",
    "    \n",
    "#     pred_last = model.predict([test_char1, test_char2, test_feature], batch_size=BATCH_SIZE)  # word/char switch\n",
    "#     last_results.append(pd.DataFrame(pred_last, columns=[\"y_pre\"]))\n",
    "    \n",
    "    best_model_file = glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\")\n",
    "    best_file_names.append(best_model_file)\n",
    "    print(\"load model %s\" % (best_model_file,))\n",
    "    model.load_weights(best_model_file)\n",
    "    pred_best = model.predict([test_char1, test_char2, test_feature], batch_size=BATCH_SIZE)  # word/char switch\n",
    "    best_results.append(pd.DataFrame(pred_best, columns=[\"y_pre\"]))\n",
    "    \n",
    "    dev_pred = model.predict([dev_x1, dev_x2, dev_f], batch_size=BATCH_SIZE)\n",
    "    dev_result = pd.DataFrame({\"pred\": dev_pred.ravel(), \"label\": dev_y})\n",
    "    dev_predictions.append(dev_result)\n",
    "\n",
    "# pd.DataFrame(pd.concat(last_results, axis=1).mean(axis=1), columns=[\"y_pre\"]).to_csv(\n",
    "#     \"./result/%s-Multi_LSTM_CNN_v5_char_last.csv\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),  # word/char switch\n",
    "#     index=False\n",
    "# )\n",
    "pd.DataFrame(pd.concat(best_results, axis=1).mean(axis=1), columns=[\"y_pre\"]).to_csv(\n",
    "    \"./result/%s-Multi_LSTM_CNN_v5_char_best.csv\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),  # word/char switch\n",
    "    index=False\n",
    ")\n",
    "\n",
    "total_dev = pd.concat(dev_predictions, axis=0)\n",
    "total_dev.to_csv(\n",
    "    \"./result/%s-Multi_LSTM_CNN_v5_char_dev_result.csv\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")), # word/char switch\n",
    "    index=False\n",
    ")\n",
    "\n",
    "model_path = \"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "os.mkdir(model_path)\n",
    "for model_name in best_file_names:\n",
    "    abs_name = os.path.split(model_name)[1]\n",
    "    os.rename(model_name, model_path + abs_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
