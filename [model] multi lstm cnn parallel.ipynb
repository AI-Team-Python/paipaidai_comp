{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_PATH = \"./data/\"\n",
    "TRAIN_PATH = DATA_PATH + \"train.csv\"\n",
    "TEST_PATH = DATA_PATH + \"test.csv\"\n",
    "WORD_EMBED_PATH = DATA_PATH + \"word_embed.txt\"\n",
    "CHAR_EMBED_PATH = DATA_PATH + \"char_embed.txt\"\n",
    "QUEST_PATH = DATA_PATH + \"question.csv\"\n",
    "\n",
    "train_data = pd.read_csv(TRAIN_PATH)\n",
    "test_data = pd.read_csv(TEST_PATH)\n",
    "question_data = pd.read_csv(QUEST_PATH)\n",
    "word_embedding_data = pd.read_csv(WORD_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "char_embedding_data = pd.read_csv(CHAR_EMBED_PATH, delimiter=\" \", header=None, index_col=0)\n",
    "\n",
    "question_data[\"words\"] = question_data[\"words\"].str.split(\" \")\n",
    "question_data[\"chars\"] = question_data[\"chars\"].str.split(\" \")\n",
    "\n",
    "label = train_data[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_WORD_NUMS = 10000\n",
    "\n",
    "word_tokenizer = Tokenizer(MAX_WORD_NUMS)\n",
    "word_tokenizer.fit_on_texts(question_data[\"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_data = np.concatenate(\n",
    "    (\n",
    "        np.zeros(shape=(1, word_embedding_data.shape[1]), dtype=np.float64),\n",
    "        word_embedding_data.loc[list(word_tokenizer.word_index.keys())[:MAX_WORD_NUMS]].values\n",
    "    ),\n",
    "    axis=0\n",
    ")\n",
    "word_embedding_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((254386, 30), (254386, 30), (172956, 30), (172956, 30))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "WORD_SEQ_LEN = 30\n",
    "\n",
    "def gen_data(data):\n",
    "    seq_word1 = word_tokenizer.texts_to_sequences(data.merge(question_data, how=\"left\", left_on=\"q1\", right_on=\"qid\")[\"words\"])\n",
    "    seq_word2 = word_tokenizer.texts_to_sequences(data.merge(question_data, how=\"left\", left_on=\"q2\", right_on=\"qid\")[\"words\"])\n",
    "    return pad_sequences(seq_word1, maxlen=WORD_SEQ_LEN, padding=\"pre\",truncating=\"pre\"), \\\n",
    "        pad_sequences(seq_word2, maxlen=WORD_SEQ_LEN, padding=\"pre\",truncating=\"pre\")\n",
    "\n",
    "word1, word2 = gen_data(train_data)\n",
    "test_word1, test_word2 = gen_data(test_data)\n",
    "\n",
    "word1.shape, word2.shape, test_word1.shape, test_word2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, Dropout, BatchNormalization, Dense, Flatten, Lambda, K\n",
    "from keras.layers import Conv1D, MaxPool1D, GlobalAveragePooling1D\n",
    "from keras.layers import LSTM, Bidirectional, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# general\n",
    "NUM_EPOCHES = 50\n",
    "BATCH_SIZE = 1024\n",
    "DENSE_SIZE = 300\n",
    "DROP_RATE = 0.3\n",
    "\n",
    "# cnn\n",
    "CONV_LEN_1 = 128\n",
    "CONV_LEN_2 = 128\n",
    "CONV_LEN_3 = 128\n",
    "CONV_LEN_4 = 128\n",
    "CONV_LEN_5 = 128\n",
    "CONV_LEN_6 = 128\n",
    "CONV_LEN = CONV_LEN_1 + CONV_LEN_2 + CONV_LEN_3 + CONV_LEN_4 + CONV_LEN_5 + CONV_LEN_6\n",
    "\n",
    "# lstm\n",
    "LSTM_SIZE_1 = 256\n",
    "LSTM_SIZE_2 = 256\n",
    "DROP_RATE_LSTM = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_layer_1(input1, input2, kernel_size, filters):\n",
    "    conv = Conv1D(filters=filters, kernel_size=kernel_size, padding=\"same\", activation=\"relu\")\n",
    "    \n",
    "    conv_a = conv(input1)\n",
    "    conv_a = GlobalAveragePooling1D()(conv_a)\n",
    "    \n",
    "    conv_b = conv(input2)\n",
    "    conv_b = GlobalAveragePooling1D()(conv_b)\n",
    "    return conv_a, conv_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Train on 228946 samples, validate on 25440 samples\n",
      "Epoch 1/50\n",
      "228946/228946 [==============================] - 158s 691us/step - loss: 0.3687 - acc: 0.8332 - val_loss: 0.2842 - val_acc: 0.8726\n",
      "Epoch 2/50\n",
      "228946/228946 [==============================] - 152s 663us/step - loss: 0.2771 - acc: 0.8794 - val_loss: 0.2561 - val_acc: 0.8881\n",
      "Epoch 3/50\n",
      "228946/228946 [==============================] - 152s 663us/step - loss: 0.2434 - acc: 0.8952 - val_loss: 0.2376 - val_acc: 0.8965\n",
      "Epoch 4/50\n",
      "228946/228946 [==============================] - 152s 663us/step - loss: 0.2208 - acc: 0.9060 - val_loss: 0.2334 - val_acc: 0.9004\n",
      "Epoch 5/50\n",
      "228946/228946 [==============================] - 152s 663us/step - loss: 0.2039 - acc: 0.9147 - val_loss: 0.2238 - val_acc: 0.9053\n",
      "Epoch 6/50\n",
      "228946/228946 [==============================] - 152s 663us/step - loss: 0.1875 - acc: 0.9210 - val_loss: 0.2222 - val_acc: 0.9067\n",
      "Epoch 7/50\n",
      "228946/228946 [==============================] - 152s 663us/step - loss: 0.1737 - acc: 0.9280 - val_loss: 0.2161 - val_acc: 0.9106\n",
      "Epoch 8/50\n",
      "228946/228946 [==============================] - 151s 662us/step - loss: 0.1628 - acc: 0.9325 - val_loss: 0.2197 - val_acc: 0.9083\n",
      "Epoch 9/50\n",
      "228946/228946 [==============================] - 152s 662us/step - loss: 0.1528 - acc: 0.9370 - val_loss: 0.2140 - val_acc: 0.9120\n",
      "Epoch 10/50\n",
      "228946/228946 [==============================] - 151s 661us/step - loss: 0.1439 - acc: 0.9408 - val_loss: 0.2283 - val_acc: 0.9088\n",
      "Epoch 11/50\n",
      "228946/228946 [==============================] - 151s 661us/step - loss: 0.1374 - acc: 0.9438 - val_loss: 0.2164 - val_acc: 0.9137\n",
      "Epoch 12/50\n",
      "228946/228946 [==============================] - 151s 661us/step - loss: 0.1284 - acc: 0.9477 - val_loss: 0.2264 - val_acc: 0.9130\n",
      "Epoch 13/50\n",
      "228946/228946 [==============================] - 151s 660us/step - loss: 0.1205 - acc: 0.9516 - val_loss: 0.2342 - val_acc: 0.9109\n",
      "Epoch 14/50\n",
      "228946/228946 [==============================] - 151s 660us/step - loss: 0.1159 - acc: 0.9533 - val_loss: 0.2327 - val_acc: 0.9143\n",
      "Epoch 15/50\n",
      "228946/228946 [==============================] - 151s 660us/step - loss: 0.1099 - acc: 0.9562 - val_loss: 0.2291 - val_acc: 0.9129\n",
      "Epoch 16/50\n",
      "228946/228946 [==============================] - 151s 659us/step - loss: 0.1060 - acc: 0.9578 - val_loss: 0.2366 - val_acc: 0.9128\n",
      "Epoch 17/50\n",
      "228946/228946 [==============================] - 151s 660us/step - loss: 0.1009 - acc: 0.9597 - val_loss: 0.2381 - val_acc: 0.9140\n",
      "Epoch 18/50\n",
      "228946/228946 [==============================] - 151s 659us/step - loss: 0.0970 - acc: 0.9617 - val_loss: 0.2340 - val_acc: 0.9156\n",
      "Epoch 19/50\n",
      "228946/228946 [==============================] - 151s 660us/step - loss: 0.0925 - acc: 0.9632 - val_loss: 0.2474 - val_acc: 0.9144\n",
      "load model ./log/20180705-162034.lstm_cnn_parallel.009.hdf5\n",
      "Train on 228946 samples, validate on 25440 samples\n",
      "Epoch 1/50\n",
      "228946/228946 [==============================] - 155s 676us/step - loss: 0.3692 - acc: 0.8342 - val_loss: 0.2876 - val_acc: 0.8730\n",
      "Epoch 2/50\n",
      "228946/228946 [==============================] - 154s 672us/step - loss: 0.2784 - acc: 0.8791 - val_loss: 0.2574 - val_acc: 0.8879\n",
      "Epoch 3/50\n",
      "228946/228946 [==============================] - 154s 673us/step - loss: 0.2449 - acc: 0.8950 - val_loss: 0.2624 - val_acc: 0.8858\n",
      "Epoch 4/50\n",
      "228946/228946 [==============================] - 154s 673us/step - loss: 0.2225 - acc: 0.9058 - val_loss: 0.2282 - val_acc: 0.9012\n",
      "Epoch 5/50\n",
      "228946/228946 [==============================] - 154s 672us/step - loss: 0.2037 - acc: 0.9145 - val_loss: 0.2274 - val_acc: 0.9044\n",
      "Epoch 6/50\n",
      "228946/228946 [==============================] - 154s 672us/step - loss: 0.1887 - acc: 0.9214 - val_loss: 0.2183 - val_acc: 0.9075\n",
      "Epoch 7/50\n",
      "228946/228946 [==============================] - 154s 672us/step - loss: 0.1758 - acc: 0.9264 - val_loss: 0.2273 - val_acc: 0.9039\n",
      "Epoch 8/50\n",
      "228946/228946 [==============================] - 154s 673us/step - loss: 0.1653 - acc: 0.9320 - val_loss: 0.2198 - val_acc: 0.9093\n",
      "Epoch 9/50\n",
      "228946/228946 [==============================] - 154s 672us/step - loss: 0.1545 - acc: 0.9364 - val_loss: 0.2197 - val_acc: 0.9092\n",
      "Epoch 10/50\n",
      "228946/228946 [==============================] - 154s 673us/step - loss: 0.1453 - acc: 0.9407 - val_loss: 0.2195 - val_acc: 0.9126\n",
      "Epoch 11/50\n",
      "228946/228946 [==============================] - 154s 673us/step - loss: 0.1375 - acc: 0.9438 - val_loss: 0.2261 - val_acc: 0.9102\n",
      "Epoch 12/50\n",
      "228946/228946 [==============================] - 154s 673us/step - loss: 0.1305 - acc: 0.9471 - val_loss: 0.2247 - val_acc: 0.9122\n",
      "Epoch 13/50\n",
      "228946/228946 [==============================] - 154s 673us/step - loss: 0.1225 - acc: 0.9505 - val_loss: 0.2212 - val_acc: 0.9139\n",
      "Epoch 14/50\n",
      "228946/228946 [==============================] - 154s 673us/step - loss: 0.1175 - acc: 0.9529 - val_loss: 0.2310 - val_acc: 0.9103\n",
      "Epoch 15/50\n",
      "228946/228946 [==============================] - 154s 673us/step - loss: 0.1117 - acc: 0.9553 - val_loss: 0.2230 - val_acc: 0.9144\n",
      "Epoch 16/50\n",
      "228946/228946 [==============================] - 154s 672us/step - loss: 0.1072 - acc: 0.9570 - val_loss: 0.2271 - val_acc: 0.9160\n",
      "load model ./log/20180705-171019.lstm_cnn_parallel.006.hdf5\n",
      "Train on 228947 samples, validate on 25439 samples\n",
      "Epoch 1/50\n",
      "228947/228947 [==============================] - 156s 683us/step - loss: 0.3684 - acc: 0.8346 - val_loss: 0.2829 - val_acc: 0.8757\n",
      "Epoch 2/50\n",
      "228947/228947 [==============================] - 154s 674us/step - loss: 0.2767 - acc: 0.8798 - val_loss: 0.2590 - val_acc: 0.8880\n",
      "Epoch 3/50\n",
      "228947/228947 [==============================] - 154s 674us/step - loss: 0.2425 - acc: 0.8960 - val_loss: 0.2399 - val_acc: 0.8985\n",
      "Epoch 4/50\n",
      "228947/228947 [==============================] - 152s 665us/step - loss: 0.2206 - acc: 0.9061 - val_loss: 0.2304 - val_acc: 0.9005\n",
      "Epoch 5/50\n",
      "228947/228947 [==============================] - 151s 662us/step - loss: 0.2019 - acc: 0.9154 - val_loss: 0.2218 - val_acc: 0.9053\n",
      "Epoch 6/50\n",
      "228947/228947 [==============================] - 153s 667us/step - loss: 0.1867 - acc: 0.9219 - val_loss: 0.2255 - val_acc: 0.9057\n",
      "Epoch 7/50\n",
      "228947/228947 [==============================] - 152s 662us/step - loss: 0.1747 - acc: 0.9272 - val_loss: 0.2182 - val_acc: 0.9092\n",
      "Epoch 8/50\n",
      "228947/228947 [==============================] - 152s 663us/step - loss: 0.1631 - acc: 0.9328 - val_loss: 0.2362 - val_acc: 0.9056\n",
      "Epoch 9/50\n",
      "228947/228947 [==============================] - 152s 662us/step - loss: 0.1528 - acc: 0.9372 - val_loss: 0.2247 - val_acc: 0.9109\n",
      "Epoch 10/50\n",
      "228947/228947 [==============================] - 152s 662us/step - loss: 0.1432 - acc: 0.9418 - val_loss: 0.2224 - val_acc: 0.9113\n",
      "Epoch 11/50\n",
      "228947/228947 [==============================] - 152s 663us/step - loss: 0.1362 - acc: 0.9449 - val_loss: 0.2198 - val_acc: 0.9126\n",
      "Epoch 12/50\n",
      "228947/228947 [==============================] - 152s 662us/step - loss: 0.1285 - acc: 0.9486 - val_loss: 0.2236 - val_acc: 0.9114\n",
      "Epoch 13/50\n",
      "228947/228947 [==============================] - 152s 662us/step - loss: 0.1217 - acc: 0.9514 - val_loss: 0.2323 - val_acc: 0.9114\n",
      "Epoch 14/50\n",
      "228947/228947 [==============================] - 152s 663us/step - loss: 0.1152 - acc: 0.9541 - val_loss: 0.2367 - val_acc: 0.9128\n",
      "Epoch 15/50\n",
      "228947/228947 [==============================] - 152s 663us/step - loss: 0.1116 - acc: 0.9549 - val_loss: 0.2286 - val_acc: 0.9133\n",
      "Epoch 16/50\n",
      "228947/228947 [==============================] - 152s 663us/step - loss: 0.1056 - acc: 0.9579 - val_loss: 0.2414 - val_acc: 0.9113\n",
      "Epoch 17/50\n",
      "228947/228947 [==============================] - 152s 663us/step - loss: 0.1013 - acc: 0.9597 - val_loss: 0.2388 - val_acc: 0.9138\n",
      "load model ./log/20180705-175308.lstm_cnn_parallel.007.hdf5\n",
      "Train on 228947 samples, validate on 25439 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228947/228947 [==============================] - 159s 694us/step - loss: 0.3670 - acc: 0.8345 - val_loss: 0.2844 - val_acc: 0.8742\n",
      "Epoch 2/50\n",
      "228947/228947 [==============================] - 154s 673us/step - loss: 0.2783 - acc: 0.8782 - val_loss: 0.2551 - val_acc: 0.8886\n",
      "Epoch 3/50\n",
      "228947/228947 [==============================] - 154s 674us/step - loss: 0.2445 - acc: 0.8954 - val_loss: 0.2383 - val_acc: 0.8974\n",
      "Epoch 4/50\n",
      "228947/228947 [==============================] - 154s 673us/step - loss: 0.2222 - acc: 0.9056 - val_loss: 0.2251 - val_acc: 0.9040\n",
      "Epoch 5/50\n",
      "228947/228947 [==============================] - 154s 673us/step - loss: 0.2035 - acc: 0.9143 - val_loss: 0.2220 - val_acc: 0.9060\n",
      "Epoch 6/50\n",
      "228947/228947 [==============================] - 154s 673us/step - loss: 0.1891 - acc: 0.9206 - val_loss: 0.2198 - val_acc: 0.9086\n",
      "Epoch 7/50\n",
      "228947/228947 [==============================] - 154s 673us/step - loss: 0.1760 - acc: 0.9267 - val_loss: 0.2172 - val_acc: 0.9094\n",
      "Epoch 8/50\n",
      "228947/228947 [==============================] - 154s 673us/step - loss: 0.1645 - acc: 0.9317 - val_loss: 0.2111 - val_acc: 0.9117\n",
      "Epoch 9/50\n",
      "228947/228947 [==============================] - 154s 673us/step - loss: 0.1536 - acc: 0.9368 - val_loss: 0.2152 - val_acc: 0.9112\n",
      "Epoch 10/50\n",
      "228947/228947 [==============================] - 154s 673us/step - loss: 0.1455 - acc: 0.9407 - val_loss: 0.2200 - val_acc: 0.9122\n",
      "Epoch 11/50\n",
      "228947/228947 [==============================] - 154s 673us/step - loss: 0.1373 - acc: 0.9439 - val_loss: 0.2204 - val_acc: 0.9132\n",
      "Epoch 12/50\n",
      "228947/228947 [==============================] - 154s 673us/step - loss: 0.1296 - acc: 0.9477 - val_loss: 0.2187 - val_acc: 0.9147\n",
      "Epoch 13/50\n",
      "228947/228947 [==============================] - 155s 675us/step - loss: 0.1232 - acc: 0.9507 - val_loss: 0.2172 - val_acc: 0.9150\n",
      "Epoch 14/50\n",
      "199680/228947 [=========================>....] - ETA: 18s - loss: 0.1155 - acc: 0.9534"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-13c19b7a4dfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdev_word1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_word2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_point\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m     )\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2665\u001b[0m                     \u001b[1;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[1;32m-> 2667\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2647\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2648\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2649\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2650\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "best_results = []\n",
    "last_results = []\n",
    "\n",
    "for i, (train_index, dev_index) in enumerate(StratifiedKFold(n_splits=10).split(X=word1, y=label)):\n",
    "    train_word1, train_word2, train_y = word1[train_index, :], word2[train_index, :], label[train_index]\n",
    "    dev_word1, dev_word2, dev_y = word1[dev_index, :], word2[dev_index, :], label[dev_index]\n",
    "\n",
    "    word_input1 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "    word_input2 = Input(shape=(WORD_SEQ_LEN,), dtype=\"int32\")\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "        input_dim=word_embedding_data.shape[0],\n",
    "        output_dim=word_embedding_data.shape[1],\n",
    "        weights=[word_embedding_data],\n",
    "        input_length=WORD_SEQ_LEN,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    word_vector1 = embedding_layer(word_input1)\n",
    "    word_vector2 = embedding_layer(word_input2)\n",
    "    \n",
    "    # LSTM layer\n",
    "    lstm_layer1 = LSTM(LSTM_SIZE_1, dropout=DROP_RATE_LSTM, recurrent_dropout=DROP_RATE_LSTM, return_sequences=True)\n",
    "    word_first_1 = lstm_layer1(word_vector1)\n",
    "    word_first_1 = Dropout(DROP_RATE_LSTM)(word_first_1)\n",
    "    word_first_2 = lstm_layer1(word_vector2)\n",
    "    word_first_2 = Dropout(DROP_RATE_LSTM)(word_first_2)\n",
    "\n",
    "    lstm_layer2 = LSTM(LSTM_SIZE_2, dropout=DROP_RATE_LSTM, recurrent_dropout=DROP_RATE_LSTM, return_sequences=False)\n",
    "    word_second_1 = lstm_layer2(word_first_1)\n",
    "    word_second_2 = lstm_layer2(word_first_2)\n",
    "    lstm_merge = concatenate([word_second_1, word_second_2])\n",
    "    \n",
    "    # CNN layer\n",
    "    conv1a, conv1b = cnn_layer_1(word_vector1, word_vector2, kernel_size=1, filters=CONV_LEN_1)\n",
    "    conv2a, conv2b = cnn_layer_1(word_vector1, word_vector2, kernel_size=2, filters=CONV_LEN_2)\n",
    "    conv3a, conv3b = cnn_layer_1(word_vector1, word_vector2, kernel_size=3, filters=CONV_LEN_3)\n",
    "    conv4a, conv4b = cnn_layer_1(word_vector1, word_vector2, kernel_size=4, filters=CONV_LEN_4)\n",
    "    conv5a, conv5b = cnn_layer_1(word_vector1, word_vector2, kernel_size=5, filters=CONV_LEN_5)\n",
    "    conv6a, conv6b = cnn_layer_1(word_vector1, word_vector2, kernel_size=6, filters=CONV_LEN_6)\n",
    "    \n",
    "    merge_a = concatenate([conv1a, conv2a, conv3a, conv4a, conv5a, conv6a])\n",
    "    merge_b = concatenate([conv1b, conv2b, conv3b, conv4b, conv5b, conv6b])\n",
    "    diff = Lambda(lambda x: K.abs(x[0] - x[1]))([merge_a, merge_b])\n",
    "    mult = Lambda(lambda x: x[0] * x[1])([merge_a, merge_b])\n",
    "    cnn_merge = concatenate([diff, mult])\n",
    "    \n",
    "    # Merge\n",
    "    lstm_merge = Dropout(DROP_RATE)(lstm_merge)\n",
    "    lstm_merge = BatchNormalization()(lstm_merge)\n",
    "    lstm_dense = Dense(DENSE_SIZE, activation=\"relu\")(lstm_merge)\n",
    "    \n",
    "    cnn_merge = Dropout(DROP_RATE)(cnn_merge)\n",
    "    cnn_merge = BatchNormalization()(cnn_merge)\n",
    "    cnn_dense = Dense(DENSE_SIZE, activation=\"relu\")(cnn_merge)\n",
    "    \n",
    "    merge = concatenate([lstm_dense, cnn_dense])\n",
    "    \n",
    "    # Encode & output\n",
    "    merge = Dropout(DROP_RATE)(merge)\n",
    "    merge = BatchNormalization()(merge)\n",
    "    x = Dense(DENSE_SIZE, activation=\"relu\")(merge)\n",
    "    x = Dropout(DROP_RATE)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[word_input1, word_input2], outputs=pred)\n",
    "    model.compile(\n",
    "        optimizer=\"nadam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"acc\"]\n",
    "    )\n",
    "    \n",
    "    early_stopping = EarlyStopping(\"val_loss\", patience=10)\n",
    "    check_point = ModelCheckpoint(\n",
    "        \"./log/%s.lstm_cnn_parallel.{epoch:03d}.hdf5\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "    \n",
    "    train_res = model.fit(\n",
    "        x=[train_word1, train_word2],\n",
    "        y=train_y,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=NUM_EPOCHES,\n",
    "        validation_data=([dev_word1, dev_word2], dev_y),\n",
    "        shuffle=True,\n",
    "        callbacks=[early_stopping, check_point]\n",
    "    )\n",
    "    \n",
    "    pred_last = model.predict([test_word1, test_word2], batch_size=BATCH_SIZE)\n",
    "    last_results.append(pd.DataFrame(pred_last, columns=[\"y_pre\"]))\n",
    "\n",
    "    print(\"load model %s\" % (glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"),))\n",
    "    model.load_weights(glob(\"./log/*.hdf5\")[-1].replace(\"\\\\\", \"/\"))\n",
    "    pred_best = model.predict([test_word1, test_word2], batch_size=BATCH_SIZE)\n",
    "    best_results.append(pd.DataFrame(pred_best, columns=[\"y_pre\"]))\n",
    "\n",
    "pd.DataFrame(pd.concat(last_results, axis=1).mean(axis=1), columns=[\"y_pre\"]).to_csv(\n",
    "    \"./result/%s-multilstm_cnn_parallel_pred_last.csv\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "pd.DataFrame(pd.concat(best_results, axis=1).mean(axis=1), columns=[\"y_pre\"]).to_csv(\n",
    "    \"./result/%s-multilstm_cnn_parallel_pred_best.csv\" % (datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
